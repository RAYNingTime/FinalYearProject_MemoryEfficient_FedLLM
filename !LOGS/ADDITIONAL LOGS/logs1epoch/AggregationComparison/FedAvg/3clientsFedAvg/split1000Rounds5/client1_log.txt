2025-05-20 23:01:41 
Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]
Generating train split:  72%|███████▏  | 86000/120000 [00:00<00:00, 850049.83 examples/s]
Generating train split: 100%|██████████| 120000/120000 [00:00<00:00, 957707.51 examples/s]
2025-05-20 23:01:41 
Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]
Generating test split: 100%|██████████| 7600/7600 [00:00<00:00, 888450.36 examples/s]
2025-05-20 23:01:43 
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 1332.35 examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 1326.44 examples/s]
2025-05-20 23:01:44 
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]
Map:  25%|██▌       | 252/1000 [00:00<00:00, 2497.00 examples/s]
Map:  52%|█████▏    | 522/1000 [00:00<00:00, 2612.71 examples/s]
Map:  82%|████████▏ | 815/1000 [00:00<00:00, 2240.79 examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 2161.05 examples/s]
2025-05-20 23:01:44 /app/client.py:56: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
2025-05-20 23:01:44   trainer = Trainer(
2025-05-20 23:01:44 WARNING :   DEPRECATED FEATURE: flwr.client.start_numpy_client() is deprecated. 
2025-05-20 23:01:44 Instead, use `flwr.client.start_client()` by ensuring you first call the `.to_client()` method as shown below: 
2025-05-20 23:01:44 flwr.client.start_client(
2025-05-20 23:01:44 server_address='<IP>:<PORT>',
2025-05-20 23:01:44 client=FlowerClient().to_client(), # <-- where FlowerClient is of type flwr.client.NumPyClient object
2025-05-20 23:01:44 )
2025-05-20 23:01:44 Using `start_numpy_client()` is deprecated.
2025-05-20 23:01:44 
2025-05-20 23:01:44             This is a deprecated feature. It will be removed
2025-05-20 23:01:44             entirely in future versions of Flower.
2025-05-20 23:01:44         
2025-05-20 23:01:44 WARNING :   DEPRECATED FEATURE: flwr.client.start_client() is deprecated.
2025-05-20 23:01:44 Instead, use the `flower-supernode` CLI command to start a SuperNode as shown below:
2025-05-20 23:01:44 
2025-05-20 23:01:44 $ flower-supernode --insecure --superlink='<IP>:<PORT>'
2025-05-20 23:01:44 
2025-05-20 23:01:44 To view all available options, run:
2025-05-20 23:01:44 
2025-05-20 23:01:44 $ flower-supernode --help
2025-05-20 23:01:44 
2025-05-20 23:01:44 Using `start_client()` is deprecated.
2025-05-20 23:01:44 
2025-05-20 23:01:44             This is a deprecated feature. It will be removed
2025-05-20 23:01:44             entirely in future versions of Flower.
2025-05-20 23:01:44         
2025-05-20 23:01:45 INFO :      
2025-05-20 23:01:45 INFO :      Received: get_parameters message 24d93871-d3fd-469e-853b-8d0c7398202d
2025-05-20 23:01:48 INFO :      Sent reply
2025-05-20 23:10:22 INFO :      
2025-05-20 23:10:22 INFO :      Received: evaluate message 05b39968-e129-4ac5-bbd2-a31e2d1f12d1
2025-05-20 23:10:30 INFO :      Sent reply
2025-05-20 23:10:40 INFO :      
2025-05-20 23:10:40 INFO :      Received: train message 85ad7703-4f1d-4be1-b49f-112bc7bd306f
2025-05-20 23:10:30 {'eval_loss': 1.2905173301696777, 'eval_model_preparation_time': 0.0066, 'eval_runtime': 6.419, 'eval_samples_per_second': 31.158, 'eval_steps_per_second': 3.895}
2025-05-20 23:11:08 {'loss': 1.451, 'grad_norm': 12.456618309020996, 'learning_rate': 4.8875e-05, 'epoch': 0.025}
2025-05-20 23:11:29 {'loss': 1.4318, 'grad_norm': 12.24586296081543, 'learning_rate': 4.7625000000000006e-05, 'epoch': 0.05}
2025-05-20 23:11:44 {'loss': 1.4996, 'grad_norm': 12.848953247070312, 'learning_rate': 4.6375e-05, 'epoch': 0.075}
2025-05-20 23:11:58 {'loss': 1.5143, 'grad_norm': 10.930893898010254, 'learning_rate': 4.5125e-05, 'epoch': 0.1}
2025-05-20 23:12:13 {'loss': 1.3977, 'grad_norm': 13.667430877685547, 'learning_rate': 4.3875e-05, 'epoch': 0.125}
2025-05-20 23:12:28 {'loss': 1.39, 'grad_norm': 16.614831924438477, 'learning_rate': 4.2625000000000006e-05, 'epoch': 0.15}
2025-05-20 23:12:49 {'loss': 1.578, 'grad_norm': 12.712342262268066, 'learning_rate': 4.1375e-05, 'epoch': 0.175}
2025-05-20 23:13:03 {'loss': 1.3244, 'grad_norm': 15.351207733154297, 'learning_rate': 4.0125e-05, 'epoch': 0.2}
2025-05-20 23:13:18 {'loss': 1.5404, 'grad_norm': 15.116842269897461, 'learning_rate': 3.8875e-05, 'epoch': 0.225}
2025-05-20 23:13:32 {'loss': 1.5154, 'grad_norm': 12.700728416442871, 'learning_rate': 3.7625e-05, 'epoch': 0.25}
2025-05-20 23:13:47 {'loss': 1.4586, 'grad_norm': 8.484721183776855, 'learning_rate': 3.6375e-05, 'epoch': 0.275}
2025-05-20 23:14:08 {'loss': 1.4385, 'grad_norm': 13.308899879455566, 'learning_rate': 3.5125e-05, 'epoch': 0.3}
2025-05-20 23:14:22 {'loss': 1.4, 'grad_norm': 10.655771255493164, 'learning_rate': 3.3875000000000003e-05, 'epoch': 0.325}
2025-05-20 23:14:37 {'loss': 1.4177, 'grad_norm': 15.089001655578613, 'learning_rate': 3.2625e-05, 'epoch': 0.35}
2025-05-20 23:14:52 {'loss': 1.5028, 'grad_norm': 15.709905624389648, 'learning_rate': 3.1375e-05, 'epoch': 0.375}
2025-05-20 23:15:06 {'loss': 1.3955, 'grad_norm': 11.215079307556152, 'learning_rate': 3.0125000000000004e-05, 'epoch': 0.4}
2025-05-20 23:15:21 {'loss': 1.4491, 'grad_norm': 12.895166397094727, 'learning_rate': 2.8875e-05, 'epoch': 0.425}
2025-05-20 23:15:42 {'loss': 1.4623, 'grad_norm': 13.712494850158691, 'learning_rate': 2.7625e-05, 'epoch': 0.45}
2025-05-20 23:15:57 {'loss': 1.3177, 'grad_norm': 15.317370414733887, 'learning_rate': 2.6375e-05, 'epoch': 0.475}
2025-05-20 23:16:11 {'loss': 1.5134, 'grad_norm': 13.470314025878906, 'learning_rate': 2.5124999999999997e-05, 'epoch': 0.5}
2025-05-20 23:16:26 {'loss': 1.6634, 'grad_norm': 11.399884223937988, 'learning_rate': 2.3875e-05, 'epoch': 0.525}
2025-05-20 23:16:40 {'loss': 1.4437, 'grad_norm': 12.39746379852295, 'learning_rate': 2.2625e-05, 'epoch': 0.55}
2025-05-20 23:16:55 {'loss': 1.5402, 'grad_norm': 9.886880874633789, 'learning_rate': 2.1375e-05, 'epoch': 0.575}
2025-05-20 23:17:16 {'loss': 1.4539, 'grad_norm': 14.482590675354004, 'learning_rate': 2.0125e-05, 'epoch': 0.6}
2025-05-20 23:17:31 {'loss': 1.586, 'grad_norm': 12.93835163116455, 'learning_rate': 1.8875e-05, 'epoch': 0.625}
2025-05-20 23:17:46 {'loss': 1.4991, 'grad_norm': 13.323896408081055, 'learning_rate': 1.7625e-05, 'epoch': 0.65}
2025-05-20 23:18:01 {'loss': 1.3289, 'grad_norm': 11.403051376342773, 'learning_rate': 1.6375e-05, 'epoch': 0.675}
2025-05-20 23:18:16 {'loss': 1.524, 'grad_norm': 13.652932167053223, 'learning_rate': 1.5125e-05, 'epoch': 0.7}
2025-05-20 23:18:30 {'loss': 1.3342, 'grad_norm': 10.888251304626465, 'learning_rate': 1.3875000000000002e-05, 'epoch': 0.725}
2025-05-20 23:18:45 {'loss': 1.4077, 'grad_norm': 11.056901931762695, 'learning_rate': 1.2625e-05, 'epoch': 0.75}
2025-05-20 23:19:06 {'loss': 1.3871, 'grad_norm': 11.09576416015625, 'learning_rate': 1.1375e-05, 'epoch': 0.775}
2025-05-20 23:19:21 {'loss': 1.4711, 'grad_norm': 12.208066940307617, 'learning_rate': 1.0125e-05, 'epoch': 0.8}
2025-05-20 23:19:35 {'loss': 1.2855, 'grad_norm': 10.107794761657715, 'learning_rate': 8.875e-06, 'epoch': 0.825}
2025-05-20 23:19:50 {'loss': 1.4039, 'grad_norm': 8.870123863220215, 'learning_rate': 7.625e-06, 'epoch': 0.85}
2025-05-20 23:20:05 {'loss': 1.3958, 'grad_norm': 11.010320663452148, 'learning_rate': 6.375000000000001e-06, 'epoch': 0.875}
2025-05-20 23:20:19 {'loss': 1.4547, 'grad_norm': 10.44267463684082, 'learning_rate': 5.125e-06, 'epoch': 0.9}
2025-05-20 23:20:41 {'loss': 1.3781, 'grad_norm': 10.510547637939453, 'learning_rate': 3.875e-06, 'epoch': 0.925}
2025-05-20 23:20:55 {'loss': 1.2875, 'grad_norm': 11.05543327331543, 'learning_rate': 2.625e-06, 'epoch': 0.95}
2025-05-20 23:21:10 {'loss': 1.5035, 'grad_norm': 13.560209274291992, 'learning_rate': 1.3750000000000002e-06, 'epoch': 0.975}
2025-05-20 23:21:25 {'loss': 1.4507, 'grad_norm': 10.800071716308594, 'learning_rate': 1.2500000000000002e-07, 'epoch': 1.0}
2025-05-20 23:21:25 {'train_runtime': 643.2731, 'train_samples_per_second': 1.244, 'train_steps_per_second': 0.622, 'train_loss': 1.4449399089813233, 'epoch': 1.0}
2025-05-20 23:22:17 {'eval_loss': 1.2874194383621216, 'eval_model_preparation_time': 0.0066, 'eval_runtime': 17.0936, 'eval_samples_per_second': 11.7, 'eval_steps_per_second': 1.463, 'epoch': 1.0}
2025-05-20 23:22:49 {'loss': 0.9247, 'grad_norm': 9.493892669677734, 'learning_rate': 4.8875e-05, 'epoch': 0.025}
2025-05-20 23:23:04 {'loss': 1.0428, 'grad_norm': 11.517707824707031, 'learning_rate': 4.7625000000000006e-05, 'epoch': 0.05}
2025-05-20 23:23:24 {'loss': 1.1255, 'grad_norm': 10.325044631958008, 'learning_rate': 4.6375e-05, 'epoch': 0.075}
2025-05-20 23:23:39 {'loss': 1.1505, 'grad_norm': 10.119040489196777, 'learning_rate': 4.5125e-05, 'epoch': 0.1}
2025-05-20 23:23:53 {'loss': 1.0695, 'grad_norm': 11.30005931854248, 'learning_rate': 4.3875e-05, 'epoch': 0.125}
2025-05-20 23:24:08 {'loss': 1.0695, 'grad_norm': 12.375972747802734, 'learning_rate': 4.2625000000000006e-05, 'epoch': 0.15}
2025-05-20 23:24:22 {'loss': 1.2341, 'grad_norm': 9.434112548828125, 'learning_rate': 4.1375e-05, 'epoch': 0.175}
2025-05-20 23:24:43 {'loss': 1.0369, 'grad_norm': 13.595855712890625, 'learning_rate': 4.0125e-05, 'epoch': 0.2}
2025-05-20 23:24:58 {'loss': 1.2105, 'grad_norm': 13.067721366882324, 'learning_rate': 3.8875e-05, 'epoch': 0.225}
2025-05-20 23:25:12 {'loss': 1.206, 'grad_norm': 9.951542854309082, 'learning_rate': 3.7625e-05, 'epoch': 0.25}
2025-05-20 23:25:27 {'loss': 1.1514, 'grad_norm': 7.7184576988220215, 'learning_rate': 3.6375e-05, 'epoch': 0.275}
2025-05-20 23:25:41 {'loss': 1.1578, 'grad_norm': 13.095786094665527, 'learning_rate': 3.5125e-05, 'epoch': 0.3}
2025-05-20 23:26:02 {'loss': 1.1244, 'grad_norm': 12.159012794494629, 'learning_rate': 3.3875000000000003e-05, 'epoch': 0.325}
2025-05-20 23:26:17 {'loss': 1.1674, 'grad_norm': 10.647987365722656, 'learning_rate': 3.2625e-05, 'epoch': 0.35}
2025-05-20 23:26:31 {'loss': 1.2128, 'grad_norm': 11.873456001281738, 'learning_rate': 3.1375e-05, 'epoch': 0.375}
2025-05-20 23:26:45 {'loss': 1.1202, 'grad_norm': 9.790912628173828, 'learning_rate': 3.0125000000000004e-05, 'epoch': 0.4}
2025-05-20 23:27:00 {'loss': 1.1899, 'grad_norm': 10.077378273010254, 'learning_rate': 2.8875e-05, 'epoch': 0.425}
2025-05-20 23:27:15 {'loss': 1.2219, 'grad_norm': 12.169807434082031, 'learning_rate': 2.7625e-05, 'epoch': 0.45}
2025-05-20 23:27:35 {'loss': 1.0966, 'grad_norm': 13.765130996704102, 'learning_rate': 2.6375e-05, 'epoch': 0.475}
2025-05-20 23:27:50 {'loss': 1.2877, 'grad_norm': 13.576817512512207, 'learning_rate': 2.5124999999999997e-05, 'epoch': 0.5}
2025-05-20 23:28:04 {'loss': 1.4075, 'grad_norm': 10.603364944458008, 'learning_rate': 2.3875e-05, 'epoch': 0.525}
2025-05-20 23:28:19 {'loss': 1.2219, 'grad_norm': 11.671853065490723, 'learning_rate': 2.2625e-05, 'epoch': 0.55}
2025-05-20 23:28:33 {'loss': 1.3288, 'grad_norm': 9.588492393493652, 'learning_rate': 2.1375e-05, 'epoch': 0.575}
2025-05-20 23:28:54 {'loss': 1.2564, 'grad_norm': 11.340323448181152, 'learning_rate': 2.0125e-05, 'epoch': 0.6}
2025-05-20 23:29:09 {'loss': 1.3652, 'grad_norm': 15.287032127380371, 'learning_rate': 1.8875e-05, 'epoch': 0.625}
2025-05-20 23:29:23 {'loss': 1.3176, 'grad_norm': 14.551977157592773, 'learning_rate': 1.7625e-05, 'epoch': 0.65}
2025-05-20 23:29:38 {'loss': 1.1609, 'grad_norm': 11.951035499572754, 'learning_rate': 1.6375e-05, 'epoch': 0.675}
2025-05-20 23:29:53 {'loss': 1.3291, 'grad_norm': 12.906556129455566, 'learning_rate': 1.5125e-05, 'epoch': 0.7}
2025-05-20 23:30:08 {'loss': 1.1583, 'grad_norm': 10.45470142364502, 'learning_rate': 1.3875000000000002e-05, 'epoch': 0.725}
2025-05-20 23:30:29 {'loss': 1.2483, 'grad_norm': 10.778995513916016, 'learning_rate': 1.2625e-05, 'epoch': 0.75}
2025-05-20 23:30:43 {'loss': 1.2122, 'grad_norm': 10.694243431091309, 'learning_rate': 1.1375e-05, 'epoch': 0.775}
2025-05-20 23:30:58 {'loss': 1.3162, 'grad_norm': 12.33538818359375, 'learning_rate': 1.0125e-05, 'epoch': 0.8}
2025-05-20 23:31:12 {'loss': 1.1661, 'grad_norm': 9.789636611938477, 'learning_rate': 8.875e-06, 'epoch': 0.825}
2025-05-20 23:31:27 {'loss': 1.2901, 'grad_norm': 9.76511287689209, 'learning_rate': 7.625e-06, 'epoch': 0.85}
2025-05-20 23:31:48 {'loss': 1.3025, 'grad_norm': 10.67513370513916, 'learning_rate': 6.375000000000001e-06, 'epoch': 0.875}
2025-05-20 23:32:02 {'loss': 1.3676, 'grad_norm': 10.052878379821777, 'learning_rate': 5.125e-06, 'epoch': 0.9}
2025-05-20 23:32:17 {'loss': 1.2597, 'grad_norm': 9.05192756652832, 'learning_rate': 3.875e-06, 'epoch': 0.925}
2025-05-20 23:32:32 {'loss': 1.1141, 'grad_norm': 10.683542251586914, 'learning_rate': 2.625e-06, 'epoch': 0.95}
2025-05-20 23:32:47 {'loss': 1.1994, 'grad_norm': 9.927199363708496, 'learning_rate': 1.3750000000000002e-06, 'epoch': 0.975}
2025-05-20 23:33:07 {'loss': 0.9191, 'grad_norm': 8.785053253173828, 'learning_rate': 1.2500000000000002e-07, 'epoch': 1.0}
2025-05-20 23:33:07 {'train_runtime': 637.7176, 'train_samples_per_second': 1.254, 'train_steps_per_second': 0.627, 'train_loss': 1.1935267043113709, 'epoch': 1.0}
2025-05-20 23:34:08 {'eval_loss': 1.3025505542755127, 'eval_model_preparation_time': 0.0066, 'eval_runtime': 17.4596, 'eval_samples_per_second': 11.455, 'eval_steps_per_second': 1.432, 'epoch': 1.0}
2025-05-20 23:34:50 {'loss': 0.668, 'grad_norm': 9.595772743225098, 'learning_rate': 4.8875e-05, 'epoch': 0.025}
2025-05-20 23:35:05 {'loss': 0.7686, 'grad_norm': 10.779637336730957, 'learning_rate': 4.7625000000000006e-05, 'epoch': 0.05}
2025-05-20 23:35:19 {'loss': 0.8503, 'grad_norm': 11.630901336669922, 'learning_rate': 4.6375e-05, 'epoch': 0.075}
2025-05-20 23:35:34 {'loss': 0.9053, 'grad_norm': 10.718778610229492, 'learning_rate': 4.5125e-05, 'epoch': 0.1}
2025-05-20 23:35:48 {'loss': 0.8549, 'grad_norm': 10.256362915039062, 'learning_rate': 4.3875e-05, 'epoch': 0.125}
2025-05-20 23:36:09 {'loss': 0.8386, 'grad_norm': 11.7557954788208, 'learning_rate': 4.2625000000000006e-05, 'epoch': 0.15}
2025-05-20 23:36:24 {'loss': 0.9978, 'grad_norm': 10.393962860107422, 'learning_rate': 4.1375e-05, 'epoch': 0.175}
2025-05-20 23:36:38 {'loss': 0.8208, 'grad_norm': 11.25646686553955, 'learning_rate': 4.0125e-05, 'epoch': 0.2}
2025-05-20 23:36:53 {'loss': 0.9937, 'grad_norm': 11.066679000854492, 'learning_rate': 3.8875e-05, 'epoch': 0.225}
2025-05-20 23:37:08 {'loss': 0.9991, 'grad_norm': 11.179468154907227, 'learning_rate': 3.7625e-05, 'epoch': 0.25}
2025-05-20 23:37:22 {'loss': 0.943, 'grad_norm': 7.482971668243408, 'learning_rate': 3.6375e-05, 'epoch': 0.275}
2025-05-20 23:37:43 {'loss': 0.9558, 'grad_norm': 10.832159996032715, 'learning_rate': 3.5125e-05, 'epoch': 0.3}
2025-05-20 23:37:58 {'loss': 0.9438, 'grad_norm': 10.99582576751709, 'learning_rate': 3.3875000000000003e-05, 'epoch': 0.325}
2025-05-20 23:38:12 {'loss': 1.0113, 'grad_norm': 9.485161781311035, 'learning_rate': 3.2625e-05, 'epoch': 0.35}
2025-05-20 23:38:27 {'loss': 1.0351, 'grad_norm': 11.729026794433594, 'learning_rate': 3.1375e-05, 'epoch': 0.375}
2025-05-20 23:38:47 {'loss': 0.937, 'grad_norm': 10.410589218139648, 'learning_rate': 3.0125000000000004e-05, 'epoch': 0.4}
2025-05-20 23:39:02 {'loss': 1.0249, 'grad_norm': 11.42074966430664, 'learning_rate': 2.8875e-05, 'epoch': 0.425}
2025-05-20 23:39:17 {'loss': 1.0607, 'grad_norm': 12.64834976196289, 'learning_rate': 2.7625e-05, 'epoch': 0.45}
2025-05-20 23:39:32 {'loss': 0.9378, 'grad_norm': 10.605660438537598, 'learning_rate': 2.6375e-05, 'epoch': 0.475}
2025-05-20 23:39:46 {'loss': 1.1283, 'grad_norm': 13.285609245300293, 'learning_rate': 2.5124999999999997e-05, 'epoch': 0.5}
2025-05-20 23:40:01 {'loss': 1.2625, 'grad_norm': 10.910090446472168, 'learning_rate': 2.3875e-05, 'epoch': 0.525}
2025-05-20 23:40:22 {'loss': 1.0671, 'grad_norm': 11.590084075927734, 'learning_rate': 2.2625e-05, 'epoch': 0.55}
2025-05-20 23:40:36 {'loss': 1.1678, 'grad_norm': 10.054574012756348, 'learning_rate': 2.1375e-05, 'epoch': 0.575}
2025-05-20 23:40:51 {'loss': 1.1201, 'grad_norm': 10.444799423217773, 'learning_rate': 2.0125e-05, 'epoch': 0.6}
2025-05-20 23:41:05 {'loss': 1.1967, 'grad_norm': 11.44869327545166, 'learning_rate': 1.8875e-05, 'epoch': 0.625}
2025-05-20 23:41:20 {'loss': 1.1848, 'grad_norm': 12.554526329040527, 'learning_rate': 1.7625e-05, 'epoch': 0.65}
2025-05-20 23:41:35 {'loss': 1.0471, 'grad_norm': 11.580827713012695, 'learning_rate': 1.6375e-05, 'epoch': 0.675}
2025-05-20 23:41:56 {'loss': 1.2165, 'grad_norm': 12.770171165466309, 'learning_rate': 1.5125e-05, 'epoch': 0.7}
2025-05-20 23:42:10 {'loss': 1.0589, 'grad_norm': 9.34829330444336, 'learning_rate': 1.3875000000000002e-05, 'epoch': 0.725}
2025-05-20 23:42:25 {'loss': 1.1681, 'grad_norm': 17.109804153442383, 'learning_rate': 1.2625e-05, 'epoch': 0.75}
2025-05-20 23:42:40 {'loss': 1.1424, 'grad_norm': 12.2387113571167, 'learning_rate': 1.1375e-05, 'epoch': 0.775}
2025-05-20 23:42:54 {'loss': 1.2433, 'grad_norm': 11.85523796081543, 'learning_rate': 1.0125e-05, 'epoch': 0.8}
2025-05-20 23:43:09 {'loss': 1.0995, 'grad_norm': 9.763352394104004, 'learning_rate': 8.875e-06, 'epoch': 0.825}
2025-05-20 23:43:30 {'loss': 1.2416, 'grad_norm': 10.112086296081543, 'learning_rate': 7.625e-06, 'epoch': 0.85}
2025-05-20 23:43:44 {'loss': 1.2548, 'grad_norm': 12.648924827575684, 'learning_rate': 6.375000000000001e-06, 'epoch': 0.875}
2025-05-20 23:43:59 {'loss': 1.3011, 'grad_norm': 9.617286682128906, 'learning_rate': 5.125e-06, 'epoch': 0.9}
2025-05-20 23:44:14 {'loss': 1.2269, 'grad_norm': 9.664241790771484, 'learning_rate': 3.875e-06, 'epoch': 0.925}
2025-05-20 23:44:28 {'loss': 1.0603, 'grad_norm': 11.645237922668457, 'learning_rate': 2.625e-06, 'epoch': 0.95}
2025-05-20 23:44:43 {'loss': 1.0972, 'grad_norm': 10.088080406188965, 'learning_rate': 1.3750000000000002e-06, 'epoch': 0.975}
2025-05-20 23:45:01 {'loss': 0.7933, 'grad_norm': 10.622546195983887, 'learning_rate': 1.2500000000000002e-07, 'epoch': 1.0}
2025-05-20 23:45:01 {'train_runtime': 638.4012, 'train_samples_per_second': 1.253, 'train_steps_per_second': 0.627, 'train_loss': 1.0406138849258424, 'epoch': 1.0}
2025-05-20 23:45:50 {'eval_loss': 1.3262461423873901, 'eval_model_preparation_time': 0.0066, 'eval_runtime': 6.3929, 'eval_samples_per_second': 31.285, 'eval_steps_per_second': 3.911, 'epoch': 1.0}
2025-05-20 23:46:25 {'loss': 0.4689, 'grad_norm': 7.809074878692627, 'learning_rate': 4.8875e-05, 'epoch': 0.025}
2025-05-20 23:46:39 {'loss': 0.5581, 'grad_norm': 12.169540405273438, 'learning_rate': 4.7625000000000006e-05, 'epoch': 0.05}
2025-05-20 23:47:00 {'loss': 0.6727, 'grad_norm': 9.69489860534668, 'learning_rate': 4.6375e-05, 'epoch': 0.075}
2025-05-20 23:47:15 {'loss': 0.7105, 'grad_norm': 8.512414932250977, 'learning_rate': 4.5125e-05, 'epoch': 0.1}
2025-05-20 23:47:29 {'loss': 0.6686, 'grad_norm': 11.562877655029297, 'learning_rate': 4.3875e-05, 'epoch': 0.125}
2025-05-20 23:47:44 {'loss': 0.6752, 'grad_norm': 12.32420539855957, 'learning_rate': 4.2625000000000006e-05, 'epoch': 0.15}
2025-05-20 23:47:58 {'loss': 0.814, 'grad_norm': 9.604641914367676, 'learning_rate': 4.1375e-05, 'epoch': 0.175}
2025-05-20 23:48:19 {'loss': 0.6639, 'grad_norm': 11.831926345825195, 'learning_rate': 4.0125e-05, 'epoch': 0.2}
2025-05-20 23:48:34 {'loss': 0.8054, 'grad_norm': 9.872432708740234, 'learning_rate': 3.8875e-05, 'epoch': 0.225}
2025-05-20 23:48:49 {'loss': 0.8169, 'grad_norm': 8.413412094116211, 'learning_rate': 3.7625e-05, 'epoch': 0.25}
2025-05-20 23:49:03 {'loss': 0.7706, 'grad_norm': 6.8104143142700195, 'learning_rate': 3.6375e-05, 'epoch': 0.275}
2025-05-20 23:49:18 {'loss': 0.8274, 'grad_norm': 11.651571273803711, 'learning_rate': 3.5125e-05, 'epoch': 0.3}
2025-05-20 23:49:39 {'loss': 0.8175, 'grad_norm': 9.974946975708008, 'learning_rate': 3.3875000000000003e-05, 'epoch': 0.325}
2025-05-20 23:49:53 {'loss': 0.8585, 'grad_norm': 9.788290023803711, 'learning_rate': 3.2625e-05, 'epoch': 0.35}
2025-05-20 23:50:08 {'loss': 0.8893, 'grad_norm': 11.091902732849121, 'learning_rate': 3.1375e-05, 'epoch': 0.375}
2025-05-20 23:50:22 {'loss': 0.801, 'grad_norm': 9.571187973022461, 'learning_rate': 3.0125000000000004e-05, 'epoch': 0.4}
2025-05-20 23:50:37 {'loss': 0.8668, 'grad_norm': 9.436062812805176, 'learning_rate': 2.8875e-05, 'epoch': 0.425}
2025-05-20 23:50:58 {'loss': 0.9097, 'grad_norm': 11.102508544921875, 'learning_rate': 2.7625e-05, 'epoch': 0.45}
2025-05-20 23:51:13 {'loss': 0.8008, 'grad_norm': 12.209568977355957, 'learning_rate': 2.6375e-05, 'epoch': 0.475}
2025-05-20 23:51:28 {'loss': 0.9931, 'grad_norm': 12.427180290222168, 'learning_rate': 2.5124999999999997e-05, 'epoch': 0.5}
2025-05-20 23:51:42 {'loss': 1.1092, 'grad_norm': 10.203137397766113, 'learning_rate': 2.3875e-05, 'epoch': 0.525}
2025-05-20 23:51:57 {'loss': 0.9375, 'grad_norm': 21.712770462036133, 'learning_rate': 2.2625e-05, 'epoch': 0.55}
2025-05-20 23:52:11 {'loss': 1.0483, 'grad_norm': 9.638463973999023, 'learning_rate': 2.1375e-05, 'epoch': 0.575}
2025-05-20 23:52:32 {'loss': 1.0075, 'grad_norm': 9.962868690490723, 'learning_rate': 2.0125e-05, 'epoch': 0.6}
2025-05-20 23:52:47 {'loss': 1.085, 'grad_norm': 11.97634506225586, 'learning_rate': 1.8875e-05, 'epoch': 0.625}
2025-05-20 23:53:01 {'loss': 1.089, 'grad_norm': 12.656718254089355, 'learning_rate': 1.7625e-05, 'epoch': 0.65}
2025-05-20 23:53:16 {'loss': 0.9695, 'grad_norm': 10.551613807678223, 'learning_rate': 1.6375e-05, 'epoch': 0.675}
2025-05-20 23:53:30 {'loss': 1.13, 'grad_norm': 11.277274131774902, 'learning_rate': 1.5125e-05, 'epoch': 0.7}
2025-05-20 23:53:52 {'loss': 0.9795, 'grad_norm': 10.310032844543457, 'learning_rate': 1.3875000000000002e-05, 'epoch': 0.725}
2025-05-20 23:54:06 {'loss': 1.1009, 'grad_norm': 25.693241119384766, 'learning_rate': 1.2625e-05, 'epoch': 0.75}
2025-05-20 23:54:21 {'loss': 1.0867, 'grad_norm': 11.229466438293457, 'learning_rate': 1.1375e-05, 'epoch': 0.775}
2025-05-20 23:54:35 {'loss': 1.1926, 'grad_norm': 12.210149765014648, 'learning_rate': 1.0125e-05, 'epoch': 0.8}
2025-05-20 23:54:50 {'loss': 1.0663, 'grad_norm': 10.116268157958984, 'learning_rate': 8.875e-06, 'epoch': 0.825}
2025-05-20 23:55:04 {'loss': 1.1878, 'grad_norm': 10.59285831451416, 'learning_rate': 7.625e-06, 'epoch': 0.85}
2025-05-20 23:55:25 {'loss': 1.2064, 'grad_norm': 12.16269302368164, 'learning_rate': 6.375000000000001e-06, 'epoch': 0.875}
2025-05-20 23:55:40 {'loss': 1.2817, 'grad_norm': 11.24182415008545, 'learning_rate': 5.125e-06, 'epoch': 0.9}
2025-05-20 23:55:54 {'loss': 1.2236, 'grad_norm': 14.893583297729492, 'learning_rate': 3.875e-06, 'epoch': 0.925}
2025-05-20 23:56:09 {'loss': 1.0307, 'grad_norm': 10.797828674316406, 'learning_rate': 2.625e-06, 'epoch': 0.95}
2025-05-20 23:56:24 {'loss': 1.0384, 'grad_norm': 12.590682983398438, 'learning_rate': 1.3750000000000002e-06, 'epoch': 0.975}
2025-05-20 23:56:41 {'loss': 0.7127, 'grad_norm': 10.151900291442871, 'learning_rate': 1.2500000000000002e-07, 'epoch': 1.0}
2025-05-20 23:56:41 {'train_runtime': 637.5145, 'train_samples_per_second': 1.255, 'train_steps_per_second': 0.627, 'train_loss': 0.9218073642253876, 'epoch': 1.0}
2025-05-20 23:57:28 {'eval_loss': 1.3527560234069824, 'eval_model_preparation_time': 0.0066, 'eval_runtime': 10.1349, 'eval_samples_per_second': 19.734, 'eval_steps_per_second': 2.467, 'epoch': 1.0}
2025-05-20 23:21:32 INFO :      Sent reply
2025-05-20 23:21:59 INFO :      
2025-05-20 23:21:59 INFO :      Received: evaluate message 1d12a2e1-94e4-4730-98ae-dc02129cfc54
2025-05-20 23:22:17 INFO :      Sent reply
2025-05-20 23:22:25 INFO :      
2025-05-20 23:22:25 INFO :      Received: train message ec9f2206-1036-4e60-8a8a-c14bad41fa39
2025-05-20 23:33:15 INFO :      Sent reply
2025-05-20 23:33:49 INFO :      
2025-05-20 23:33:49 INFO :      Received: evaluate message 82059186-b8f4-4149-a74c-3dcb0aae6369
2025-05-20 23:34:08 INFO :      Sent reply
2025-05-20 23:34:19 INFO :      
2025-05-20 23:34:19 INFO :      Received: train message 51cd9644-a8b3-467c-84d4-1dc22414a38d
2025-05-20 23:45:06 INFO :      Sent reply
2025-05-20 23:45:42 INFO :      
2025-05-20 23:45:42 INFO :      Received: evaluate message 8ecf4e9a-9bf9-458f-8493-6b79bacb570e
2025-05-20 23:45:50 INFO :      Sent reply
2025-05-20 23:46:00 INFO :      
2025-05-20 23:46:00 INFO :      Received: train message 4b8fec09-153f-48ec-9f84-aec8514242ff
2025-05-20 23:56:44 INFO :      Sent reply
2025-05-20 23:57:17 INFO :      
2025-05-20 23:57:17 INFO :      Received: evaluate message 1963bbbd-65f8-4e25-9c2d-f037911b3ab6
2025-05-20 23:57:28 INFO :      Sent reply
2025-05-20 23:57:30 INFO :      
2025-05-20 23:57:30 INFO :      Received: reconnect message 4ea656a5-1aa3-4583-a189-183e337131fc
2025-05-20 23:57:30 INFO :      Disconnect and shut down
