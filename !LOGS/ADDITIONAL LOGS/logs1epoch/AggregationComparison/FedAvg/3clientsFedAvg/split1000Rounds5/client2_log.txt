2025-05-20 23:01:51 
Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]
Generating train split:  86%|████████▌ | 103000/120000 [00:00<00:00, 1016169.56 examples/s]
Generating train split: 100%|██████████| 120000/120000 [00:00<00:00, 1057807.57 examples/s]
2025-05-20 23:01:51 
Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]
Generating test split: 100%|██████████| 7600/7600 [00:00<00:00, 1037113.17 examples/s]
2025-05-20 23:01:53 
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 1297.15 examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 1290.89 examples/s]
2025-05-20 23:01:54 
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]
Map:  23%|██▎       | 231/1000 [00:00<00:00, 2281.40 examples/s]
Map:  48%|████▊     | 476/1000 [00:00<00:00, 2369.14 examples/s]
Map:  73%|███████▎  | 729/1000 [00:00<00:00, 2439.95 examples/s]
Map:  98%|█████████▊| 978/1000 [00:00<00:00, 2456.04 examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 2245.93 examples/s]
2025-05-20 23:01:54 /app/client.py:56: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
2025-05-20 23:01:54   trainer = Trainer(
2025-05-20 23:01:54 WARNING :   DEPRECATED FEATURE: flwr.client.start_numpy_client() is deprecated. 
2025-05-20 23:01:54 Instead, use `flwr.client.start_client()` by ensuring you first call the `.to_client()` method as shown below: 
2025-05-20 23:01:54 flwr.client.start_client(
2025-05-20 23:01:54 server_address='<IP>:<PORT>',
2025-05-20 23:01:54 client=FlowerClient().to_client(), # <-- where FlowerClient is of type flwr.client.NumPyClient object
2025-05-20 23:01:54 )
2025-05-20 23:01:54 Using `start_numpy_client()` is deprecated.
2025-05-20 23:01:54 
2025-05-20 23:01:54             This is a deprecated feature. It will be removed
2025-05-20 23:01:54             entirely in future versions of Flower.
2025-05-20 23:01:54         
2025-05-20 23:01:54 WARNING :   DEPRECATED FEATURE: flwr.client.start_client() is deprecated.
2025-05-20 23:01:54 Instead, use the `flower-supernode` CLI command to start a SuperNode as shown below:
2025-05-20 23:01:54 
2025-05-20 23:01:54 $ flower-supernode --insecure --superlink='<IP>:<PORT>'
2025-05-20 23:01:54 
2025-05-20 23:01:54 To view all available options, run:
2025-05-20 23:01:54 
2025-05-20 23:01:54 $ flower-supernode --help
2025-05-20 23:01:54 
2025-05-20 23:01:54 Using `start_client()` is deprecated.
2025-05-20 23:01:54 
2025-05-20 23:01:54             This is a deprecated feature. It will be removed
2025-05-20 23:01:54             entirely in future versions of Flower.
2025-05-20 23:01:54         
2025-05-20 23:03:03 INFO :      
2025-05-20 23:03:03 INFO :      Received: train message cd54de87-7159-4d76-a0f2-3ac9a7c061d2
2025-05-20 23:03:17 {'loss': 2.5464, 'grad_norm': 13.050023078918457, 'learning_rate': 4.8875e-05, 'epoch': 0.025}
2025-05-20 23:03:30 {'loss': 1.6582, 'grad_norm': 35.893226623535156, 'learning_rate': 4.7625000000000006e-05, 'epoch': 0.05}
2025-05-20 23:03:40 {'loss': 1.6262, 'grad_norm': 13.196735382080078, 'learning_rate': 4.6375e-05, 'epoch': 0.075}
2025-05-20 23:03:49 {'loss': 1.5437, 'grad_norm': 17.02415657043457, 'learning_rate': 4.5125e-05, 'epoch': 0.1}
2025-05-20 23:03:59 {'loss': 1.4678, 'grad_norm': 11.372418403625488, 'learning_rate': 4.3875e-05, 'epoch': 0.125}
2025-05-20 23:04:09 {'loss': 1.5648, 'grad_norm': 13.881046295166016, 'learning_rate': 4.2625000000000006e-05, 'epoch': 0.15}
2025-05-20 23:04:18 {'loss': 1.3518, 'grad_norm': 11.408464431762695, 'learning_rate': 4.1375e-05, 'epoch': 0.175}
2025-05-20 23:04:32 {'loss': 1.4709, 'grad_norm': 9.584808349609375, 'learning_rate': 4.0125e-05, 'epoch': 0.2}
2025-05-20 23:04:41 {'loss': 1.3597, 'grad_norm': 9.200722694396973, 'learning_rate': 3.8875e-05, 'epoch': 0.225}
2025-05-20 23:04:51 {'loss': 1.497, 'grad_norm': 14.409963607788086, 'learning_rate': 3.7625e-05, 'epoch': 0.25}
2025-05-20 23:05:00 {'loss': 1.499, 'grad_norm': 11.060396194458008, 'learning_rate': 3.6375e-05, 'epoch': 0.275}
2025-05-20 23:05:10 {'loss': 1.4217, 'grad_norm': 13.578251838684082, 'learning_rate': 3.5125e-05, 'epoch': 0.3}
2025-05-20 23:05:20 {'loss': 1.4481, 'grad_norm': 18.430217742919922, 'learning_rate': 3.3875000000000003e-05, 'epoch': 0.325}
2025-05-20 23:05:33 {'loss': 1.4554, 'grad_norm': 12.407533645629883, 'learning_rate': 3.2625e-05, 'epoch': 0.35}
2025-05-20 23:05:42 {'loss': 1.4376, 'grad_norm': 13.25733470916748, 'learning_rate': 3.1375e-05, 'epoch': 0.375}
2025-05-20 23:05:52 {'loss': 1.4026, 'grad_norm': 11.547636985778809, 'learning_rate': 3.0125000000000004e-05, 'epoch': 0.4}
2025-05-20 23:06:02 {'loss': 1.3881, 'grad_norm': 11.850982666015625, 'learning_rate': 2.8875e-05, 'epoch': 0.425}
2025-05-20 23:06:12 {'loss': 1.3356, 'grad_norm': 18.262470245361328, 'learning_rate': 2.7625e-05, 'epoch': 0.45}
2025-05-20 23:06:25 {'loss': 1.7855, 'grad_norm': 18.304658889770508, 'learning_rate': 2.6375e-05, 'epoch': 0.475}
2025-05-20 23:06:35 {'loss': 1.5549, 'grad_norm': 19.354894638061523, 'learning_rate': 2.5124999999999997e-05, 'epoch': 0.5}
2025-05-20 23:06:44 {'loss': 1.5303, 'grad_norm': 13.836915016174316, 'learning_rate': 2.3875e-05, 'epoch': 0.525}
2025-05-20 23:06:54 {'loss': 1.2938, 'grad_norm': 11.410408020019531, 'learning_rate': 2.2625e-05, 'epoch': 0.55}
2025-05-20 23:07:04 {'loss': 1.2941, 'grad_norm': 14.32803726196289, 'learning_rate': 2.1375e-05, 'epoch': 0.575}
2025-05-20 23:07:13 {'loss': 1.3517, 'grad_norm': 12.446013450622559, 'learning_rate': 2.0125e-05, 'epoch': 0.6}
2025-05-20 23:07:23 {'loss': 1.3911, 'grad_norm': 11.689190864562988, 'learning_rate': 1.8875e-05, 'epoch': 0.625}
2025-05-20 23:07:36 {'loss': 1.4253, 'grad_norm': 15.368087768554688, 'learning_rate': 1.7625e-05, 'epoch': 0.65}
2025-05-20 23:07:46 {'loss': 1.5152, 'grad_norm': 12.535024642944336, 'learning_rate': 1.6375e-05, 'epoch': 0.675}
2025-05-20 23:07:56 {'loss': 1.4706, 'grad_norm': 14.214369773864746, 'learning_rate': 1.5125e-05, 'epoch': 0.7}
2025-05-20 23:08:05 {'loss': 1.4185, 'grad_norm': 11.14585018157959, 'learning_rate': 1.3875000000000002e-05, 'epoch': 0.725}
2025-05-20 23:08:15 {'loss': 1.3739, 'grad_norm': 13.81734848022461, 'learning_rate': 1.2625e-05, 'epoch': 0.75}
2025-05-20 23:08:25 {'loss': 1.4123, 'grad_norm': 10.078946113586426, 'learning_rate': 1.1375e-05, 'epoch': 0.775}
2025-05-20 23:08:38 {'loss': 1.4956, 'grad_norm': 9.239145278930664, 'learning_rate': 1.0125e-05, 'epoch': 0.8}
2025-05-20 23:08:48 {'loss': 1.35, 'grad_norm': 11.062772750854492, 'learning_rate': 8.875e-06, 'epoch': 0.825}
2025-05-20 23:08:57 {'loss': 1.3809, 'grad_norm': 11.624031066894531, 'learning_rate': 7.625e-06, 'epoch': 0.85}
2025-05-20 23:09:07 {'loss': 1.3947, 'grad_norm': 12.035932540893555, 'learning_rate': 6.375000000000001e-06, 'epoch': 0.875}
2025-05-20 23:09:17 {'loss': 1.4167, 'grad_norm': 12.273659706115723, 'learning_rate': 5.125e-06, 'epoch': 0.9}
2025-05-20 23:09:26 {'loss': 1.4396, 'grad_norm': 10.047100067138672, 'learning_rate': 3.875e-06, 'epoch': 0.925}
2025-05-20 23:09:36 {'loss': 1.386, 'grad_norm': 10.50915241241455, 'learning_rate': 2.625e-06, 'epoch': 0.95}
2025-05-20 23:09:49 {'loss': 1.4708, 'grad_norm': 15.485207557678223, 'learning_rate': 1.3750000000000002e-06, 'epoch': 0.975}
2025-05-20 23:09:59 {'loss': 1.3677, 'grad_norm': 16.712385177612305, 'learning_rate': 1.2500000000000002e-07, 'epoch': 1.0}
2025-05-20 23:09:59 {'train_runtime': 415.2868, 'train_samples_per_second': 1.926, 'train_steps_per_second': 0.963, 'train_loss': 1.4748459100723266, 'epoch': 1.0}
2025-05-20 23:10:34 {'eval_loss': 1.279516577720642, 'eval_runtime': 9.0122, 'eval_samples_per_second': 22.192, 'eval_steps_per_second': 2.774, 'epoch': 1.0}
2025-05-20 23:11:06 {'loss': 0.8657, 'grad_norm': 10.210213661193848, 'learning_rate': 4.8875e-05, 'epoch': 0.025}
2025-05-20 23:11:20 {'loss': 0.9908, 'grad_norm': 12.382089614868164, 'learning_rate': 4.7625000000000006e-05, 'epoch': 0.05}
2025-05-20 23:11:35 {'loss': 1.0204, 'grad_norm': 10.797719955444336, 'learning_rate': 4.6375e-05, 'epoch': 0.075}
2025-05-20 23:11:56 {'loss': 1.0304, 'grad_norm': 12.49388599395752, 'learning_rate': 4.5125e-05, 'epoch': 0.1}
2025-05-20 23:12:10 {'loss': 1.0031, 'grad_norm': 9.287665367126465, 'learning_rate': 4.3875e-05, 'epoch': 0.125}
2025-05-20 23:12:25 {'loss': 1.1109, 'grad_norm': 11.337331771850586, 'learning_rate': 4.2625000000000006e-05, 'epoch': 0.15}
2025-05-20 23:12:40 {'loss': 0.9053, 'grad_norm': 9.200176239013672, 'learning_rate': 4.1375e-05, 'epoch': 0.175}
2025-05-20 23:12:54 {'loss': 1.0058, 'grad_norm': 8.43521499633789, 'learning_rate': 4.0125e-05, 'epoch': 0.2}
2025-05-20 23:13:15 {'loss': 0.9773, 'grad_norm': 10.216856002807617, 'learning_rate': 3.8875e-05, 'epoch': 0.225}
2025-05-20 23:13:30 {'loss': 1.0886, 'grad_norm': 12.868535995483398, 'learning_rate': 3.7625e-05, 'epoch': 0.25}
2025-05-20 23:13:44 {'loss': 1.0977, 'grad_norm': 10.04824161529541, 'learning_rate': 3.6375e-05, 'epoch': 0.275}
2025-05-20 23:13:59 {'loss': 1.0476, 'grad_norm': 10.627493858337402, 'learning_rate': 3.5125e-05, 'epoch': 0.3}
2025-05-20 23:14:14 {'loss': 1.1107, 'grad_norm': 12.632072448730469, 'learning_rate': 3.3875000000000003e-05, 'epoch': 0.325}
2025-05-20 23:14:34 {'loss': 1.0766, 'grad_norm': 10.142757415771484, 'learning_rate': 3.2625e-05, 'epoch': 0.35}
2025-05-20 23:14:49 {'loss': 1.0319, 'grad_norm': 11.635747909545898, 'learning_rate': 3.1375e-05, 'epoch': 0.375}
2025-05-20 23:15:04 {'loss': 1.0637, 'grad_norm': 9.984382629394531, 'learning_rate': 3.0125000000000004e-05, 'epoch': 0.4}
2025-05-20 23:15:18 {'loss': 1.0345, 'grad_norm': 8.660492897033691, 'learning_rate': 2.8875e-05, 'epoch': 0.425}
2025-05-20 23:15:33 {'loss': 1.0528, 'grad_norm': 10.36031723022461, 'learning_rate': 2.7625e-05, 'epoch': 0.45}
2025-05-20 23:15:48 {'loss': 1.4015, 'grad_norm': 18.913715362548828, 'learning_rate': 2.6375e-05, 'epoch': 0.475}
2025-05-20 23:16:09 {'loss': 1.243, 'grad_norm': 14.52741813659668, 'learning_rate': 2.5124999999999997e-05, 'epoch': 0.5}
2025-05-20 23:16:23 {'loss': 1.1975, 'grad_norm': 11.004883766174316, 'learning_rate': 2.3875e-05, 'epoch': 0.525}
2025-05-20 23:16:38 {'loss': 1.0119, 'grad_norm': 12.063363075256348, 'learning_rate': 2.2625e-05, 'epoch': 0.55}
2025-05-20 23:16:52 {'loss': 1.0251, 'grad_norm': 9.297499656677246, 'learning_rate': 2.1375e-05, 'epoch': 0.575}
2025-05-20 23:17:07 {'loss': 1.0995, 'grad_norm': 10.859943389892578, 'learning_rate': 2.0125e-05, 'epoch': 0.6}
2025-05-20 23:17:28 {'loss': 1.1219, 'grad_norm': 10.330802917480469, 'learning_rate': 1.8875e-05, 'epoch': 0.625}
2025-05-20 23:17:43 {'loss': 1.1882, 'grad_norm': 12.72608757019043, 'learning_rate': 1.7625e-05, 'epoch': 0.65}
2025-05-20 23:17:58 {'loss': 1.26, 'grad_norm': 11.108488082885742, 'learning_rate': 1.6375e-05, 'epoch': 0.675}
2025-05-20 23:18:13 {'loss': 1.2355, 'grad_norm': 14.22891902923584, 'learning_rate': 1.5125e-05, 'epoch': 0.7}
2025-05-20 23:18:28 {'loss': 1.2065, 'grad_norm': 9.864171981811523, 'learning_rate': 1.3875000000000002e-05, 'epoch': 0.725}
2025-05-20 23:18:49 {'loss': 1.1814, 'grad_norm': 12.320405006408691, 'learning_rate': 1.2625e-05, 'epoch': 0.75}
2025-05-20 23:19:04 {'loss': 1.2061, 'grad_norm': 10.013818740844727, 'learning_rate': 1.1375e-05, 'epoch': 0.775}
2025-05-20 23:19:18 {'loss': 1.3086, 'grad_norm': 8.338957786560059, 'learning_rate': 1.0125e-05, 'epoch': 0.8}
2025-05-20 23:19:33 {'loss': 1.1964, 'grad_norm': 11.82064437866211, 'learning_rate': 8.875e-06, 'epoch': 0.825}
2025-05-20 23:19:48 {'loss': 1.2367, 'grad_norm': 10.381199836730957, 'learning_rate': 7.625e-06, 'epoch': 0.85}
2025-05-20 23:20:02 {'loss': 1.2523, 'grad_norm': 11.581661224365234, 'learning_rate': 6.375000000000001e-06, 'epoch': 0.875}
2025-05-20 23:20:17 {'loss': 1.2998, 'grad_norm': 12.586067199707031, 'learning_rate': 5.125e-06, 'epoch': 0.9}
2025-05-20 23:20:38 {'loss': 1.3071, 'grad_norm': 9.543307304382324, 'learning_rate': 3.875e-06, 'epoch': 0.925}
2025-05-20 23:20:53 {'loss': 1.2116, 'grad_norm': 8.998209953308105, 'learning_rate': 2.625e-06, 'epoch': 0.95}
2025-05-20 23:21:08 {'loss': 1.1857, 'grad_norm': 10.687552452087402, 'learning_rate': 1.3750000000000002e-06, 'epoch': 0.975}
2025-05-20 23:21:22 {'loss': 0.8248, 'grad_norm': 10.262175559997559, 'learning_rate': 1.2500000000000002e-07, 'epoch': 1.0}
2025-05-20 23:21:22 {'train_runtime': 637.1221, 'train_samples_per_second': 1.256, 'train_steps_per_second': 0.628, 'train_loss': 1.117882990837097, 'epoch': 1.0}
2025-05-20 23:22:18 {'eval_loss': 1.2804142236709595, 'eval_runtime': 15.8123, 'eval_samples_per_second': 12.648, 'eval_steps_per_second': 1.581, 'epoch': 1.0}
2025-05-20 23:22:51 {'loss': 0.6115, 'grad_norm': 7.728077411651611, 'learning_rate': 4.8875e-05, 'epoch': 0.025}
2025-05-20 23:23:06 {'loss': 0.7149, 'grad_norm': 10.20044231414795, 'learning_rate': 4.7625000000000006e-05, 'epoch': 0.05}
2025-05-20 23:23:27 {'loss': 0.7804, 'grad_norm': 9.569857597351074, 'learning_rate': 4.6375e-05, 'epoch': 0.075}
2025-05-20 23:23:41 {'loss': 0.8056, 'grad_norm': 11.098546981811523, 'learning_rate': 4.5125e-05, 'epoch': 0.1}
2025-05-20 23:23:56 {'loss': 0.8128, 'grad_norm': 7.881398677825928, 'learning_rate': 4.3875e-05, 'epoch': 0.125}
2025-05-20 23:24:10 {'loss': 0.902, 'grad_norm': 11.322746276855469, 'learning_rate': 4.2625000000000006e-05, 'epoch': 0.15}
2025-05-20 23:24:25 {'loss': 0.7251, 'grad_norm': 9.315702438354492, 'learning_rate': 4.1375e-05, 'epoch': 0.175}
2025-05-20 23:24:39 {'loss': 0.8002, 'grad_norm': 8.148896217346191, 'learning_rate': 4.0125e-05, 'epoch': 0.2}
2025-05-20 23:25:00 {'loss': 0.7946, 'grad_norm': 9.061634063720703, 'learning_rate': 3.8875e-05, 'epoch': 0.225}
2025-05-20 23:25:15 {'loss': 0.9024, 'grad_norm': 11.841525077819824, 'learning_rate': 3.7625e-05, 'epoch': 0.25}
2025-05-20 23:25:29 {'loss': 0.8989, 'grad_norm': 11.307926177978516, 'learning_rate': 3.6375e-05, 'epoch': 0.275}
2025-05-20 23:25:44 {'loss': 0.8631, 'grad_norm': 9.65240478515625, 'learning_rate': 3.5125e-05, 'epoch': 0.3}
2025-05-20 23:25:58 {'loss': 0.9276, 'grad_norm': 13.548967361450195, 'learning_rate': 3.3875000000000003e-05, 'epoch': 0.325}
2025-05-20 23:26:19 {'loss': 0.8957, 'grad_norm': 10.430482864379883, 'learning_rate': 3.2625e-05, 'epoch': 0.35}
2025-05-20 23:26:33 {'loss': 0.8908, 'grad_norm': 11.899687767028809, 'learning_rate': 3.1375e-05, 'epoch': 0.375}
2025-05-20 23:26:48 {'loss': 0.8954, 'grad_norm': 9.615949630737305, 'learning_rate': 3.0125000000000004e-05, 'epoch': 0.4}
2025-05-20 23:27:02 {'loss': 0.8711, 'grad_norm': 12.26245403289795, 'learning_rate': 2.8875e-05, 'epoch': 0.425}
2025-05-20 23:27:17 {'loss': 0.9078, 'grad_norm': 14.952259063720703, 'learning_rate': 2.7625e-05, 'epoch': 0.45}
2025-05-20 23:27:38 {'loss': 1.2139, 'grad_norm': 16.688138961791992, 'learning_rate': 2.6375e-05, 'epoch': 0.475}
2025-05-20 23:27:52 {'loss': 1.0668, 'grad_norm': 16.1630916595459, 'learning_rate': 2.5124999999999997e-05, 'epoch': 0.5}
2025-05-20 23:28:07 {'loss': 1.0537, 'grad_norm': 12.0314302444458, 'learning_rate': 2.3875e-05, 'epoch': 0.525}
2025-05-20 23:28:21 {'loss': 0.8874, 'grad_norm': 11.05836009979248, 'learning_rate': 2.2625e-05, 'epoch': 0.55}
2025-05-20 23:28:36 {'loss': 0.9106, 'grad_norm': 9.33621597290039, 'learning_rate': 2.1375e-05, 'epoch': 0.575}
2025-05-20 23:28:56 {'loss': 0.9844, 'grad_norm': 11.97430419921875, 'learning_rate': 2.0125e-05, 'epoch': 0.6}
2025-05-20 23:29:11 {'loss': 1.0117, 'grad_norm': 9.621456146240234, 'learning_rate': 1.8875e-05, 'epoch': 0.625}
2025-05-20 23:29:26 {'loss': 1.0712, 'grad_norm': 12.649971008300781, 'learning_rate': 1.7625e-05, 'epoch': 0.65}
2025-05-20 23:29:40 {'loss': 1.1721, 'grad_norm': 11.690756797790527, 'learning_rate': 1.6375e-05, 'epoch': 0.675}
2025-05-20 23:29:55 {'loss': 1.1221, 'grad_norm': 16.155467987060547, 'learning_rate': 1.5125e-05, 'epoch': 0.7}
2025-05-20 23:30:10 {'loss': 1.1263, 'grad_norm': 10.897554397583008, 'learning_rate': 1.3875000000000002e-05, 'epoch': 0.725}
2025-05-20 23:30:31 {'loss': 1.0853, 'grad_norm': 12.408319473266602, 'learning_rate': 1.2625e-05, 'epoch': 0.75}
2025-05-20 23:30:46 {'loss': 1.1412, 'grad_norm': 10.418569564819336, 'learning_rate': 1.1375e-05, 'epoch': 0.775}
2025-05-20 23:31:00 {'loss': 1.2415, 'grad_norm': 8.977782249450684, 'learning_rate': 1.0125e-05, 'epoch': 0.8}
2025-05-20 23:31:15 {'loss': 1.1512, 'grad_norm': 11.2990140914917, 'learning_rate': 8.875e-06, 'epoch': 0.825}
2025-05-20 23:31:29 {'loss': 1.2008, 'grad_norm': 11.530643463134766, 'learning_rate': 7.625e-06, 'epoch': 0.85}
2025-05-20 23:31:44 {'loss': 1.2377, 'grad_norm': 10.976553916931152, 'learning_rate': 6.375000000000001e-06, 'epoch': 0.875}
2025-05-20 23:32:05 {'loss': 1.2816, 'grad_norm': 13.320792198181152, 'learning_rate': 5.125e-06, 'epoch': 0.9}
2025-05-20 23:32:19 {'loss': 1.2702, 'grad_norm': 11.025824546813965, 'learning_rate': 3.875e-06, 'epoch': 0.925}
2025-05-20 23:32:34 {'loss': 1.1532, 'grad_norm': 10.295973777770996, 'learning_rate': 2.625e-06, 'epoch': 0.95}
2025-05-20 23:32:49 {'loss': 1.1057, 'grad_norm': 9.569718360900879, 'learning_rate': 1.3750000000000002e-06, 'epoch': 0.975}
2025-05-20 23:33:03 {'loss': 0.698, 'grad_norm': 10.938387870788574, 'learning_rate': 1.2500000000000002e-07, 'epoch': 1.0}
2025-05-20 23:33:03 {'train_runtime': 635.5238, 'train_samples_per_second': 1.259, 'train_steps_per_second': 0.629, 'train_loss': 0.9796641695499421, 'epoch': 1.0}
2025-05-20 23:34:07 {'eval_loss': 1.3013970851898193, 'eval_runtime': 16.6556, 'eval_samples_per_second': 12.008, 'eval_steps_per_second': 1.501, 'epoch': 1.0}
2025-05-20 23:34:41 {'loss': 0.4338, 'grad_norm': 6.966341972351074, 'learning_rate': 4.8875e-05, 'epoch': 0.025}
2025-05-20 23:34:56 {'loss': 0.518, 'grad_norm': 9.269281387329102, 'learning_rate': 4.7625000000000006e-05, 'epoch': 0.05}
2025-05-20 23:35:17 {'loss': 0.6112, 'grad_norm': 9.218218803405762, 'learning_rate': 4.6375e-05, 'epoch': 0.075}
2025-05-20 23:35:31 {'loss': 0.6335, 'grad_norm': 16.9802303314209, 'learning_rate': 4.5125e-05, 'epoch': 0.1}
2025-05-20 23:35:46 {'loss': 0.63, 'grad_norm': 7.075717449188232, 'learning_rate': 4.3875e-05, 'epoch': 0.125}
2025-05-20 23:36:00 {'loss': 0.7285, 'grad_norm': 10.796363830566406, 'learning_rate': 4.2625000000000006e-05, 'epoch': 0.15}
2025-05-20 23:36:21 {'loss': 0.591, 'grad_norm': 8.180428504943848, 'learning_rate': 4.1375e-05, 'epoch': 0.175}
2025-05-20 23:36:36 {'loss': 0.6457, 'grad_norm': 7.9847025871276855, 'learning_rate': 4.0125e-05, 'epoch': 0.2}
2025-05-20 23:36:50 {'loss': 0.657, 'grad_norm': 8.830182075500488, 'learning_rate': 3.8875e-05, 'epoch': 0.225}
2025-05-20 23:37:05 {'loss': 0.7464, 'grad_norm': 11.327420234680176, 'learning_rate': 3.7625e-05, 'epoch': 0.25}
2025-05-20 23:37:20 {'loss': 0.765, 'grad_norm': 11.080843925476074, 'learning_rate': 3.6375e-05, 'epoch': 0.275}
2025-05-20 23:37:41 {'loss': 0.7254, 'grad_norm': 8.605554580688477, 'learning_rate': 3.5125e-05, 'epoch': 0.3}
2025-05-20 23:37:55 {'loss': 0.8051, 'grad_norm': 10.014032363891602, 'learning_rate': 3.3875000000000003e-05, 'epoch': 0.325}
2025-05-20 23:38:10 {'loss': 0.7656, 'grad_norm': 11.03598403930664, 'learning_rate': 3.2625e-05, 'epoch': 0.35}
2025-05-20 23:38:24 {'loss': 0.7606, 'grad_norm': 11.275432586669922, 'learning_rate': 3.1375e-05, 'epoch': 0.375}
2025-05-20 23:38:39 {'loss': 0.7626, 'grad_norm': 9.776992797851562, 'learning_rate': 3.0125000000000004e-05, 'epoch': 0.4}
2025-05-20 23:38:59 {'loss': 0.7862, 'grad_norm': 12.458795547485352, 'learning_rate': 2.8875e-05, 'epoch': 0.425}
2025-05-20 23:39:14 {'loss': 0.8004, 'grad_norm': 10.928690910339355, 'learning_rate': 2.7625e-05, 'epoch': 0.45}
2025-05-20 23:39:29 {'loss': 1.0586, 'grad_norm': 15.904935836791992, 'learning_rate': 2.6375e-05, 'epoch': 0.475}
2025-05-20 23:39:44 {'loss': 0.9407, 'grad_norm': 15.930944442749023, 'learning_rate': 2.5124999999999997e-05, 'epoch': 0.5}
2025-05-20 23:39:59 {'loss': 0.9155, 'grad_norm': 13.449831008911133, 'learning_rate': 2.3875e-05, 'epoch': 0.525}
2025-05-20 23:40:19 {'loss': 0.7963, 'grad_norm': 10.762216567993164, 'learning_rate': 2.2625e-05, 'epoch': 0.55}
2025-05-20 23:40:34 {'loss': 0.8074, 'grad_norm': 9.500200271606445, 'learning_rate': 2.1375e-05, 'epoch': 0.575}
2025-05-20 23:40:48 {'loss': 0.9122, 'grad_norm': 11.723003387451172, 'learning_rate': 2.0125e-05, 'epoch': 0.6}
2025-05-20 23:41:03 {'loss': 0.9232, 'grad_norm': 10.44594669342041, 'learning_rate': 1.8875e-05, 'epoch': 0.625}
2025-05-20 23:41:18 {'loss': 0.9812, 'grad_norm': 12.055641174316406, 'learning_rate': 1.7625e-05, 'epoch': 0.65}
2025-05-20 23:41:32 {'loss': 1.0826, 'grad_norm': 10.653623580932617, 'learning_rate': 1.6375e-05, 'epoch': 0.675}
2025-05-20 23:41:54 {'loss': 1.0419, 'grad_norm': 17.016218185424805, 'learning_rate': 1.5125e-05, 'epoch': 0.7}
2025-05-20 23:42:08 {'loss': 1.0617, 'grad_norm': 10.210477828979492, 'learning_rate': 1.3875000000000002e-05, 'epoch': 0.725}
2025-05-20 23:42:23 {'loss': 1.0479, 'grad_norm': 12.666207313537598, 'learning_rate': 1.2625e-05, 'epoch': 0.75}
2025-05-20 23:42:37 {'loss': 1.093, 'grad_norm': 10.846309661865234, 'learning_rate': 1.1375e-05, 'epoch': 0.775}
2025-05-20 23:42:52 {'loss': 1.1771, 'grad_norm': 9.572707176208496, 'learning_rate': 1.0125e-05, 'epoch': 0.8}
2025-05-20 23:43:13 {'loss': 1.1181, 'grad_norm': 9.9727783203125, 'learning_rate': 8.875e-06, 'epoch': 0.825}
2025-05-20 23:43:28 {'loss': 1.1495, 'grad_norm': 10.61838436126709, 'learning_rate': 7.625e-06, 'epoch': 0.85}
2025-05-20 23:43:42 {'loss': 1.2154, 'grad_norm': 10.81649398803711, 'learning_rate': 6.375000000000001e-06, 'epoch': 0.875}
2025-05-20 23:43:57 {'loss': 1.2552, 'grad_norm': 13.553102493286133, 'learning_rate': 5.125e-06, 'epoch': 0.9}
2025-05-20 23:44:12 {'loss': 1.2386, 'grad_norm': 10.480167388916016, 'learning_rate': 3.875e-06, 'epoch': 0.925}
2025-05-20 23:44:33 {'loss': 1.1264, 'grad_norm': 9.648447036743164, 'learning_rate': 2.625e-06, 'epoch': 0.95}
2025-05-20 23:44:47 {'loss': 1.0235, 'grad_norm': 9.836942672729492, 'learning_rate': 1.3750000000000002e-06, 'epoch': 0.975}
2025-05-20 23:44:59 {'loss': 0.6323, 'grad_norm': 10.853080749511719, 'learning_rate': 1.2500000000000002e-07, 'epoch': 1.0}
2025-05-20 23:44:59 {'train_runtime': 637.9806, 'train_samples_per_second': 1.254, 'train_steps_per_second': 0.627, 'train_loss': 0.8741091501712799, 'epoch': 1.0}
2025-05-20 23:45:42 {'eval_loss': 1.328070044517517, 'eval_runtime': 2.5219, 'eval_samples_per_second': 79.305, 'eval_steps_per_second': 9.913, 'epoch': 1.0}
2025-05-20 23:46:22 {'loss': 0.3044, 'grad_norm': 8.202683448791504, 'learning_rate': 4.8875e-05, 'epoch': 0.025}
2025-05-20 23:46:37 {'loss': 0.3779, 'grad_norm': 14.340445518493652, 'learning_rate': 4.7625000000000006e-05, 'epoch': 0.05}
2025-05-20 23:46:58 {'loss': 0.4657, 'grad_norm': 9.323668479919434, 'learning_rate': 4.6375e-05, 'epoch': 0.075}
2025-05-20 23:47:12 {'loss': 0.5086, 'grad_norm': 11.63489818572998, 'learning_rate': 4.5125e-05, 'epoch': 0.1}
2025-05-20 23:47:27 {'loss': 0.519, 'grad_norm': 6.489085674285889, 'learning_rate': 4.3875e-05, 'epoch': 0.125}
2025-05-20 23:47:42 {'loss': 0.5714, 'grad_norm': 9.999792098999023, 'learning_rate': 4.2625000000000006e-05, 'epoch': 0.15}
2025-05-20 23:48:02 {'loss': 0.484, 'grad_norm': 8.060674667358398, 'learning_rate': 4.1375e-05, 'epoch': 0.175}
2025-05-20 23:48:17 {'loss': 0.5335, 'grad_norm': 8.616730690002441, 'learning_rate': 4.0125e-05, 'epoch': 0.2}
2025-05-20 23:48:31 {'loss': 0.5412, 'grad_norm': 7.0527119636535645, 'learning_rate': 3.8875e-05, 'epoch': 0.225}
2025-05-20 23:48:46 {'loss': 0.6228, 'grad_norm': 13.306285858154297, 'learning_rate': 3.7625e-05, 'epoch': 0.25}
2025-05-20 23:49:01 {'loss': 0.6285, 'grad_norm': 8.390660285949707, 'learning_rate': 3.6375e-05, 'epoch': 0.275}
2025-05-20 23:49:22 {'loss': 0.6327, 'grad_norm': 10.43660831451416, 'learning_rate': 3.5125e-05, 'epoch': 0.3}
2025-05-20 23:49:37 {'loss': 0.6565, 'grad_norm': 10.773643493652344, 'learning_rate': 3.3875000000000003e-05, 'epoch': 0.325}
2025-05-20 23:49:51 {'loss': 0.6429, 'grad_norm': 10.403911590576172, 'learning_rate': 3.2625e-05, 'epoch': 0.35}
2025-05-20 23:50:06 {'loss': 0.6323, 'grad_norm': 11.758662223815918, 'learning_rate': 3.1375e-05, 'epoch': 0.375}
2025-05-20 23:50:20 {'loss': 0.6466, 'grad_norm': 8.935632705688477, 'learning_rate': 3.0125000000000004e-05, 'epoch': 0.4}
2025-05-20 23:50:35 {'loss': 0.6686, 'grad_norm': 10.912370681762695, 'learning_rate': 2.8875e-05, 'epoch': 0.425}
2025-05-20 23:50:56 {'loss': 0.6741, 'grad_norm': 11.95406723022461, 'learning_rate': 2.7625e-05, 'epoch': 0.45}
2025-05-20 23:51:11 {'loss': 0.9483, 'grad_norm': 17.666820526123047, 'learning_rate': 2.6375e-05, 'epoch': 0.475}
2025-05-20 23:51:25 {'loss': 0.8217, 'grad_norm': 13.052643775939941, 'learning_rate': 2.5124999999999997e-05, 'epoch': 0.5}
2025-05-20 23:51:40 {'loss': 0.8292, 'grad_norm': 12.069938659667969, 'learning_rate': 2.3875e-05, 'epoch': 0.525}
2025-05-20 23:51:54 {'loss': 0.712, 'grad_norm': 11.66739559173584, 'learning_rate': 2.2625e-05, 'epoch': 0.55}
2025-05-20 23:52:15 {'loss': 0.723, 'grad_norm': 10.586044311523438, 'learning_rate': 2.1375e-05, 'epoch': 0.575}
2025-05-20 23:52:30 {'loss': 0.8224, 'grad_norm': 12.060876846313477, 'learning_rate': 2.0125e-05, 'epoch': 0.6}
2025-05-20 23:52:44 {'loss': 0.8187, 'grad_norm': 8.363102912902832, 'learning_rate': 1.8875e-05, 'epoch': 0.625}
2025-05-20 23:52:59 {'loss': 0.885, 'grad_norm': 13.094632148742676, 'learning_rate': 1.7625e-05, 'epoch': 0.65}
2025-05-20 23:53:14 {'loss': 0.9916, 'grad_norm': 10.385615348815918, 'learning_rate': 1.6375e-05, 'epoch': 0.675}
2025-05-20 23:53:34 {'loss': 0.9778, 'grad_norm': 14.37473201751709, 'learning_rate': 1.5125e-05, 'epoch': 0.7}
2025-05-20 23:53:49 {'loss': 0.9775, 'grad_norm': 10.604422569274902, 'learning_rate': 1.3875000000000002e-05, 'epoch': 0.725}
2025-05-20 23:54:04 {'loss': 0.9885, 'grad_norm': 12.54092788696289, 'learning_rate': 1.2625e-05, 'epoch': 0.75}
2025-05-20 23:54:19 {'loss': 1.0426, 'grad_norm': 10.319331169128418, 'learning_rate': 1.1375e-05, 'epoch': 0.775}
2025-05-20 23:54:33 {'loss': 1.1214, 'grad_norm': 8.65898323059082, 'learning_rate': 1.0125e-05, 'epoch': 0.8}
2025-05-20 23:54:48 {'loss': 1.064, 'grad_norm': 10.773058891296387, 'learning_rate': 8.875e-06, 'epoch': 0.825}
2025-05-20 23:55:08 {'loss': 1.1271, 'grad_norm': 11.795721054077148, 'learning_rate': 7.625e-06, 'epoch': 0.85}
2025-05-20 23:55:23 {'loss': 1.1879, 'grad_norm': 11.593695640563965, 'learning_rate': 6.375000000000001e-06, 'epoch': 0.875}
2025-05-20 23:55:37 {'loss': 1.2229, 'grad_norm': 12.655503273010254, 'learning_rate': 5.125e-06, 'epoch': 0.9}
2025-05-20 23:55:52 {'loss': 1.2284, 'grad_norm': 11.089410781860352, 'learning_rate': 3.875e-06, 'epoch': 0.925}
2025-05-20 23:56:07 {'loss': 1.0884, 'grad_norm': 9.519998550415039, 'learning_rate': 2.625e-06, 'epoch': 0.95}
2025-05-20 23:56:21 {'loss': 0.977, 'grad_norm': 11.911577224731445, 'learning_rate': 1.3750000000000002e-06, 'epoch': 0.975}
2025-05-20 23:56:35 {'loss': 0.5364, 'grad_norm': 10.28929615020752, 'learning_rate': 1.2500000000000002e-07, 'epoch': 1.0}
2025-05-20 23:56:35 {'train_runtime': 633.1901, 'train_samples_per_second': 1.263, 'train_steps_per_second': 0.632, 'train_loss': 0.7800655335187912, 'epoch': 1.0}
2025-05-20 23:57:30 {'eval_loss': 1.3523417711257935, 'eval_runtime': 8.0591, 'eval_samples_per_second': 24.817, 'eval_steps_per_second': 3.102, 'epoch': 1.0}
2025-05-20 23:10:03 INFO :      Sent reply
2025-05-20 23:10:23 INFO :      
2025-05-20 23:10:23 INFO :      Received: evaluate message a89f5f5c-18ab-4254-bf18-86fcc9f90fb1
2025-05-20 23:10:34 INFO :      Sent reply
2025-05-20 23:10:41 INFO :      
2025-05-20 23:10:41 INFO :      Received: train message 7e34ee73-effb-4bf9-9257-6f4e2fb88b2e
2025-05-20 23:21:31 INFO :      Sent reply
2025-05-20 23:21:59 INFO :      
2025-05-20 23:21:59 INFO :      Received: evaluate message 4b7dad98-a2b5-4d00-a5c1-5f4ddf0e4bae
2025-05-20 23:22:18 INFO :      Sent reply
2025-05-20 23:22:25 INFO :      
2025-05-20 23:22:25 INFO :      Received: train message a1267dc5-ec12-4544-a968-2f83bc92a8af
2025-05-20 23:33:16 INFO :      Sent reply
2025-05-20 23:33:49 INFO :      
2025-05-20 23:33:49 INFO :      Received: evaluate message 227fb0ee-1e88-4e18-a23e-2091570513ea
2025-05-20 23:34:07 INFO :      Sent reply
2025-05-20 23:34:19 INFO :      
2025-05-20 23:34:19 INFO :      Received: train message c9b630f8-e800-4b00-bf75-2a1e7444a893
2025-05-20 23:45:05 INFO :      Sent reply
2025-05-20 23:45:38 INFO :      
2025-05-20 23:45:38 INFO :      Received: evaluate message e0085b44-69d7-4c46-9e0c-bedd3d3b31c6
2025-05-20 23:45:42 INFO :      Sent reply
2025-05-20 23:45:59 INFO :      
2025-05-20 23:45:59 INFO :      Received: train message f56a4f64-0466-499d-9607-30d2795ff47f
2025-05-20 23:56:44 INFO :      Sent reply
2025-05-20 23:57:18 INFO :      
2025-05-20 23:57:18 INFO :      Received: evaluate message 11cb566e-a89a-4fd4-ab75-79f7ca966d99
2025-05-20 23:57:30 INFO :      Sent reply
2025-05-20 23:57:30 INFO :      
2025-05-20 23:57:30 INFO :      Received: reconnect message 717157f4-8e11-438f-b023-b6799c97b3e1
2025-05-20 23:57:30 INFO :      Disconnect and shut down
