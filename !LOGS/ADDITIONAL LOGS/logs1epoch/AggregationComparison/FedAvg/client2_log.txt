2025-05-19 21:41:07 
Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]
Generating train split: 100%|██████████| 120000/120000 [00:00<00:00, 1096308.83 examples/s]
Generating train split: 100%|██████████| 120000/120000 [00:00<00:00, 1093372.38 examples/s]
2025-05-19 21:41:07 
Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]
Generating test split: 100%|██████████| 7600/7600 [00:00<00:00, 958611.57 examples/s]
2025-05-19 21:41:09 
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 1348.46 examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 1342.68 examples/s]
2025-05-19 21:41:09 
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]
Map:  34%|███▍      | 338/1000 [00:00<00:00, 3343.68 examples/s]
Map:  70%|███████   | 705/1000 [00:00<00:00, 3520.11 examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 3155.02 examples/s]
2025-05-19 21:41:09 /app/client.py:49: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
2025-05-19 21:41:09   trainer = Trainer(
2025-05-19 21:41:10 WARNING :   DEPRECATED FEATURE: flwr.client.start_numpy_client() is deprecated. 
2025-05-19 21:41:10 Instead, use `flwr.client.start_client()` by ensuring you first call the `.to_client()` method as shown below: 
2025-05-19 21:41:10 flwr.client.start_client(
2025-05-19 21:41:10 server_address='<IP>:<PORT>',
2025-05-19 21:41:10 client=FlowerClient().to_client(), # <-- where FlowerClient is of type flwr.client.NumPyClient object
2025-05-19 21:41:10 )
2025-05-19 21:41:10 Using `start_numpy_client()` is deprecated.
2025-05-19 21:41:10 
2025-05-19 21:41:10             This is a deprecated feature. It will be removed
2025-05-19 21:41:10             entirely in future versions of Flower.
2025-05-19 21:41:10         
2025-05-19 21:41:10 WARNING :   DEPRECATED FEATURE: flwr.client.start_client() is deprecated.
2025-05-19 21:41:10 Instead, use the `flower-supernode` CLI command to start a SuperNode as shown below:
2025-05-19 21:41:10 
2025-05-19 21:41:10 $ flower-supernode --insecure --superlink='<IP>:<PORT>'
2025-05-19 21:41:10 
2025-05-19 21:41:10 To view all available options, run:
2025-05-19 21:41:10 
2025-05-19 21:41:10 $ flower-supernode --help
2025-05-19 21:41:10 
2025-05-19 21:41:10 Using `start_client()` is deprecated.
2025-05-19 21:41:10 
2025-05-19 21:41:10             This is a deprecated feature. It will be removed
2025-05-19 21:41:10             entirely in future versions of Flower.
2025-05-19 21:41:10         
2025-05-19 21:41:17 INFO :      
2025-05-19 21:41:17 INFO :      Received: train message c12235d9-3e3e-48c1-b0c7-6c41cfee9af1
2025-05-19 21:41:34 {'loss': 2.601, 'grad_norm': 12.700850486755371, 'learning_rate': 4.8875e-05, 'epoch': 0.025}
2025-05-19 21:41:44 {'loss': 1.701, 'grad_norm': 15.263579368591309, 'learning_rate': 4.7625000000000006e-05, 'epoch': 0.05}
2025-05-19 21:41:58 {'loss': 1.5413, 'grad_norm': 14.924647331237793, 'learning_rate': 4.6375e-05, 'epoch': 0.075}
2025-05-19 21:42:08 {'loss': 1.5084, 'grad_norm': 11.737829208374023, 'learning_rate': 4.5125e-05, 'epoch': 0.1}
2025-05-19 21:42:18 {'loss': 1.4867, 'grad_norm': 13.93914794921875, 'learning_rate': 4.3875e-05, 'epoch': 0.125}
2025-05-19 21:42:28 {'loss': 1.4997, 'grad_norm': 14.920110702514648, 'learning_rate': 4.2625000000000006e-05, 'epoch': 0.15}
2025-05-19 21:42:41 {'loss': 1.4803, 'grad_norm': 14.05017375946045, 'learning_rate': 4.1375e-05, 'epoch': 0.175}
2025-05-19 21:42:51 {'loss': 1.6589, 'grad_norm': 11.854897499084473, 'learning_rate': 4.0125e-05, 'epoch': 0.2}
2025-05-19 21:43:01 {'loss': 1.4721, 'grad_norm': 16.295398712158203, 'learning_rate': 3.8875e-05, 'epoch': 0.225}
2025-05-19 21:43:15 {'loss': 1.5535, 'grad_norm': 17.57485580444336, 'learning_rate': 3.7625e-05, 'epoch': 0.25}
2025-05-19 21:43:25 {'loss': 1.4841, 'grad_norm': 12.247942924499512, 'learning_rate': 3.6375e-05, 'epoch': 0.275}
2025-05-19 21:43:35 {'loss': 1.5989, 'grad_norm': 13.974462509155273, 'learning_rate': 3.5125e-05, 'epoch': 0.3}
2025-05-19 21:43:45 {'loss': 1.4312, 'grad_norm': 14.314990043640137, 'learning_rate': 3.3875000000000003e-05, 'epoch': 0.325}
2025-05-19 21:43:59 {'loss': 1.4187, 'grad_norm': 13.82353401184082, 'learning_rate': 3.2625e-05, 'epoch': 0.35}
2025-05-19 21:44:09 {'loss': 1.4701, 'grad_norm': 13.539532661437988, 'learning_rate': 3.1375e-05, 'epoch': 0.375}
2025-05-19 21:44:18 {'loss': 1.3575, 'grad_norm': 10.871011734008789, 'learning_rate': 3.0125000000000004e-05, 'epoch': 0.4}
2025-05-19 21:44:32 {'loss': 1.4066, 'grad_norm': 12.065603256225586, 'learning_rate': 2.8875e-05, 'epoch': 0.425}
2025-05-19 21:44:42 {'loss': 1.412, 'grad_norm': 13.48873233795166, 'learning_rate': 2.7625e-05, 'epoch': 0.45}
2025-05-19 21:44:52 {'loss': 1.3088, 'grad_norm': 11.071293830871582, 'learning_rate': 2.6375e-05, 'epoch': 0.475}
2025-05-19 21:45:06 {'loss': 1.6416, 'grad_norm': 11.690345764160156, 'learning_rate': 2.5124999999999997e-05, 'epoch': 0.5}
2025-05-19 21:45:16 {'loss': 1.287, 'grad_norm': 11.70107364654541, 'learning_rate': 2.3875e-05, 'epoch': 0.525}
2025-05-19 21:45:26 {'loss': 1.2886, 'grad_norm': 17.37563705444336, 'learning_rate': 2.2625e-05, 'epoch': 0.55}
2025-05-19 21:45:36 {'loss': 1.3883, 'grad_norm': 13.114779472351074, 'learning_rate': 2.1375e-05, 'epoch': 0.575}
2025-05-19 21:45:50 {'loss': 1.6138, 'grad_norm': 9.251484870910645, 'learning_rate': 2.0125e-05, 'epoch': 0.6}
2025-05-19 21:46:00 {'loss': 1.3731, 'grad_norm': 11.324997901916504, 'learning_rate': 1.8875e-05, 'epoch': 0.625}
2025-05-19 21:46:10 {'loss': 1.5071, 'grad_norm': 13.083330154418945, 'learning_rate': 1.7625e-05, 'epoch': 0.65}
2025-05-19 21:46:20 {'loss': 1.3798, 'grad_norm': 13.587925910949707, 'learning_rate': 1.6375e-05, 'epoch': 0.675}
2025-05-19 21:46:34 {'loss': 1.346, 'grad_norm': 10.428629875183105, 'learning_rate': 1.5125e-05, 'epoch': 0.7}
2025-05-19 21:46:44 {'loss': 1.3275, 'grad_norm': 9.480512619018555, 'learning_rate': 1.3875000000000002e-05, 'epoch': 0.725}
2025-05-19 21:46:54 {'loss': 1.3643, 'grad_norm': 13.955072402954102, 'learning_rate': 1.2625e-05, 'epoch': 0.75}
2025-05-19 21:47:07 {'loss': 1.3997, 'grad_norm': 9.311558723449707, 'learning_rate': 1.1375e-05, 'epoch': 0.775}
2025-05-19 21:47:17 {'loss': 1.4925, 'grad_norm': 11.113691329956055, 'learning_rate': 1.0125e-05, 'epoch': 0.8}
2025-05-19 21:47:27 {'loss': 1.592, 'grad_norm': 10.390338897705078, 'learning_rate': 8.875e-06, 'epoch': 0.825}
2025-05-19 21:47:37 {'loss': 1.5369, 'grad_norm': 13.552906036376953, 'learning_rate': 7.625e-06, 'epoch': 0.85}
2025-05-19 21:47:51 {'loss': 1.2543, 'grad_norm': 11.043596267700195, 'learning_rate': 6.375000000000001e-06, 'epoch': 0.875}
2025-05-19 21:48:01 {'loss': 1.2883, 'grad_norm': 10.747010231018066, 'learning_rate': 5.125e-06, 'epoch': 0.9}
2025-05-19 21:48:10 {'loss': 1.3394, 'grad_norm': 14.550005912780762, 'learning_rate': 3.875e-06, 'epoch': 0.925}
2025-05-19 21:48:24 {'loss': 1.3901, 'grad_norm': 15.557666778564453, 'learning_rate': 2.625e-06, 'epoch': 0.95}
2025-05-19 21:48:34 {'loss': 1.4073, 'grad_norm': 14.270679473876953, 'learning_rate': 1.3750000000000002e-06, 'epoch': 0.975}
2025-05-19 21:48:44 {'loss': 1.4331, 'grad_norm': 13.249505043029785, 'learning_rate': 1.2500000000000002e-07, 'epoch': 1.0}
2025-05-19 21:48:44 {'train_runtime': 445.575, 'train_samples_per_second': 1.795, 'train_steps_per_second': 0.898, 'train_loss': 1.4760309290885925, 'epoch': 1.0}
2025-05-19 21:49:18 {'eval_loss': 1.2957048416137695, 'eval_runtime': 11.4907, 'eval_samples_per_second': 17.405, 'eval_steps_per_second': 2.176, 'epoch': 1.0}
2025-05-19 21:49:37 {'loss': 0.8169, 'grad_norm': 9.148890495300293, 'learning_rate': 4.8875e-05, 'epoch': 0.025}
2025-05-19 21:49:50 {'loss': 1.0207, 'grad_norm': 10.021646499633789, 'learning_rate': 4.7625000000000006e-05, 'epoch': 0.05}
2025-05-19 21:50:00 {'loss': 1.0093, 'grad_norm': 13.605888366699219, 'learning_rate': 4.6375e-05, 'epoch': 0.075}
2025-05-19 21:50:10 {'loss': 0.9907, 'grad_norm': 9.3651704788208, 'learning_rate': 4.5125e-05, 'epoch': 0.1}
2025-05-19 21:50:20 {'loss': 0.9801, 'grad_norm': 11.05280876159668, 'learning_rate': 4.3875e-05, 'epoch': 0.125}
2025-05-19 21:50:33 {'loss': 1.0073, 'grad_norm': 11.444302558898926, 'learning_rate': 4.2625000000000006e-05, 'epoch': 0.15}
2025-05-19 21:50:43 {'loss': 1.0411, 'grad_norm': 11.465323448181152, 'learning_rate': 4.1375e-05, 'epoch': 0.175}
2025-05-19 21:50:53 {'loss': 1.1974, 'grad_norm': 11.063536643981934, 'learning_rate': 4.0125e-05, 'epoch': 0.2}
2025-05-19 21:51:03 {'loss': 1.0706, 'grad_norm': 12.20649242401123, 'learning_rate': 3.8875e-05, 'epoch': 0.225}
2025-05-19 21:51:16 {'loss': 1.0939, 'grad_norm': 12.82288646697998, 'learning_rate': 3.7625e-05, 'epoch': 0.25}
2025-05-19 21:51:26 {'loss': 1.0776, 'grad_norm': 10.486796379089355, 'learning_rate': 3.6375e-05, 'epoch': 0.275}
2025-05-19 21:51:40 {'loss': 1.1745, 'grad_norm': 11.80184555053711, 'learning_rate': 3.5125e-05, 'epoch': 0.3}
2025-05-19 21:51:50 {'loss': 1.0292, 'grad_norm': 11.449085235595703, 'learning_rate': 3.3875000000000003e-05, 'epoch': 0.325}
2025-05-19 21:52:04 {'loss': 1.0372, 'grad_norm': 13.903372764587402, 'learning_rate': 3.2625e-05, 'epoch': 0.35}
2025-05-19 21:52:18 {'loss': 1.113, 'grad_norm': 10.999345779418945, 'learning_rate': 3.1375e-05, 'epoch': 0.375}
2025-05-19 21:52:28 {'loss': 1.0203, 'grad_norm': 9.980940818786621, 'learning_rate': 3.0125000000000004e-05, 'epoch': 0.4}
2025-05-19 21:52:42 {'loss': 1.0657, 'grad_norm': 10.661231994628906, 'learning_rate': 2.8875e-05, 'epoch': 0.425}
2025-05-19 21:52:52 {'loss': 1.0936, 'grad_norm': 12.393813133239746, 'learning_rate': 2.7625e-05, 'epoch': 0.45}
2025-05-19 21:53:02 {'loss': 1.0281, 'grad_norm': 10.617676734924316, 'learning_rate': 2.6375e-05, 'epoch': 0.475}
2025-05-19 21:53:16 {'loss': 1.3161, 'grad_norm': 9.173596382141113, 'learning_rate': 2.5124999999999997e-05, 'epoch': 0.5}
2025-05-19 21:53:25 {'loss': 1.0057, 'grad_norm': 11.037248611450195, 'learning_rate': 2.3875e-05, 'epoch': 0.525}
2025-05-19 21:53:35 {'loss': 1.0362, 'grad_norm': 11.103860855102539, 'learning_rate': 2.2625e-05, 'epoch': 0.55}
2025-05-19 21:53:45 {'loss': 1.1123, 'grad_norm': 11.225512504577637, 'learning_rate': 2.1375e-05, 'epoch': 0.575}
2025-05-19 21:53:58 {'loss': 1.3076, 'grad_norm': 9.476460456848145, 'learning_rate': 2.0125e-05, 'epoch': 0.6}
2025-05-19 21:54:08 {'loss': 1.1002, 'grad_norm': 10.445063591003418, 'learning_rate': 1.8875e-05, 'epoch': 0.625}
2025-05-19 21:54:18 {'loss': 1.2723, 'grad_norm': 12.930829048156738, 'learning_rate': 1.7625e-05, 'epoch': 0.65}
2025-05-19 21:54:27 {'loss': 1.1594, 'grad_norm': 11.111831665039062, 'learning_rate': 1.6375e-05, 'epoch': 0.675}
2025-05-19 21:54:41 {'loss': 1.1143, 'grad_norm': 9.424970626831055, 'learning_rate': 1.5125e-05, 'epoch': 0.7}
2025-05-19 21:54:51 {'loss': 1.1413, 'grad_norm': 10.92104434967041, 'learning_rate': 1.3875000000000002e-05, 'epoch': 0.725}
2025-05-19 21:55:00 {'loss': 1.1574, 'grad_norm': 13.99210262298584, 'learning_rate': 1.2625e-05, 'epoch': 0.75}
2025-05-19 21:55:10 {'loss': 1.2016, 'grad_norm': 9.174077987670898, 'learning_rate': 1.1375e-05, 'epoch': 0.775}
2025-05-19 21:55:24 {'loss': 1.3334, 'grad_norm': 11.26921558380127, 'learning_rate': 1.0125e-05, 'epoch': 0.8}
2025-05-19 21:55:33 {'loss': 1.4088, 'grad_norm': 11.80623722076416, 'learning_rate': 8.875e-06, 'epoch': 0.825}
2025-05-19 21:55:43 {'loss': 1.4047, 'grad_norm': 13.111936569213867, 'learning_rate': 7.625e-06, 'epoch': 0.85}
2025-05-19 21:55:57 {'loss': 1.1555, 'grad_norm': 10.67598819732666, 'learning_rate': 6.375000000000001e-06, 'epoch': 0.875}
2025-05-19 21:56:06 {'loss': 1.1765, 'grad_norm': 9.604667663574219, 'learning_rate': 5.125e-06, 'epoch': 0.9}
2025-05-19 21:56:16 {'loss': 1.2066, 'grad_norm': 13.42385196685791, 'learning_rate': 3.875e-06, 'epoch': 0.925}
2025-05-19 21:56:26 {'loss': 1.2385, 'grad_norm': 14.328103065490723, 'learning_rate': 2.625e-06, 'epoch': 0.95}
2025-05-19 21:56:39 {'loss': 1.144, 'grad_norm': 11.361928939819336, 'learning_rate': 1.3750000000000002e-06, 'epoch': 0.975}
2025-05-19 21:56:45 {'loss': 0.8552, 'grad_norm': 8.67873477935791, 'learning_rate': 1.2500000000000002e-07, 'epoch': 1.0}
2025-05-19 21:56:45 {'train_runtime': 439.6118, 'train_samples_per_second': 1.82, 'train_steps_per_second': 0.91, 'train_loss': 1.1178684043884277, 'epoch': 1.0}
2025-05-19 21:57:07 {'eval_loss': 1.315976858139038, 'eval_runtime': 6.9868, 'eval_samples_per_second': 28.626, 'eval_steps_per_second': 3.578, 'epoch': 1.0}
2025-05-19 21:57:25 {'loss': 0.4966, 'grad_norm': 8.323256492614746, 'learning_rate': 4.8875e-05, 'epoch': 0.025}
2025-05-19 21:57:38 {'loss': 0.6726, 'grad_norm': 12.173890113830566, 'learning_rate': 4.7625000000000006e-05, 'epoch': 0.05}
2025-05-19 21:57:48 {'loss': 0.6887, 'grad_norm': 11.783048629760742, 'learning_rate': 4.6375e-05, 'epoch': 0.075}
2025-05-19 21:57:58 {'loss': 0.6687, 'grad_norm': 9.960758209228516, 'learning_rate': 4.5125e-05, 'epoch': 0.1}
2025-05-19 21:48:52 INFO :      Sent reply
2025-05-19 21:49:04 INFO :      
2025-05-19 21:49:04 INFO :      Received: evaluate message dcb6e2df-fad7-4383-ae10-4560b030e2bd
2025-05-19 21:49:18 INFO :      Sent reply
2025-05-19 21:49:22 INFO :      
2025-05-19 21:49:22 INFO :      Received: train message 0a394a7a-0ee9-4770-8e61-f8b226e0dc90
2025-05-19 21:56:48 INFO :      Sent reply
2025-05-19 21:56:59 INFO :      
2025-05-19 21:56:59 INFO :      Received: evaluate message ea4e7c94-9686-45b0-a7dd-9d08dc955be9
2025-05-19 21:57:07 INFO :      Sent reply
2025-05-19 21:57:14 INFO :      
2025-05-19 21:57:14 INFO :      Received: train message 1c6b448a-d6e0-4740-9b65-d5680c246bc6
2025-05-19 22:04:34 INFO :      Sent reply
2025-05-19 22:04:45 INFO :      
2025-05-19 22:04:45 INFO :      Received: evaluate message 3b435e19-a4fa-40b6-8f53-6e70f0c5c2e2
2025-05-19 22:04:57 INFO :      Sent reply
2025-05-19 22:05:02 INFO :      
2025-05-19 22:05:02 INFO :      Received: train message 49f8c0fb-cc43-4e86-bb8a-e9cccb546a1e
2025-05-19 22:12:23 INFO :      Sent reply
2025-05-19 22:12:36 INFO :      
2025-05-19 22:12:36 INFO :      Received: evaluate message b0f2df33-184c-4bfe-bd2a-e1bdc5158b4c
2025-05-19 22:12:49 INFO :      Sent reply
2025-05-19 22:12:55 INFO :      
2025-05-19 22:12:55 INFO :      Received: train message e23c08aa-cf8c-4b28-8874-bc6a2805cd31
2025-05-19 22:20:20 INFO :      Sent reply
2025-05-19 22:20:32 INFO :      
2025-05-19 22:20:32 INFO :      Received: evaluate message a31b9efd-f84f-45eb-a8b0-552ac09b9a97
2025-05-19 22:20:46 INFO :      Sent reply
2025-05-19 22:20:47 INFO :      
2025-05-19 22:20:47 INFO :      Received: reconnect message eeee8109-0ad0-4339-9e49-1dad81915d50
2025-05-19 22:20:47 INFO :      Disconnect and shut down
2025-05-19 21:58:08 {'loss': 0.7065, 'grad_norm': 9.68499755859375, 'learning_rate': 4.3875e-05, 'epoch': 0.125}
2025-05-19 21:58:21 {'loss': 0.7045, 'grad_norm': 10.126258850097656, 'learning_rate': 4.2625000000000006e-05, 'epoch': 0.15}
2025-05-19 21:58:31 {'loss': 0.8068, 'grad_norm': 11.002570152282715, 'learning_rate': 4.1375e-05, 'epoch': 0.175}
2025-05-19 21:58:41 {'loss': 0.9051, 'grad_norm': 9.315770149230957, 'learning_rate': 4.0125e-05, 'epoch': 0.2}
2025-05-19 21:58:54 {'loss': 0.8137, 'grad_norm': 11.523633003234863, 'learning_rate': 3.8875e-05, 'epoch': 0.225}
2025-05-19 21:59:04 {'loss': 0.829, 'grad_norm': 12.63362979888916, 'learning_rate': 3.7625e-05, 'epoch': 0.25}
2025-05-19 21:59:14 {'loss': 0.8248, 'grad_norm': 10.225377082824707, 'learning_rate': 3.6375e-05, 'epoch': 0.275}
2025-05-19 21:59:23 {'loss': 0.9099, 'grad_norm': 11.281805038452148, 'learning_rate': 3.5125e-05, 'epoch': 0.3}
2025-05-19 21:59:37 {'loss': 0.7942, 'grad_norm': 11.917525291442871, 'learning_rate': 3.3875000000000003e-05, 'epoch': 0.325}
2025-05-19 21:59:47 {'loss': 0.7821, 'grad_norm': 12.603915214538574, 'learning_rate': 3.2625e-05, 'epoch': 0.35}
2025-05-19 21:59:56 {'loss': 0.8967, 'grad_norm': 12.197171211242676, 'learning_rate': 3.1375e-05, 'epoch': 0.375}
2025-05-19 22:00:06 {'loss': 0.8183, 'grad_norm': 9.737065315246582, 'learning_rate': 3.0125000000000004e-05, 'epoch': 0.4}
2025-05-19 22:00:20 {'loss': 0.8353, 'grad_norm': 9.494439125061035, 'learning_rate': 2.8875e-05, 'epoch': 0.425}
2025-05-19 22:00:29 {'loss': 0.8755, 'grad_norm': 10.710211753845215, 'learning_rate': 2.7625e-05, 'epoch': 0.45}
2025-05-19 22:00:39 {'loss': 0.8216, 'grad_norm': 9.999065399169922, 'learning_rate': 2.6375e-05, 'epoch': 0.475}
2025-05-19 22:00:49 {'loss': 1.0794, 'grad_norm': 8.519927024841309, 'learning_rate': 2.5124999999999997e-05, 'epoch': 0.5}
2025-05-19 22:01:02 {'loss': 0.8318, 'grad_norm': 9.933632850646973, 'learning_rate': 2.3875e-05, 'epoch': 0.525}
2025-05-19 22:01:13 {'loss': 0.8331, 'grad_norm': 10.820300102233887, 'learning_rate': 2.2625e-05, 'epoch': 0.55}
2025-05-19 22:01:22 {'loss': 0.9145, 'grad_norm': 9.267352104187012, 'learning_rate': 2.1375e-05, 'epoch': 0.575}
2025-05-19 22:01:32 {'loss': 1.0996, 'grad_norm': 9.102652549743652, 'learning_rate': 2.0125e-05, 'epoch': 0.6}
2025-05-19 22:01:45 {'loss': 0.9561, 'grad_norm': 9.807397842407227, 'learning_rate': 1.8875e-05, 'epoch': 0.625}
2025-05-19 22:01:55 {'loss': 1.0913, 'grad_norm': 12.861555099487305, 'learning_rate': 1.7625e-05, 'epoch': 0.65}
2025-05-19 22:02:04 {'loss': 0.9999, 'grad_norm': 10.960318565368652, 'learning_rate': 1.6375e-05, 'epoch': 0.675}
2025-05-19 22:02:14 {'loss': 0.9782, 'grad_norm': 9.684760093688965, 'learning_rate': 1.5125e-05, 'epoch': 0.7}
2025-05-19 22:02:28 {'loss': 0.9979, 'grad_norm': 7.925026893615723, 'learning_rate': 1.3875000000000002e-05, 'epoch': 0.725}
2025-05-19 22:02:37 {'loss': 1.0328, 'grad_norm': 13.45937728881836, 'learning_rate': 1.2625e-05, 'epoch': 0.75}
2025-05-19 22:02:47 {'loss': 1.0904, 'grad_norm': 9.600123405456543, 'learning_rate': 1.1375e-05, 'epoch': 0.775}
2025-05-19 22:02:57 {'loss': 1.2111, 'grad_norm': 11.176173210144043, 'learning_rate': 1.0125e-05, 'epoch': 0.8}
2025-05-19 22:03:11 {'loss': 1.33, 'grad_norm': 10.596264839172363, 'learning_rate': 8.875e-06, 'epoch': 0.825}
2025-05-19 22:03:21 {'loss': 1.3275, 'grad_norm': 14.350841522216797, 'learning_rate': 7.625e-06, 'epoch': 0.85}
2025-05-19 22:03:30 {'loss': 1.0913, 'grad_norm': 11.2418212890625, 'learning_rate': 6.375000000000001e-06, 'epoch': 0.875}
2025-05-19 22:03:44 {'loss': 1.1338, 'grad_norm': 9.051799774169922, 'learning_rate': 5.125e-06, 'epoch': 0.9}
2025-05-19 22:03:54 {'loss': 1.1555, 'grad_norm': 13.556404113769531, 'learning_rate': 3.875e-06, 'epoch': 0.925}
2025-05-19 22:04:03 {'loss': 1.1679, 'grad_norm': 12.837728500366211, 'learning_rate': 2.625e-06, 'epoch': 0.95}
2025-05-19 22:04:17 {'loss': 1.0084, 'grad_norm': 12.307862281799316, 'learning_rate': 1.3750000000000002e-06, 'epoch': 0.975}
2025-05-19 22:04:27 {'loss': 0.6458, 'grad_norm': 8.399918556213379, 'learning_rate': 1.2500000000000002e-07, 'epoch': 1.0}
2025-05-19 22:04:27 {'train_runtime': 432.1543, 'train_samples_per_second': 1.851, 'train_steps_per_second': 0.926, 'train_loss': 0.9206824195384979, 'epoch': 1.0}
2025-05-19 22:04:57 {'eval_loss': 1.3498566150665283, 'eval_runtime': 11.494, 'eval_samples_per_second': 17.4, 'eval_steps_per_second': 2.175, 'epoch': 1.0}
2025-05-19 22:05:15 {'loss': 0.3173, 'grad_norm': 6.691844463348389, 'learning_rate': 4.8875e-05, 'epoch': 0.025}
2025-05-19 22:05:29 {'loss': 0.4571, 'grad_norm': 10.8958101272583, 'learning_rate': 4.7625000000000006e-05, 'epoch': 0.05}
2025-05-19 22:05:39 {'loss': 0.4691, 'grad_norm': 9.034096717834473, 'learning_rate': 4.6375e-05, 'epoch': 0.075}
2025-05-19 22:05:48 {'loss': 0.4673, 'grad_norm': 6.611781120300293, 'learning_rate': 4.5125e-05, 'epoch': 0.1}
2025-05-19 22:05:58 {'loss': 0.5203, 'grad_norm': 8.947614669799805, 'learning_rate': 4.3875e-05, 'epoch': 0.125}
2025-05-19 22:06:12 {'loss': 0.5303, 'grad_norm': 8.585323333740234, 'learning_rate': 4.2625000000000006e-05, 'epoch': 0.15}
2025-05-19 22:06:22 {'loss': 0.6061, 'grad_norm': 10.98798942565918, 'learning_rate': 4.1375e-05, 'epoch': 0.175}
2025-05-19 22:06:31 {'loss': 0.6734, 'grad_norm': 9.655182838439941, 'learning_rate': 4.0125e-05, 'epoch': 0.2}
2025-05-19 22:06:41 {'loss': 0.6196, 'grad_norm': 10.448259353637695, 'learning_rate': 3.8875e-05, 'epoch': 0.225}
2025-05-19 22:06:55 {'loss': 0.6108, 'grad_norm': 11.224895477294922, 'learning_rate': 3.7625e-05, 'epoch': 0.25}
2025-05-19 22:07:05 {'loss': 0.611, 'grad_norm': 9.05972957611084, 'learning_rate': 3.6375e-05, 'epoch': 0.275}
2025-05-19 22:07:15 {'loss': 0.6891, 'grad_norm': 10.2525634765625, 'learning_rate': 3.5125e-05, 'epoch': 0.3}
2025-05-19 22:07:28 {'loss': 0.6065, 'grad_norm': 10.730223655700684, 'learning_rate': 3.3875000000000003e-05, 'epoch': 0.325}
2025-05-19 22:07:38 {'loss': 0.609, 'grad_norm': 11.49425983428955, 'learning_rate': 3.2625e-05, 'epoch': 0.35}
2025-05-19 22:07:48 {'loss': 0.6987, 'grad_norm': 10.858344078063965, 'learning_rate': 3.1375e-05, 'epoch': 0.375}
2025-05-19 22:08:02 {'loss': 0.6416, 'grad_norm': 10.919306755065918, 'learning_rate': 3.0125000000000004e-05, 'epoch': 0.4}
2025-05-19 22:08:11 {'loss': 0.6523, 'grad_norm': 9.597764015197754, 'learning_rate': 2.8875e-05, 'epoch': 0.425}
2025-05-19 22:08:21 {'loss': 0.7122, 'grad_norm': 10.325018882751465, 'learning_rate': 2.7625e-05, 'epoch': 0.45}
2025-05-19 22:08:31 {'loss': 0.6787, 'grad_norm': 10.122397422790527, 'learning_rate': 2.6375e-05, 'epoch': 0.475}
2025-05-19 22:08:44 {'loss': 0.894, 'grad_norm': 8.02126407623291, 'learning_rate': 2.5124999999999997e-05, 'epoch': 0.5}
2025-05-19 22:08:54 {'loss': 0.6894, 'grad_norm': 10.970135688781738, 'learning_rate': 2.3875e-05, 'epoch': 0.525}
2025-05-19 22:09:04 {'loss': 0.6833, 'grad_norm': 15.25362777709961, 'learning_rate': 2.2625e-05, 'epoch': 0.55}
2025-05-19 22:09:13 {'loss': 0.7735, 'grad_norm': 8.779211044311523, 'learning_rate': 2.1375e-05, 'epoch': 0.575}
2025-05-19 22:09:27 {'loss': 0.9367, 'grad_norm': 8.897405624389648, 'learning_rate': 2.0125e-05, 'epoch': 0.6}
2025-05-19 22:09:36 {'loss': 0.8118, 'grad_norm': 10.182997703552246, 'learning_rate': 1.8875e-05, 'epoch': 0.625}
2025-05-19 22:09:46 {'loss': 0.9566, 'grad_norm': 14.38769817352295, 'learning_rate': 1.7625e-05, 'epoch': 0.65}
2025-05-19 22:09:56 {'loss': 0.8907, 'grad_norm': 9.91710090637207, 'learning_rate': 1.6375e-05, 'epoch': 0.675}
2025-05-19 22:10:09 {'loss': 0.8623, 'grad_norm': 10.040773391723633, 'learning_rate': 1.5125e-05, 'epoch': 0.7}
2025-05-19 22:10:19 {'loss': 0.8975, 'grad_norm': 7.483434200286865, 'learning_rate': 1.3875000000000002e-05, 'epoch': 0.725}
2025-05-19 22:10:29 {'loss': 0.912, 'grad_norm': 14.630880355834961, 'learning_rate': 1.2625e-05, 'epoch': 0.75}
2025-05-19 22:10:39 {'loss': 0.9926, 'grad_norm': 8.749459266662598, 'learning_rate': 1.1375e-05, 'epoch': 0.775}
2025-05-19 22:10:52 {'loss': 1.1097, 'grad_norm': 11.290421485900879, 'learning_rate': 1.0125e-05, 'epoch': 0.8}
2025-05-19 22:11:02 {'loss': 1.2564, 'grad_norm': 10.388688087463379, 'learning_rate': 8.875e-06, 'epoch': 0.825}
2025-05-19 22:11:12 {'loss': 1.2606, 'grad_norm': 14.205547332763672, 'learning_rate': 7.625e-06, 'epoch': 0.85}
2025-05-19 22:11:22 {'loss': 1.0515, 'grad_norm': 14.999049186706543, 'learning_rate': 6.375000000000001e-06, 'epoch': 0.875}
2025-05-19 22:11:35 {'loss': 1.1035, 'grad_norm': 10.290188789367676, 'learning_rate': 5.125e-06, 'epoch': 0.9}
2025-05-19 22:11:45 {'loss': 1.1202, 'grad_norm': 13.725208282470703, 'learning_rate': 3.875e-06, 'epoch': 0.925}
2025-05-19 22:11:55 {'loss': 1.1176, 'grad_norm': 12.894729614257812, 'learning_rate': 2.625e-06, 'epoch': 0.95}
2025-05-19 22:12:04 {'loss': 0.9293, 'grad_norm': 11.836440086364746, 'learning_rate': 1.3750000000000002e-06, 'epoch': 0.975}
2025-05-19 22:12:18 {'loss': 0.54, 'grad_norm': 7.207558631896973, 'learning_rate': 1.2500000000000002e-07, 'epoch': 1.0}
2025-05-19 22:12:18 {'train_runtime': 434.2579, 'train_samples_per_second': 1.842, 'train_steps_per_second': 0.921, 'train_loss': 0.7739799404144287, 'epoch': 1.0}
2025-05-19 22:12:49 {'eval_loss': 1.3877944946289062, 'eval_runtime': 11.3907, 'eval_samples_per_second': 17.558, 'eval_steps_per_second': 2.195, 'epoch': 1.0}
2025-05-19 22:13:13 {'loss': 0.1953, 'grad_norm': 4.701920032501221, 'learning_rate': 4.8875e-05, 'epoch': 0.025}
2025-05-19 22:13:23 {'loss': 0.2932, 'grad_norm': 11.074020385742188, 'learning_rate': 4.7625000000000006e-05, 'epoch': 0.05}
2025-05-19 22:13:33 {'loss': 0.3161, 'grad_norm': 8.740823745727539, 'learning_rate': 4.6375e-05, 'epoch': 0.075}
2025-05-19 22:13:47 {'loss': 0.3099, 'grad_norm': 6.836973190307617, 'learning_rate': 4.5125e-05, 'epoch': 0.1}
2025-05-19 22:13:57 {'loss': 0.3542, 'grad_norm': 9.999842643737793, 'learning_rate': 4.3875e-05, 'epoch': 0.125}
2025-05-19 22:14:06 {'loss': 0.3802, 'grad_norm': 6.481639385223389, 'learning_rate': 4.2625000000000006e-05, 'epoch': 0.15}
2025-05-19 22:14:16 {'loss': 0.4199, 'grad_norm': 10.585611343383789, 'learning_rate': 4.1375e-05, 'epoch': 0.175}
2025-05-19 22:14:30 {'loss': 0.4824, 'grad_norm': 7.603935718536377, 'learning_rate': 4.0125e-05, 'epoch': 0.2}
2025-05-19 22:14:40 {'loss': 0.4627, 'grad_norm': 12.535714149475098, 'learning_rate': 3.8875e-05, 'epoch': 0.225}
2025-05-19 22:14:50 {'loss': 0.4814, 'grad_norm': 11.645744323730469, 'learning_rate': 3.7625e-05, 'epoch': 0.25}
2025-05-19 22:15:00 {'loss': 0.4607, 'grad_norm': 9.046867370605469, 'learning_rate': 3.6375e-05, 'epoch': 0.275}
2025-05-19 22:15:13 {'loss': 0.5073, 'grad_norm': 8.38771915435791, 'learning_rate': 3.5125e-05, 'epoch': 0.3}
2025-05-19 22:15:23 {'loss': 0.4422, 'grad_norm': 10.059892654418945, 'learning_rate': 3.3875000000000003e-05, 'epoch': 0.325}
2025-05-19 22:15:33 {'loss': 0.4729, 'grad_norm': 10.391289710998535, 'learning_rate': 3.2625e-05, 'epoch': 0.35}
2025-05-19 22:15:43 {'loss': 0.5235, 'grad_norm': 11.864968299865723, 'learning_rate': 3.1375e-05, 'epoch': 0.375}
2025-05-19 22:15:56 {'loss': 0.5224, 'grad_norm': 8.09363079071045, 'learning_rate': 3.0125000000000004e-05, 'epoch': 0.4}
2025-05-19 22:16:06 {'loss': 0.5094, 'grad_norm': 10.366538047790527, 'learning_rate': 2.8875e-05, 'epoch': 0.425}
2025-05-19 22:16:16 {'loss': 0.5638, 'grad_norm': 10.434895515441895, 'learning_rate': 2.7625e-05, 'epoch': 0.45}
2025-05-19 22:16:30 {'loss': 0.5489, 'grad_norm': 9.646598815917969, 'learning_rate': 2.6375e-05, 'epoch': 0.475}
2025-05-19 22:16:39 {'loss': 0.7321, 'grad_norm': 9.223023414611816, 'learning_rate': 2.5124999999999997e-05, 'epoch': 0.5}
2025-05-19 22:16:49 {'loss': 0.5661, 'grad_norm': 10.274593353271484, 'learning_rate': 2.3875e-05, 'epoch': 0.525}
2025-05-19 22:16:59 {'loss': 0.5762, 'grad_norm': 10.298903465270996, 'learning_rate': 2.2625e-05, 'epoch': 0.55}
2025-05-19 22:17:13 {'loss': 0.6315, 'grad_norm': 8.021410942077637, 'learning_rate': 2.1375e-05, 'epoch': 0.575}
2025-05-19 22:17:23 {'loss': 0.7896, 'grad_norm': 11.537708282470703, 'learning_rate': 2.0125e-05, 'epoch': 0.6}
2025-05-19 22:17:33 {'loss': 0.6781, 'grad_norm': 10.335805892944336, 'learning_rate': 1.8875e-05, 'epoch': 0.625}
2025-05-19 22:17:43 {'loss': 0.8497, 'grad_norm': 14.671294212341309, 'learning_rate': 1.7625e-05, 'epoch': 0.65}
2025-05-19 22:17:56 {'loss': 0.7762, 'grad_norm': 11.32373046875, 'learning_rate': 1.6375e-05, 'epoch': 0.675}
2025-05-19 22:18:06 {'loss': 0.7529, 'grad_norm': 11.194236755371094, 'learning_rate': 1.5125e-05, 'epoch': 0.7}
2025-05-19 22:18:16 {'loss': 0.8043, 'grad_norm': 8.335582733154297, 'learning_rate': 1.3875000000000002e-05, 'epoch': 0.725}
2025-05-19 22:18:26 {'loss': 0.8278, 'grad_norm': 12.483955383300781, 'learning_rate': 1.2625e-05, 'epoch': 0.75}
2025-05-19 22:18:40 {'loss': 0.909, 'grad_norm': 10.050858497619629, 'learning_rate': 1.1375e-05, 'epoch': 0.775}
2025-05-19 22:18:50 {'loss': 1.0497, 'grad_norm': 12.529306411743164, 'learning_rate': 1.0125e-05, 'epoch': 0.8}
2025-05-19 22:19:00 {'loss': 1.1864, 'grad_norm': 10.975589752197266, 'learning_rate': 8.875e-06, 'epoch': 0.825}
2025-05-19 22:19:10 {'loss': 1.2064, 'grad_norm': 12.729923248291016, 'learning_rate': 7.625e-06, 'epoch': 0.85}
2025-05-19 22:19:23 {'loss': 1.0083, 'grad_norm': 11.830711364746094, 'learning_rate': 6.375000000000001e-06, 'epoch': 0.875}
2025-05-19 22:19:33 {'loss': 1.0778, 'grad_norm': 12.095139503479004, 'learning_rate': 5.125e-06, 'epoch': 0.9}
2025-05-19 22:19:43 {'loss': 1.0692, 'grad_norm': 15.788616180419922, 'learning_rate': 3.875e-06, 'epoch': 0.925}
2025-05-19 22:19:53 {'loss': 1.0837, 'grad_norm': 14.878055572509766, 'learning_rate': 2.625e-06, 'epoch': 0.95}
2025-05-19 22:20:07 {'loss': 0.8404, 'grad_norm': 11.088014602661133, 'learning_rate': 1.3750000000000002e-06, 'epoch': 0.975}
2025-05-19 22:20:16 {'loss': 0.4409, 'grad_norm': 5.666486740112305, 'learning_rate': 1.2500000000000002e-07, 'epoch': 1.0}
2025-05-19 22:20:16 {'train_runtime': 438.853, 'train_samples_per_second': 1.823, 'train_steps_per_second': 0.911, 'train_loss': 0.6525749158859253, 'epoch': 1.0}
2025-05-19 22:20:46 {'eval_loss': 1.4297178983688354, 'eval_runtime': 13.2485, 'eval_samples_per_second': 15.096, 'eval_steps_per_second': 1.887, 'epoch': 1.0}
