2025-05-21 19:47:15 
Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]
Generating train split:  97%|█████████▋| 116000/120000 [00:00<00:00, 1144744.40 examples/s]
Generating train split: 100%|██████████| 120000/120000 [00:00<00:00, 1149077.16 examples/s]
2025-05-21 19:47:15 
Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]
Generating test split: 100%|██████████| 7600/7600 [00:00<00:00, 1058921.38 examples/s]
2025-05-21 19:47:17 
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 1162.37 examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 1156.53 examples/s]
2025-05-21 19:47:17 
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]
Map:  24%|██▎       | 237/1000 [00:00<00:00, 2346.28 examples/s]
Map:  52%|█████▏    | 517/1000 [00:00<00:00, 2608.35 examples/s]
Map:  79%|███████▉  | 791/1000 [00:00<00:00, 2667.30 examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 2468.34 examples/s]
2025-05-21 19:47:17 /app/client.py:56: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
2025-05-21 19:47:17   trainer = Trainer(
2025-05-21 19:47:18 WARNING :   DEPRECATED FEATURE: flwr.client.start_numpy_client() is deprecated. 
2025-05-21 19:47:18 Instead, use `flwr.client.start_client()` by ensuring you first call the `.to_client()` method as shown below: 
2025-05-21 19:47:18 flwr.client.start_client(
2025-05-21 19:47:18 server_address='<IP>:<PORT>',
2025-05-21 19:47:18 client=FlowerClient().to_client(), # <-- where FlowerClient is of type flwr.client.NumPyClient object
2025-05-21 19:47:18 )
2025-05-21 19:47:18 Using `start_numpy_client()` is deprecated.
2025-05-21 19:47:18 
2025-05-21 19:47:18             This is a deprecated feature. It will be removed
2025-05-21 19:47:18             entirely in future versions of Flower.
2025-05-21 19:47:18         
2025-05-21 19:47:18 WARNING :   DEPRECATED FEATURE: flwr.client.start_client() is deprecated.
2025-05-21 19:47:18 Instead, use the `flower-supernode` CLI command to start a SuperNode as shown below:
2025-05-21 19:47:18 
2025-05-21 19:47:18 $ flower-supernode --insecure --superlink='<IP>:<PORT>'
2025-05-21 19:47:18 
2025-05-21 19:47:18 To view all available options, run:
2025-05-21 19:47:18 
2025-05-21 19:47:18 $ flower-supernode --help
2025-05-21 19:47:18 
2025-05-21 19:47:18 Using `start_client()` is deprecated.
2025-05-21 19:47:18 
2025-05-21 19:47:18             This is a deprecated feature. It will be removed
2025-05-21 19:47:18             entirely in future versions of Flower.
2025-05-21 19:47:18         
2025-05-21 19:47:18 INFO :      
2025-05-21 19:47:18 INFO :      Received: get_parameters message af58a939-493a-462f-91be-c0badac77db2
2025-05-21 19:47:22 INFO :      Sent reply
2025-05-21 19:48:02 INFO :      
2025-05-21 19:48:02 INFO :      Received: train message 528eaa13-d5dc-4c34-8ba2-4c1083730004
2025-05-21 19:48:23 {'loss': 4.2233, 'grad_norm': 12.954025268554688, 'learning_rate': 4.8875e-05, 'epoch': 0.025}
2025-05-21 19:48:43 {'loss': 2.3755, 'grad_norm': 16.513608932495117, 'learning_rate': 4.7625000000000006e-05, 'epoch': 0.05}
2025-05-21 19:48:58 {'loss': 2.132, 'grad_norm': 11.991493225097656, 'learning_rate': 4.6375e-05, 'epoch': 0.075}
2025-05-21 19:49:13 {'loss': 2.0234, 'grad_norm': 13.898946762084961, 'learning_rate': 4.5125e-05, 'epoch': 0.1}
2025-05-21 19:49:28 {'loss': 2.131, 'grad_norm': 14.57367992401123, 'learning_rate': 4.3875e-05, 'epoch': 0.125}
2025-05-21 19:49:42 {'loss': 2.1199, 'grad_norm': 10.800742149353027, 'learning_rate': 4.2625000000000006e-05, 'epoch': 0.15}
2025-05-21 19:49:57 {'loss': 2.0293, 'grad_norm': 15.612289428710938, 'learning_rate': 4.1375e-05, 'epoch': 0.175}
2025-05-21 19:50:13 {'loss': 2.1849, 'grad_norm': 12.633953094482422, 'learning_rate': 4.0125e-05, 'epoch': 0.2}
2025-05-21 19:50:34 {'loss': 2.0481, 'grad_norm': 10.769915580749512, 'learning_rate': 3.8875e-05, 'epoch': 0.225}
2025-05-21 19:50:49 {'loss': 1.9727, 'grad_norm': 9.797770500183105, 'learning_rate': 3.7625e-05, 'epoch': 0.25}
2025-05-21 19:51:03 {'loss': 2.1969, 'grad_norm': 15.72296142578125, 'learning_rate': 3.6375e-05, 'epoch': 0.275}
2025-05-21 19:51:18 {'loss': 2.089, 'grad_norm': 10.656142234802246, 'learning_rate': 3.5125e-05, 'epoch': 0.3}
2025-05-21 19:51:33 {'loss': 1.8625, 'grad_norm': 11.282824516296387, 'learning_rate': 3.3875000000000003e-05, 'epoch': 0.325}
2025-05-21 19:51:48 {'loss': 1.9878, 'grad_norm': 12.777449607849121, 'learning_rate': 3.2625e-05, 'epoch': 0.35}
2025-05-21 19:52:03 {'loss': 1.9152, 'grad_norm': 12.17918586730957, 'learning_rate': 3.1375e-05, 'epoch': 0.375}
2025-05-21 19:52:17 {'loss': 1.7906, 'grad_norm': 12.128702163696289, 'learning_rate': 3.0125000000000004e-05, 'epoch': 0.4}
2025-05-21 19:52:32 {'loss': 2.0385, 'grad_norm': 11.784635543823242, 'learning_rate': 2.8875e-05, 'epoch': 0.425}
2025-05-21 19:52:54 {'loss': 1.553, 'grad_norm': 9.383516311645508, 'learning_rate': 2.7625e-05, 'epoch': 0.45}
2025-05-21 19:53:08 {'loss': 1.7568, 'grad_norm': 10.852519035339355, 'learning_rate': 2.6375e-05, 'epoch': 0.475}
2025-05-21 19:53:23 {'loss': 1.7939, 'grad_norm': 14.081692695617676, 'learning_rate': 2.5124999999999997e-05, 'epoch': 0.5}
2025-05-21 19:53:38 {'loss': 1.6901, 'grad_norm': 9.656120300292969, 'learning_rate': 2.3875e-05, 'epoch': 0.525}
2025-05-21 19:53:53 {'loss': 1.8905, 'grad_norm': 10.919647216796875, 'learning_rate': 2.2625e-05, 'epoch': 0.55}
2025-05-21 19:54:08 {'loss': 1.7214, 'grad_norm': 9.826437950134277, 'learning_rate': 2.1375e-05, 'epoch': 0.575}
2025-05-21 19:54:23 {'loss': 1.8111, 'grad_norm': 10.13541030883789, 'learning_rate': 2.0125e-05, 'epoch': 0.6}
2025-05-21 19:54:39 {'loss': 1.8427, 'grad_norm': 12.092211723327637, 'learning_rate': 1.8875e-05, 'epoch': 0.625}
2025-05-21 19:55:00 {'loss': 1.843, 'grad_norm': 12.117457389831543, 'learning_rate': 1.7625e-05, 'epoch': 0.65}
2025-05-21 19:55:15 {'loss': 1.624, 'grad_norm': 9.809255599975586, 'learning_rate': 1.6375e-05, 'epoch': 0.675}
2025-05-21 19:55:30 {'loss': 1.9728, 'grad_norm': 8.96179485321045, 'learning_rate': 1.5125e-05, 'epoch': 0.7}
2025-05-21 19:55:45 {'loss': 1.7584, 'grad_norm': 12.607402801513672, 'learning_rate': 1.3875000000000002e-05, 'epoch': 0.725}
2025-05-21 19:56:00 {'loss': 1.8289, 'grad_norm': 13.16430950164795, 'learning_rate': 1.2625e-05, 'epoch': 0.75}
2025-05-21 19:56:15 {'loss': 1.9457, 'grad_norm': 9.673770904541016, 'learning_rate': 1.1375e-05, 'epoch': 0.775}
2025-05-21 19:56:30 {'loss': 1.8119, 'grad_norm': 9.84853744506836, 'learning_rate': 1.0125e-05, 'epoch': 0.8}
2025-05-21 19:56:52 {'loss': 1.9359, 'grad_norm': 11.89953899383545, 'learning_rate': 8.875e-06, 'epoch': 0.825}
2025-05-21 19:57:08 {'loss': 1.5859, 'grad_norm': 13.208569526672363, 'learning_rate': 7.625e-06, 'epoch': 0.85}
2025-05-21 19:57:23 {'loss': 1.6172, 'grad_norm': 14.357946395874023, 'learning_rate': 6.375000000000001e-06, 'epoch': 0.875}
2025-05-21 19:57:38 {'loss': 1.5714, 'grad_norm': 12.735970497131348, 'learning_rate': 5.125e-06, 'epoch': 0.9}
2025-05-21 19:57:53 {'loss': 1.5767, 'grad_norm': 7.974838733673096, 'learning_rate': 3.875e-06, 'epoch': 0.925}
2025-05-21 19:58:08 {'loss': 1.6952, 'grad_norm': 8.705812454223633, 'learning_rate': 2.625e-06, 'epoch': 0.95}
2025-05-21 19:58:23 {'loss': 1.8273, 'grad_norm': 11.742932319641113, 'learning_rate': 1.3750000000000002e-06, 'epoch': 0.975}
2025-05-21 19:58:38 {'loss': 1.6129, 'grad_norm': 9.030025482177734, 'learning_rate': 1.2500000000000002e-07, 'epoch': 1.0}
2025-05-21 19:58:38 {'train_runtime': 634.2554, 'train_samples_per_second': 1.261, 'train_steps_per_second': 0.631, 'train_loss': 1.934671380519867, 'epoch': 1.0}
2025-05-21 19:59:50 {'eval_loss': 1.4997073411941528, 'eval_runtime': 13.5504, 'eval_samples_per_second': 14.76, 'eval_steps_per_second': 1.845, 'epoch': 1.0}
2025-05-21 20:00:14 {'loss': 1.4552, 'grad_norm': 9.420260429382324, 'learning_rate': 4.8875e-05, 'epoch': 0.025}
2025-05-21 20:00:29 {'loss': 1.4413, 'grad_norm': 10.429144859313965, 'learning_rate': 4.7625000000000006e-05, 'epoch': 0.05}
2025-05-21 20:00:43 {'loss': 1.4216, 'grad_norm': 10.24740219116211, 'learning_rate': 4.6375e-05, 'epoch': 0.075}
2025-05-21 20:00:58 {'loss': 1.3494, 'grad_norm': 10.759786605834961, 'learning_rate': 4.5125e-05, 'epoch': 0.1}
2025-05-21 20:01:13 {'loss': 1.4762, 'grad_norm': 11.833795547485352, 'learning_rate': 4.3875e-05, 'epoch': 0.125}
2025-05-21 20:01:28 {'loss': 1.3942, 'grad_norm': 8.18248176574707, 'learning_rate': 4.2625000000000006e-05, 'epoch': 0.15}
2025-05-21 20:01:42 {'loss': 1.4473, 'grad_norm': 11.21352481842041, 'learning_rate': 4.1375e-05, 'epoch': 0.175}
2025-05-21 20:01:57 {'loss': 1.5846, 'grad_norm': 11.305547714233398, 'learning_rate': 4.0125e-05, 'epoch': 0.2}
2025-05-21 20:02:18 {'loss': 1.5052, 'grad_norm': 11.411730766296387, 'learning_rate': 3.8875e-05, 'epoch': 0.225}
2025-05-21 20:02:33 {'loss': 1.4674, 'grad_norm': 9.412025451660156, 'learning_rate': 3.7625e-05, 'epoch': 0.25}
2025-05-21 20:02:47 {'loss': 1.6849, 'grad_norm': 17.190711975097656, 'learning_rate': 3.6375e-05, 'epoch': 0.275}
2025-05-21 20:03:02 {'loss': 1.6025, 'grad_norm': 9.016007423400879, 'learning_rate': 3.5125e-05, 'epoch': 0.3}
2025-05-21 20:03:17 {'loss': 1.4266, 'grad_norm': 10.22894287109375, 'learning_rate': 3.3875000000000003e-05, 'epoch': 0.325}
2025-05-21 20:03:32 {'loss': 1.5477, 'grad_norm': 10.64968204498291, 'learning_rate': 3.2625e-05, 'epoch': 0.35}
2025-05-21 20:03:47 {'loss': 1.4989, 'grad_norm': 13.10212516784668, 'learning_rate': 3.1375e-05, 'epoch': 0.375}
2025-05-21 20:04:07 {'loss': 1.3863, 'grad_norm': 10.388589859008789, 'learning_rate': 3.0125000000000004e-05, 'epoch': 0.4}
2025-05-21 20:04:22 {'loss': 1.6051, 'grad_norm': 11.734784126281738, 'learning_rate': 2.8875e-05, 'epoch': 0.425}
2025-05-21 20:04:37 {'loss': 1.2137, 'grad_norm': 7.325946807861328, 'learning_rate': 2.7625e-05, 'epoch': 0.45}
2025-05-21 20:04:51 {'loss': 1.3731, 'grad_norm': 11.25169563293457, 'learning_rate': 2.6375e-05, 'epoch': 0.475}
2025-05-21 20:05:06 {'loss': 1.4118, 'grad_norm': 11.298596382141113, 'learning_rate': 2.5124999999999997e-05, 'epoch': 0.5}
2025-05-21 20:05:21 {'loss': 1.3559, 'grad_norm': 8.982268333435059, 'learning_rate': 2.3875e-05, 'epoch': 0.525}
2025-05-21 20:05:35 {'loss': 1.5301, 'grad_norm': 9.201070785522461, 'learning_rate': 2.2625e-05, 'epoch': 0.55}
2025-05-21 20:05:56 {'loss': 1.4224, 'grad_norm': 9.133702278137207, 'learning_rate': 2.1375e-05, 'epoch': 0.575}
2025-05-21 20:06:11 {'loss': 1.5222, 'grad_norm': 9.239363670349121, 'learning_rate': 2.0125e-05, 'epoch': 0.6}
2025-05-21 20:06:26 {'loss': 1.5006, 'grad_norm': 11.269484519958496, 'learning_rate': 1.8875e-05, 'epoch': 0.625}
2025-05-21 20:06:41 {'loss': 1.5184, 'grad_norm': 12.430991172790527, 'learning_rate': 1.7625e-05, 'epoch': 0.65}
2025-05-21 20:06:56 {'loss': 1.3424, 'grad_norm': 7.4793877601623535, 'learning_rate': 1.6375e-05, 'epoch': 0.675}
2025-05-21 20:07:10 {'loss': 1.6248, 'grad_norm': 7.780627727508545, 'learning_rate': 1.5125e-05, 'epoch': 0.7}
2025-05-21 20:07:25 {'loss': 1.4478, 'grad_norm': 9.906732559204102, 'learning_rate': 1.3875000000000002e-05, 'epoch': 0.725}
2025-05-21 20:07:46 {'loss': 1.5515, 'grad_norm': 12.263371467590332, 'learning_rate': 1.2625e-05, 'epoch': 0.75}
2025-05-21 20:08:01 {'loss': 1.6818, 'grad_norm': 8.951496124267578, 'learning_rate': 1.1375e-05, 'epoch': 0.775}
2025-05-21 20:08:15 {'loss': 1.5569, 'grad_norm': 9.383681297302246, 'learning_rate': 1.0125e-05, 'epoch': 0.8}
2025-05-21 20:08:30 {'loss': 1.6695, 'grad_norm': 12.302236557006836, 'learning_rate': 8.875e-06, 'epoch': 0.825}
2025-05-21 20:08:45 {'loss': 1.4201, 'grad_norm': 13.251298904418945, 'learning_rate': 7.625e-06, 'epoch': 0.85}
2025-05-21 20:09:00 {'loss': 1.4203, 'grad_norm': 13.797249794006348, 'learning_rate': 6.375000000000001e-06, 'epoch': 0.875}
2025-05-21 20:09:15 {'loss': 1.3559, 'grad_norm': 11.043560028076172, 'learning_rate': 5.125e-06, 'epoch': 0.9}
2025-05-21 20:09:30 {'loss': 1.3888, 'grad_norm': 7.517943382263184, 'learning_rate': 3.875e-06, 'epoch': 0.925}
2025-05-21 20:09:51 {'loss': 1.42, 'grad_norm': 8.153421401977539, 'learning_rate': 2.625e-06, 'epoch': 0.95}
2025-05-21 20:10:06 {'loss': 1.4088, 'grad_norm': 9.156494140625, 'learning_rate': 1.3750000000000002e-06, 'epoch': 0.975}
2025-05-21 20:10:20 {'loss': 0.9868, 'grad_norm': 6.709001541137695, 'learning_rate': 1.2500000000000002e-07, 'epoch': 1.0}
2025-05-21 20:10:20 {'train_runtime': 617.6942, 'train_samples_per_second': 1.295, 'train_steps_per_second': 0.648, 'train_loss': 1.4604412961006163, 'epoch': 1.0}
2025-05-21 20:11:29 {'eval_loss': 1.444985032081604, 'eval_runtime': 17.9879, 'eval_samples_per_second': 11.119, 'eval_steps_per_second': 1.39, 'epoch': 1.0}
2025-05-21 20:11:59 {'loss': 1.0324, 'grad_norm': 9.528066635131836, 'learning_rate': 4.8875e-05, 'epoch': 0.025}
2025-05-21 20:12:14 {'loss': 1.1009, 'grad_norm': 8.901395797729492, 'learning_rate': 4.7625000000000006e-05, 'epoch': 0.05}
2025-05-21 20:12:29 {'loss': 1.1143, 'grad_norm': 8.784963607788086, 'learning_rate': 4.6375e-05, 'epoch': 0.075}
2025-05-21 20:12:44 {'loss': 1.0485, 'grad_norm': 9.645702362060547, 'learning_rate': 4.5125e-05, 'epoch': 0.1}
2025-05-21 20:12:59 {'loss': 1.1574, 'grad_norm': 11.43061637878418, 'learning_rate': 4.3875e-05, 'epoch': 0.125}
2025-05-21 20:13:14 {'loss': 1.0866, 'grad_norm': 7.865504741668701, 'learning_rate': 4.2625000000000006e-05, 'epoch': 0.15}
2025-05-21 20:13:28 {'loss': 1.1642, 'grad_norm': 11.570097923278809, 'learning_rate': 4.1375e-05, 'epoch': 0.175}
2025-05-21 20:13:43 {'loss': 1.2753, 'grad_norm': 10.39939022064209, 'learning_rate': 4.0125e-05, 'epoch': 0.2}
2025-05-21 20:13:58 {'loss': 1.2263, 'grad_norm': 10.580490112304688, 'learning_rate': 3.8875e-05, 'epoch': 0.225}
2025-05-21 20:14:13 {'loss': 1.2092, 'grad_norm': 7.975372314453125, 'learning_rate': 3.7625e-05, 'epoch': 0.25}
2025-05-21 20:14:33 {'loss': 1.3989, 'grad_norm': 13.550152778625488, 'learning_rate': 3.6375e-05, 'epoch': 0.275}
2025-05-21 20:14:48 {'loss': 1.3677, 'grad_norm': 8.532435417175293, 'learning_rate': 3.5125e-05, 'epoch': 0.3}
2025-05-21 20:15:02 {'loss': 1.1887, 'grad_norm': 9.24434757232666, 'learning_rate': 3.3875000000000003e-05, 'epoch': 0.325}
2025-05-21 20:15:17 {'loss': 1.2994, 'grad_norm': 11.188736915588379, 'learning_rate': 3.2625e-05, 'epoch': 0.35}
2025-05-21 20:15:32 {'loss': 1.2856, 'grad_norm': 11.396942138671875, 'learning_rate': 3.1375e-05, 'epoch': 0.375}
2025-05-21 20:15:46 {'loss': 1.1913, 'grad_norm': 9.829720497131348, 'learning_rate': 3.0125000000000004e-05, 'epoch': 0.4}
2025-05-21 20:16:01 {'loss': 1.3739, 'grad_norm': 10.284720420837402, 'learning_rate': 2.8875e-05, 'epoch': 0.425}
2025-05-21 20:16:21 {'loss': 1.0478, 'grad_norm': 6.633624076843262, 'learning_rate': 2.7625e-05, 'epoch': 0.45}
2025-05-21 20:16:36 {'loss': 1.1771, 'grad_norm': 10.17331314086914, 'learning_rate': 2.6375e-05, 'epoch': 0.475}
2025-05-21 20:16:51 {'loss': 1.1998, 'grad_norm': 9.991976737976074, 'learning_rate': 2.5124999999999997e-05, 'epoch': 0.5}
2025-05-21 20:17:05 {'loss': 1.1828, 'grad_norm': 7.298666477203369, 'learning_rate': 2.3875e-05, 'epoch': 0.525}
2025-05-21 20:17:20 {'loss': 1.3057, 'grad_norm': 8.157010078430176, 'learning_rate': 2.2625e-05, 'epoch': 0.55}
2025-05-21 20:17:35 {'loss': 1.2555, 'grad_norm': 8.582914352416992, 'learning_rate': 2.1375e-05, 'epoch': 0.575}
2025-05-21 20:17:49 {'loss': 1.339, 'grad_norm': 8.17495346069336, 'learning_rate': 2.0125e-05, 'epoch': 0.6}
2025-05-21 20:18:04 {'loss': 1.3438, 'grad_norm': 10.891945838928223, 'learning_rate': 1.8875e-05, 'epoch': 0.625}
2025-05-21 20:18:19 {'loss': 1.3826, 'grad_norm': 12.040497779846191, 'learning_rate': 1.7625e-05, 'epoch': 0.65}
2025-05-21 20:18:40 {'loss': 1.1961, 'grad_norm': 7.853000640869141, 'learning_rate': 1.6375e-05, 'epoch': 0.675}
2025-05-21 20:18:54 {'loss': 1.4605, 'grad_norm': 7.614507675170898, 'learning_rate': 1.5125e-05, 'epoch': 0.7}
2025-05-21 20:19:09 {'loss': 1.3226, 'grad_norm': 10.03121566772461, 'learning_rate': 1.3875000000000002e-05, 'epoch': 0.725}
2025-05-21 20:19:24 {'loss': 1.4354, 'grad_norm': 12.398896217346191, 'learning_rate': 1.2625e-05, 'epoch': 0.75}
2025-05-21 20:19:39 {'loss': 1.5436, 'grad_norm': 10.27746295928955, 'learning_rate': 1.1375e-05, 'epoch': 0.775}
2025-05-21 20:19:53 {'loss': 1.4318, 'grad_norm': 9.431663513183594, 'learning_rate': 1.0125e-05, 'epoch': 0.8}
2025-05-21 20:20:14 {'loss': 1.5683, 'grad_norm': 12.294126510620117, 'learning_rate': 8.875e-06, 'epoch': 0.825}
2025-05-21 20:20:29 {'loss': 1.3553, 'grad_norm': 11.82529067993164, 'learning_rate': 7.625e-06, 'epoch': 0.85}
2025-05-21 20:20:43 {'loss': 1.3391, 'grad_norm': 13.688200950622559, 'learning_rate': 6.375000000000001e-06, 'epoch': 0.875}
2025-05-21 20:20:58 {'loss': 1.2889, 'grad_norm': 9.428756713867188, 'learning_rate': 5.125e-06, 'epoch': 0.9}
2025-05-21 20:21:13 {'loss': 1.3289, 'grad_norm': 8.529440879821777, 'learning_rate': 3.875e-06, 'epoch': 0.925}
2025-05-21 20:21:27 {'loss': 1.3692, 'grad_norm': 7.8065972328186035, 'learning_rate': 2.625e-06, 'epoch': 0.95}
2025-05-21 20:21:42 {'loss': 1.3013, 'grad_norm': 9.516777992248535, 'learning_rate': 1.3750000000000002e-06, 'epoch': 0.975}
2025-05-21 20:22:03 {'loss': 0.8057, 'grad_norm': 7.882843971252441, 'learning_rate': 1.2500000000000002e-07, 'epoch': 1.0}
2025-05-21 20:22:03 {'train_runtime': 621.955, 'train_samples_per_second': 1.286, 'train_steps_per_second': 0.643, 'train_loss': 1.2625316429138183, 'epoch': 1.0}
2025-05-21 20:23:08 {'eval_loss': 1.450119972229004, 'eval_runtime': 13.5692, 'eval_samples_per_second': 14.739, 'eval_steps_per_second': 1.842, 'epoch': 1.0}
2025-05-21 20:23:42 {'loss': 0.7329, 'grad_norm': 9.909875869750977, 'learning_rate': 4.8875e-05, 'epoch': 0.025}
2025-05-21 20:23:57 {'loss': 0.8419, 'grad_norm': 8.805980682373047, 'learning_rate': 4.7625000000000006e-05, 'epoch': 0.05}
2025-05-21 20:24:11 {'loss': 0.8525, 'grad_norm': 7.99783992767334, 'learning_rate': 4.6375e-05, 'epoch': 0.075}
2025-05-21 20:24:26 {'loss': 0.8204, 'grad_norm': 9.73426628112793, 'learning_rate': 4.5125e-05, 'epoch': 0.1}
2025-05-21 20:24:40 {'loss': 0.9389, 'grad_norm': 8.992510795593262, 'learning_rate': 4.3875e-05, 'epoch': 0.125}
2025-05-21 20:24:55 {'loss': 0.8665, 'grad_norm': 7.008146286010742, 'learning_rate': 4.2625000000000006e-05, 'epoch': 0.15}
2025-05-21 20:25:17 {'loss': 0.9597, 'grad_norm': 10.501806259155273, 'learning_rate': 4.1375e-05, 'epoch': 0.175}
2025-05-21 20:25:32 {'loss': 1.0587, 'grad_norm': 10.392850875854492, 'learning_rate': 4.0125e-05, 'epoch': 0.2}
2025-05-21 20:25:47 {'loss': 1.0171, 'grad_norm': 8.985000610351562, 'learning_rate': 3.8875e-05, 'epoch': 0.225}
2025-05-21 20:26:01 {'loss': 0.9979, 'grad_norm': 7.976985931396484, 'learning_rate': 3.7625e-05, 'epoch': 0.25}
2025-05-21 20:26:16 {'loss': 1.1615, 'grad_norm': 14.195406913757324, 'learning_rate': 3.6375e-05, 'epoch': 0.275}
2025-05-21 20:26:31 {'loss': 1.1788, 'grad_norm': 8.576539039611816, 'learning_rate': 3.5125e-05, 'epoch': 0.3}
2025-05-21 20:26:52 {'loss': 1.0249, 'grad_norm': 9.48916244506836, 'learning_rate': 3.3875000000000003e-05, 'epoch': 0.325}
2025-05-21 20:27:07 {'loss': 1.1246, 'grad_norm': 9.58755111694336, 'learning_rate': 3.2625e-05, 'epoch': 0.35}
2025-05-21 20:27:22 {'loss': 1.0902, 'grad_norm': 10.1045560836792, 'learning_rate': 3.1375e-05, 'epoch': 0.375}
2025-05-21 20:27:36 {'loss': 1.0071, 'grad_norm': 8.78282356262207, 'learning_rate': 3.0125000000000004e-05, 'epoch': 0.4}
2025-05-21 20:27:51 {'loss': 1.1838, 'grad_norm': 11.667179107666016, 'learning_rate': 2.8875e-05, 'epoch': 0.425}
2025-05-21 20:28:06 {'loss': 0.9033, 'grad_norm': 6.042152404785156, 'learning_rate': 2.7625e-05, 'epoch': 0.45}
2025-05-21 20:28:21 {'loss': 1.0283, 'grad_norm': 10.382340431213379, 'learning_rate': 2.6375e-05, 'epoch': 0.475}
2025-05-21 20:28:42 {'loss': 1.0535, 'grad_norm': 10.482985496520996, 'learning_rate': 2.5124999999999997e-05, 'epoch': 0.5}
2025-05-21 20:28:57 {'loss': 1.0331, 'grad_norm': 7.413179874420166, 'learning_rate': 2.3875e-05, 'epoch': 0.525}
2025-05-21 20:29:12 {'loss': 1.1645, 'grad_norm': 9.174880981445312, 'learning_rate': 2.2625e-05, 'epoch': 0.55}
2025-05-21 20:29:26 {'loss': 1.1284, 'grad_norm': 8.847214698791504, 'learning_rate': 2.1375e-05, 'epoch': 0.575}
2025-05-21 20:29:41 {'loss': 1.2275, 'grad_norm': 7.719832897186279, 'learning_rate': 2.0125e-05, 'epoch': 0.6}
2025-05-21 20:29:56 {'loss': 1.2175, 'grad_norm': 11.021728515625, 'learning_rate': 1.8875e-05, 'epoch': 0.625}
2025-05-21 20:30:10 {'loss': 1.2497, 'grad_norm': 10.769959449768066, 'learning_rate': 1.7625e-05, 'epoch': 0.65}
2025-05-21 20:30:25 {'loss': 1.1141, 'grad_norm': 8.275976181030273, 'learning_rate': 1.6375e-05, 'epoch': 0.675}
2025-05-21 20:30:40 {'loss': 1.3424, 'grad_norm': 7.696570873260498, 'learning_rate': 1.5125e-05, 'epoch': 0.7}
2025-05-21 20:31:01 {'loss': 1.2191, 'grad_norm': 9.885377883911133, 'learning_rate': 1.3875000000000002e-05, 'epoch': 0.725}
2025-05-21 20:31:16 {'loss': 1.3221, 'grad_norm': 9.545700073242188, 'learning_rate': 1.2625e-05, 'epoch': 0.75}
2025-05-21 20:31:31 {'loss': 1.4843, 'grad_norm': 10.579052925109863, 'learning_rate': 1.1375e-05, 'epoch': 0.775}
2025-05-21 20:31:46 {'loss': 1.3752, 'grad_norm': 9.797499656677246, 'learning_rate': 1.0125e-05, 'epoch': 0.8}
2025-05-21 20:32:01 {'loss': 1.4841, 'grad_norm': 11.597721099853516, 'learning_rate': 8.875e-06, 'epoch': 0.825}
2025-05-21 20:32:15 {'loss': 1.3117, 'grad_norm': 13.167474746704102, 'learning_rate': 7.625e-06, 'epoch': 0.85}
2025-05-21 20:32:30 {'loss': 1.296, 'grad_norm': 14.367318153381348, 'learning_rate': 6.375000000000001e-06, 'epoch': 0.875}
2025-05-21 20:32:45 {'loss': 1.2382, 'grad_norm': 10.42911434173584, 'learning_rate': 5.125e-06, 'epoch': 0.9}
2025-05-21 20:33:06 {'loss': 1.2925, 'grad_norm': 7.989569187164307, 'learning_rate': 3.875e-06, 'epoch': 0.925}
2025-05-21 20:33:21 {'loss': 1.3336, 'grad_norm': 7.346583366394043, 'learning_rate': 2.625e-06, 'epoch': 0.95}
2025-05-21 20:33:35 {'loss': 1.2291, 'grad_norm': 10.086655616760254, 'learning_rate': 1.3750000000000002e-06, 'epoch': 0.975}
2025-05-21 20:33:50 {'loss': 0.7276, 'grad_norm': 6.491422176361084, 'learning_rate': 1.2500000000000002e-07, 'epoch': 1.0}
2025-05-21 20:33:50 {'train_runtime': 627.7628, 'train_samples_per_second': 1.274, 'train_steps_per_second': 0.637, 'train_loss': 1.1158346295356751, 'epoch': 1.0}
2025-05-21 20:34:48 {'eval_loss': 1.458422064781189, 'eval_runtime': 6.5014, 'eval_samples_per_second': 30.763, 'eval_steps_per_second': 3.845, 'epoch': 1.0}
2025-05-21 20:35:22 {'loss': 0.5207, 'grad_norm': 6.14423131942749, 'learning_rate': 4.8875e-05, 'epoch': 0.025}
2025-05-21 20:35:37 {'loss': 0.6444, 'grad_norm': 8.70709228515625, 'learning_rate': 4.7625000000000006e-05, 'epoch': 0.05}
2025-05-21 20:35:52 {'loss': 0.6906, 'grad_norm': 7.8771586418151855, 'learning_rate': 4.6375e-05, 'epoch': 0.075}
2025-05-21 20:36:07 {'loss': 0.6549, 'grad_norm': 8.399989128112793, 'learning_rate': 4.5125e-05, 'epoch': 0.1}
2025-05-21 20:36:21 {'loss': 0.7521, 'grad_norm': 9.419617652893066, 'learning_rate': 4.3875e-05, 'epoch': 0.125}
2025-05-21 20:36:43 {'loss': 0.7219, 'grad_norm': 6.46270751953125, 'learning_rate': 4.2625000000000006e-05, 'epoch': 0.15}
2025-05-21 20:36:57 {'loss': 0.7636, 'grad_norm': 9.955446243286133, 'learning_rate': 4.1375e-05, 'epoch': 0.175}
2025-05-21 20:37:12 {'loss': 0.8991, 'grad_norm': 9.609922409057617, 'learning_rate': 4.0125e-05, 'epoch': 0.2}
2025-05-21 20:37:27 {'loss': 0.8499, 'grad_norm': 8.651144981384277, 'learning_rate': 3.8875e-05, 'epoch': 0.225}
2025-05-21 20:37:42 {'loss': 0.8492, 'grad_norm': 8.970626831054688, 'learning_rate': 3.7625e-05, 'epoch': 0.25}
2025-05-21 20:37:56 {'loss': 0.9926, 'grad_norm': 11.90354061126709, 'learning_rate': 3.6375e-05, 'epoch': 0.275}
2025-05-21 20:38:11 {'loss': 1.0369, 'grad_norm': 7.719776153564453, 'learning_rate': 3.5125e-05, 'epoch': 0.3}
2025-05-21 20:38:32 {'loss': 0.8712, 'grad_norm': 8.276220321655273, 'learning_rate': 3.3875000000000003e-05, 'epoch': 0.325}
2025-05-21 20:38:46 {'loss': 0.9654, 'grad_norm': 9.194217681884766, 'learning_rate': 3.2625e-05, 'epoch': 0.35}
2025-05-21 20:39:01 {'loss': 0.9539, 'grad_norm': 10.731237411499023, 'learning_rate': 3.1375e-05, 'epoch': 0.375}
2025-05-21 20:39:15 {'loss': 0.8758, 'grad_norm': 10.569161415100098, 'learning_rate': 3.0125000000000004e-05, 'epoch': 0.4}
2025-05-21 20:39:30 {'loss': 1.0564, 'grad_norm': 10.646767616271973, 'learning_rate': 2.8875e-05, 'epoch': 0.425}
2025-05-21 20:39:45 {'loss': 0.7904, 'grad_norm': 6.3005290031433105, 'learning_rate': 2.7625e-05, 'epoch': 0.45}
2025-05-21 20:40:00 {'loss': 0.9185, 'grad_norm': 10.474082946777344, 'learning_rate': 2.6375e-05, 'epoch': 0.475}
2025-05-21 20:40:21 {'loss': 0.9237, 'grad_norm': 10.17939281463623, 'learning_rate': 2.5124999999999997e-05, 'epoch': 0.5}
2025-05-21 20:40:36 {'loss': 0.9328, 'grad_norm': 7.29088020324707, 'learning_rate': 2.3875e-05, 'epoch': 0.525}
2025-05-21 20:40:50 {'loss': 1.0547, 'grad_norm': 8.76284408569336, 'learning_rate': 2.2625e-05, 'epoch': 0.55}
2025-05-21 20:41:05 {'loss': 1.0132, 'grad_norm': 8.606558799743652, 'learning_rate': 2.1375e-05, 'epoch': 0.575}
2025-05-21 20:41:19 {'loss': 1.1236, 'grad_norm': 7.9064507484436035, 'learning_rate': 2.0125e-05, 'epoch': 0.6}
2025-05-21 20:41:34 {'loss': 1.1195, 'grad_norm': 12.964583396911621, 'learning_rate': 1.8875e-05, 'epoch': 0.625}
2025-05-21 20:41:49 {'loss': 1.163, 'grad_norm': 11.097870826721191, 'learning_rate': 1.7625e-05, 'epoch': 0.65}
2025-05-21 20:42:03 {'loss': 1.0207, 'grad_norm': 8.30655288696289, 'learning_rate': 1.6375e-05, 'epoch': 0.675}
2025-05-21 20:42:25 {'loss': 1.266, 'grad_norm': 9.02221965789795, 'learning_rate': 1.5125e-05, 'epoch': 0.7}
2025-05-21 20:42:39 {'loss': 1.1511, 'grad_norm': 9.989028930664062, 'learning_rate': 1.3875000000000002e-05, 'epoch': 0.725}
2025-05-21 20:42:54 {'loss': 1.2649, 'grad_norm': 10.26553726196289, 'learning_rate': 1.2625e-05, 'epoch': 0.75}
2025-05-21 20:43:09 {'loss': 1.3962, 'grad_norm': 10.640146255493164, 'learning_rate': 1.1375e-05, 'epoch': 0.775}
2025-05-21 20:43:23 {'loss': 1.3101, 'grad_norm': 9.565919876098633, 'learning_rate': 1.0125e-05, 'epoch': 0.8}
2025-05-21 20:43:38 {'loss': 1.438, 'grad_norm': 11.464754104614258, 'learning_rate': 8.875e-06, 'epoch': 0.825}
2025-05-21 20:43:52 {'loss': 1.2831, 'grad_norm': 14.445042610168457, 'learning_rate': 7.625e-06, 'epoch': 0.85}
2025-05-21 20:44:14 {'loss': 1.2608, 'grad_norm': 15.194551467895508, 'learning_rate': 6.375000000000001e-06, 'epoch': 0.875}
2025-05-21 20:44:28 {'loss': 1.2061, 'grad_norm': 11.075868606567383, 'learning_rate': 5.125e-06, 'epoch': 0.9}
2025-05-21 20:44:43 {'loss': 1.2784, 'grad_norm': 8.953091621398926, 'learning_rate': 3.875e-06, 'epoch': 0.925}
2025-05-21 20:44:57 {'loss': 1.2938, 'grad_norm': 7.456394195556641, 'learning_rate': 2.625e-06, 'epoch': 0.95}
2025-05-21 20:45:12 {'loss': 1.1706, 'grad_norm': 8.771936416625977, 'learning_rate': 1.3750000000000002e-06, 'epoch': 0.975}
2025-05-21 20:45:27 {'loss': 0.6232, 'grad_norm': 6.4964399337768555, 'learning_rate': 1.2500000000000002e-07, 'epoch': 1.0}
2025-05-21 20:45:27 {'train_runtime': 625.3924, 'train_samples_per_second': 1.279, 'train_steps_per_second': 0.64, 'train_loss': 1.0025282144546508, 'epoch': 1.0}
2025-05-21 20:46:27 {'eval_loss': 1.4752764701843262, 'eval_runtime': 13.2062, 'eval_samples_per_second': 15.144, 'eval_steps_per_second': 1.893, 'epoch': 1.0}
2025-05-21 19:58:52 INFO :      Sent reply
2025-05-21 19:59:33 INFO :      
2025-05-21 19:59:33 INFO :      Received: evaluate message ea0887f4-cb81-4a1b-a3fa-a7c9a68dd6c9
2025-05-21 19:59:50 INFO :      Sent reply
2025-05-21 20:00:00 INFO :      
2025-05-21 20:00:00 INFO :      Received: train message 0c97b4e8-a03b-436c-b302-458f9cc3d1a2
2025-05-21 20:10:28 INFO :      Sent reply
2025-05-21 20:11:09 INFO :      
2025-05-21 20:11:09 INFO :      Received: evaluate message 08fb81f0-18a0-4156-83b3-3fc32469e4e4
2025-05-21 20:11:29 INFO :      Sent reply
2025-05-21 20:11:39 INFO :      
2025-05-21 20:11:39 INFO :      Received: train message ba588f51-3c03-414e-902d-5fb8a4c22e8f
2025-05-21 20:22:10 INFO :      Sent reply
2025-05-21 20:22:53 INFO :      
2025-05-21 20:22:53 INFO :      Received: evaluate message ffca43ca-295b-452f-80eb-f445b003ec54
2025-05-21 20:23:08 INFO :      Sent reply
2025-05-21 20:23:21 INFO :      
2025-05-21 20:23:21 INFO :      Received: train message 48a235c7-35ef-489a-9de7-fca98d83639d
2025-05-21 20:34:06 INFO :      Sent reply
2025-05-21 20:34:40 INFO :      
2025-05-21 20:34:40 INFO :      Received: evaluate message 18bd21ee-3ec6-4cfd-a473-2013e2d3f313
2025-05-21 20:34:48 INFO :      Sent reply
2025-05-21 20:34:59 INFO :      
2025-05-21 20:34:59 INFO :      Received: train message 180ef9bb-008c-4ed9-9920-0ba0db36f2b6
2025-05-21 20:45:40 INFO :      Sent reply
2025-05-21 20:46:13 INFO :      
2025-05-21 20:46:13 INFO :      Received: evaluate message 60ecb761-51f8-468e-88c3-0b2acddf8b77
2025-05-21 20:46:27 INFO :      Sent reply
2025-05-21 20:46:29 INFO :      
2025-05-21 20:46:29 INFO :      Received: reconnect message 925fa6e1-2370-4438-8815-7d42e32a7c39
2025-05-21 20:46:29 INFO :      Disconnect and shut down
