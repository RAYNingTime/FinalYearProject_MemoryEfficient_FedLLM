2025-05-19 23:50:47 
Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]
Generating train split:  88%|████████▊ | 106000/120000 [00:00<00:00, 1044761.63 examples/s]
Generating train split: 100%|██████████| 120000/120000 [00:00<00:00, 1089571.11 examples/s]
2025-05-19 23:50:47 
Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]
Generating test split: 100%|██████████| 7600/7600 [00:00<00:00, 987843.14 examples/s]
2025-05-19 23:50:49 
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 1167.12 examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 1161.70 examples/s]
2025-05-19 23:50:50 
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]
Map:  15%|█▌        | 154/1000 [00:00<00:00, 1518.30 examples/s]
Map:  42%|████▎     | 425/1000 [00:00<00:00, 2211.64 examples/s]
Map:  65%|██████▌   | 653/1000 [00:00<00:00, 2238.44 examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 1864.03 examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 1916.54 examples/s]
2025-05-19 23:50:50 /app/client.py:49: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
2025-05-19 23:50:50   trainer = Trainer(
2025-05-19 23:50:50 WARNING :   DEPRECATED FEATURE: flwr.client.start_numpy_client() is deprecated. 
2025-05-19 23:50:50 Instead, use `flwr.client.start_client()` by ensuring you first call the `.to_client()` method as shown below: 
2025-05-19 23:50:50 flwr.client.start_client(
2025-05-19 23:50:50 server_address='<IP>:<PORT>',
2025-05-19 23:50:50 client=FlowerClient().to_client(), # <-- where FlowerClient is of type flwr.client.NumPyClient object
2025-05-19 23:50:50 )
2025-05-19 23:50:50 Using `start_numpy_client()` is deprecated.
2025-05-19 23:50:50 
2025-05-19 23:50:50             This is a deprecated feature. It will be removed
2025-05-19 23:50:50             entirely in future versions of Flower.
2025-05-19 23:50:50         
2025-05-19 23:50:50 WARNING :   DEPRECATED FEATURE: flwr.client.start_client() is deprecated.
2025-05-19 23:50:50 Instead, use the `flower-supernode` CLI command to start a SuperNode as shown below:
2025-05-19 23:50:50 
2025-05-19 23:50:50 $ flower-supernode --insecure --superlink='<IP>:<PORT>'
2025-05-19 23:50:50 
2025-05-19 23:50:50 To view all available options, run:
2025-05-19 23:50:50 
2025-05-19 23:50:50 $ flower-supernode --help
2025-05-19 23:50:50 
2025-05-19 23:50:50 Using `start_client()` is deprecated.
2025-05-19 23:50:50 
2025-05-19 23:50:50             This is a deprecated feature. It will be removed
2025-05-19 23:50:50             entirely in future versions of Flower.
2025-05-19 23:50:50         
2025-05-19 23:50:57 INFO :      
2025-05-19 23:50:57 INFO :      Received: train message 52da20e4-252d-4c58-86c0-fc277ed80323
2025-05-19 23:58:29 INFO :      Sent reply
2025-05-19 23:58:44 INFO :      
2025-05-19 23:58:44 INFO :      Received: evaluate message 2292a55a-738d-4480-8c16-5c164eb5ed90
2025-05-19 23:58:56 INFO :      Sent reply
2025-05-19 23:59:03 INFO :      
2025-05-19 23:59:03 INFO :      Received: train message ed548485-0258-4b25-8ff3-17481d4dcde9
2025-05-19 23:51:12 {'loss': 2.7598, 'grad_norm': 17.604230880737305, 'learning_rate': 4.8875e-05, 'epoch': 0.025}
2025-05-19 23:51:26 {'loss': 1.543, 'grad_norm': 14.407962799072266, 'learning_rate': 4.7625000000000006e-05, 'epoch': 0.05}
2025-05-19 23:51:36 {'loss': 1.591, 'grad_norm': 13.166777610778809, 'learning_rate': 4.6375e-05, 'epoch': 0.075}
2025-05-19 23:51:46 {'loss': 1.5719, 'grad_norm': 15.105612754821777, 'learning_rate': 4.5125e-05, 'epoch': 0.1}
2025-05-19 23:51:56 {'loss': 1.4166, 'grad_norm': 13.012226104736328, 'learning_rate': 4.3875e-05, 'epoch': 0.125}
2025-05-19 23:52:10 {'loss': 1.5056, 'grad_norm': 12.547675132751465, 'learning_rate': 4.2625000000000006e-05, 'epoch': 0.15}
2025-05-19 23:52:20 {'loss': 1.4671, 'grad_norm': 18.42292022705078, 'learning_rate': 4.1375e-05, 'epoch': 0.175}
2025-05-19 23:52:30 {'loss': 1.5242, 'grad_norm': 15.363168716430664, 'learning_rate': 4.0125e-05, 'epoch': 0.2}
2025-05-19 23:52:40 {'loss': 1.4884, 'grad_norm': 12.928376197814941, 'learning_rate': 3.8875e-05, 'epoch': 0.225}
2025-05-19 23:52:54 {'loss': 1.3895, 'grad_norm': 10.98826789855957, 'learning_rate': 3.7625e-05, 'epoch': 0.25}
2025-05-19 23:53:04 {'loss': 1.4708, 'grad_norm': 13.530718803405762, 'learning_rate': 3.6375e-05, 'epoch': 0.275}
2025-05-19 23:53:14 {'loss': 1.3434, 'grad_norm': 13.533729553222656, 'learning_rate': 3.5125e-05, 'epoch': 0.3}
2025-05-19 23:53:24 {'loss': 1.5699, 'grad_norm': 15.200932502746582, 'learning_rate': 3.3875000000000003e-05, 'epoch': 0.325}
2025-05-19 23:53:37 {'loss': 1.4453, 'grad_norm': 10.833179473876953, 'learning_rate': 3.2625e-05, 'epoch': 0.35}
2025-05-19 23:53:47 {'loss': 1.4371, 'grad_norm': 10.477789878845215, 'learning_rate': 3.1375e-05, 'epoch': 0.375}
2025-05-19 23:53:57 {'loss': 1.486, 'grad_norm': 10.705987930297852, 'learning_rate': 3.0125000000000004e-05, 'epoch': 0.4}
2025-05-19 23:54:11 {'loss': 1.5085, 'grad_norm': 13.890881538391113, 'learning_rate': 2.8875e-05, 'epoch': 0.425}
2025-05-19 23:54:21 {'loss': 1.5706, 'grad_norm': 11.560198783874512, 'learning_rate': 2.7625e-05, 'epoch': 0.45}
2025-05-19 23:54:31 {'loss': 1.3972, 'grad_norm': 8.996017456054688, 'learning_rate': 2.6375e-05, 'epoch': 0.475}
2025-05-19 23:54:41 {'loss': 1.4451, 'grad_norm': 11.047466278076172, 'learning_rate': 2.5124999999999997e-05, 'epoch': 0.5}
2025-05-19 23:54:55 {'loss': 1.3262, 'grad_norm': 12.232487678527832, 'learning_rate': 2.3875e-05, 'epoch': 0.525}
2025-05-19 23:55:05 {'loss': 1.394, 'grad_norm': 16.292150497436523, 'learning_rate': 2.2625e-05, 'epoch': 0.55}
2025-05-19 23:55:15 {'loss': 1.2907, 'grad_norm': 8.96226692199707, 'learning_rate': 2.1375e-05, 'epoch': 0.575}
2025-05-19 23:55:25 {'loss': 1.3861, 'grad_norm': 13.995253562927246, 'learning_rate': 2.0125e-05, 'epoch': 0.6}
2025-05-19 23:55:39 {'loss': 1.4243, 'grad_norm': 14.756558418273926, 'learning_rate': 1.8875e-05, 'epoch': 0.625}
2025-05-19 23:55:48 {'loss': 1.4368, 'grad_norm': 13.60417366027832, 'learning_rate': 1.7625e-05, 'epoch': 0.65}
2025-05-19 23:55:59 {'loss': 1.4688, 'grad_norm': 11.508594512939453, 'learning_rate': 1.6375e-05, 'epoch': 0.675}
2025-05-19 23:56:08 {'loss': 1.4027, 'grad_norm': 20.860326766967773, 'learning_rate': 1.5125e-05, 'epoch': 0.7}
2025-05-19 23:56:22 {'loss': 1.5211, 'grad_norm': 9.909388542175293, 'learning_rate': 1.3875000000000002e-05, 'epoch': 0.725}
2025-05-19 23:56:32 {'loss': 1.3001, 'grad_norm': 11.181875228881836, 'learning_rate': 1.2625e-05, 'epoch': 0.75}
2025-05-19 23:56:42 {'loss': 1.4516, 'grad_norm': 12.733729362487793, 'learning_rate': 1.1375e-05, 'epoch': 0.775}
2025-05-19 23:56:56 {'loss': 1.3772, 'grad_norm': 16.060955047607422, 'learning_rate': 1.0125e-05, 'epoch': 0.8}
2025-05-19 23:57:05 {'loss': 1.3013, 'grad_norm': 10.461155891418457, 'learning_rate': 8.875e-06, 'epoch': 0.825}
2025-05-19 23:57:15 {'loss': 1.3686, 'grad_norm': 12.861287117004395, 'learning_rate': 7.625e-06, 'epoch': 0.85}
2025-05-19 23:57:29 {'loss': 1.3944, 'grad_norm': 11.323580741882324, 'learning_rate': 6.375000000000001e-06, 'epoch': 0.875}
2025-05-19 23:57:39 {'loss': 1.3242, 'grad_norm': 11.004451751708984, 'learning_rate': 5.125e-06, 'epoch': 0.9}
2025-05-19 23:57:49 {'loss': 1.357, 'grad_norm': 11.688297271728516, 'learning_rate': 3.875e-06, 'epoch': 0.925}
2025-05-19 23:57:59 {'loss': 1.4146, 'grad_norm': 8.961406707763672, 'learning_rate': 2.625e-06, 'epoch': 0.95}
2025-05-19 23:58:13 {'loss': 1.6025, 'grad_norm': 17.735198974609375, 'learning_rate': 1.3750000000000002e-06, 'epoch': 0.975}
2025-05-19 23:58:23 {'loss': 1.3325, 'grad_norm': 10.548309326171875, 'learning_rate': 1.2500000000000002e-07, 'epoch': 1.0}
2025-05-19 23:58:23 {'train_runtime': 444.0233, 'train_samples_per_second': 1.802, 'train_steps_per_second': 0.901, 'train_loss': 1.4701423859596252, 'epoch': 1.0}
2025-05-19 23:58:56 {'eval_loss': 1.32596755027771, 'eval_runtime': 11.325, 'eval_samples_per_second': 17.66, 'eval_steps_per_second': 2.208, 'epoch': 1.0}
2025-05-19 23:59:19 {'loss': 0.827, 'grad_norm': 10.056236267089844, 'learning_rate': 4.8875e-05, 'epoch': 0.025}
2025-05-19 23:59:33 {'loss': 0.9502, 'grad_norm': 10.239407539367676, 'learning_rate': 4.7625000000000006e-05, 'epoch': 0.05}
2025-05-19 23:59:42 {'loss': 1.0146, 'grad_norm': 9.249767303466797, 'learning_rate': 4.6375e-05, 'epoch': 0.075}
2025-05-19 23:59:52 {'loss': 1.053, 'grad_norm': 12.840761184692383, 'learning_rate': 4.5125e-05, 'epoch': 0.1}
2025-05-20 00:00:02 {'loss': 0.967, 'grad_norm': 11.121004104614258, 'learning_rate': 4.3875e-05, 'epoch': 0.125}
2025-05-20 00:00:16 {'loss': 1.045, 'grad_norm': 11.978292465209961, 'learning_rate': 4.2625000000000006e-05, 'epoch': 0.15}
2025-05-20 00:00:25 {'loss': 1.0499, 'grad_norm': 16.416696548461914, 'learning_rate': 4.1375e-05, 'epoch': 0.175}
2025-05-20 00:00:35 {'loss': 1.0834, 'grad_norm': 13.916332244873047, 'learning_rate': 4.0125e-05, 'epoch': 0.2}
2025-05-20 00:00:45 {'loss': 1.078, 'grad_norm': 10.052988052368164, 'learning_rate': 3.8875e-05, 'epoch': 0.225}
2025-05-20 00:00:58 {'loss': 1.0336, 'grad_norm': 11.402817726135254, 'learning_rate': 3.7625e-05, 'epoch': 0.25}
2025-05-20 00:01:08 {'loss': 1.0681, 'grad_norm': 11.239121437072754, 'learning_rate': 3.6375e-05, 'epoch': 0.275}
2025-05-20 00:01:17 {'loss': 0.9906, 'grad_norm': 10.320005416870117, 'learning_rate': 3.5125e-05, 'epoch': 0.3}
2025-05-20 00:01:31 {'loss': 1.1484, 'grad_norm': 12.48259162902832, 'learning_rate': 3.3875000000000003e-05, 'epoch': 0.325}
2025-05-20 00:01:41 {'loss': 1.0292, 'grad_norm': 9.416449546813965, 'learning_rate': 3.2625e-05, 'epoch': 0.35}
2025-05-20 00:01:50 {'loss': 1.08, 'grad_norm': 9.665884971618652, 'learning_rate': 3.1375e-05, 'epoch': 0.375}
2025-05-20 00:02:04 {'loss': 1.0896, 'grad_norm': 9.044415473937988, 'learning_rate': 3.0125000000000004e-05, 'epoch': 0.4}
2025-05-20 00:02:14 {'loss': 1.1524, 'grad_norm': 12.395891189575195, 'learning_rate': 2.8875e-05, 'epoch': 0.425}
2025-05-20 00:02:23 {'loss': 1.2146, 'grad_norm': 11.056660652160645, 'learning_rate': 2.7625e-05, 'epoch': 0.45}
2025-05-20 00:02:33 {'loss': 1.0969, 'grad_norm': 8.201874732971191, 'learning_rate': 2.6375e-05, 'epoch': 0.475}
2025-05-20 00:02:47 {'loss': 1.1315, 'grad_norm': 9.775704383850098, 'learning_rate': 2.5124999999999997e-05, 'epoch': 0.5}
2025-05-20 00:02:56 {'loss': 1.0436, 'grad_norm': 14.277939796447754, 'learning_rate': 2.3875e-05, 'epoch': 0.525}
2025-05-20 00:03:06 {'loss': 1.1174, 'grad_norm': 13.05564022064209, 'learning_rate': 2.2625e-05, 'epoch': 0.55}
2025-05-20 00:03:16 {'loss': 1.0223, 'grad_norm': 7.9885640144348145, 'learning_rate': 2.1375e-05, 'epoch': 0.575}
2025-05-20 00:03:30 {'loss': 1.132, 'grad_norm': 10.283963203430176, 'learning_rate': 2.0125e-05, 'epoch': 0.6}
2025-05-20 00:03:39 {'loss': 1.1708, 'grad_norm': 13.194202423095703, 'learning_rate': 1.8875e-05, 'epoch': 0.625}
2025-05-20 00:03:49 {'loss': 1.1881, 'grad_norm': 12.392294883728027, 'learning_rate': 1.7625e-05, 'epoch': 0.65}
2025-05-20 00:04:03 {'loss': 1.2342, 'grad_norm': 11.447757720947266, 'learning_rate': 1.6375e-05, 'epoch': 0.675}
2025-05-20 00:04:12 {'loss': 1.1673, 'grad_norm': 13.118592262268066, 'learning_rate': 1.5125e-05, 'epoch': 0.7}
2025-05-20 00:04:22 {'loss': 1.3091, 'grad_norm': 9.305563926696777, 'learning_rate': 1.3875000000000002e-05, 'epoch': 0.725}
2025-05-20 00:04:32 {'loss': 1.1022, 'grad_norm': 9.58268928527832, 'learning_rate': 1.2625e-05, 'epoch': 0.75}
2025-05-20 00:04:45 {'loss': 1.2728, 'grad_norm': 15.293339729309082, 'learning_rate': 1.1375e-05, 'epoch': 0.775}
2025-05-20 00:04:55 {'loss': 1.2099, 'grad_norm': 16.392236709594727, 'learning_rate': 1.0125e-05, 'epoch': 0.8}
2025-05-20 00:05:05 {'loss': 1.1519, 'grad_norm': 10.654824256896973, 'learning_rate': 8.875e-06, 'epoch': 0.825}
2025-05-20 00:05:15 {'loss': 1.2416, 'grad_norm': 11.718361854553223, 'learning_rate': 7.625e-06, 'epoch': 0.85}
2025-05-20 00:05:28 {'loss': 1.27, 'grad_norm': 11.517179489135742, 'learning_rate': 6.375000000000001e-06, 'epoch': 0.875}
2025-05-20 00:05:38 {'loss': 1.2175, 'grad_norm': 10.89023494720459, 'learning_rate': 5.125e-06, 'epoch': 0.9}
2025-05-20 00:05:48 {'loss': 1.2363, 'grad_norm': 12.534943580627441, 'learning_rate': 3.875e-06, 'epoch': 0.925}
2025-05-20 00:06:02 {'loss': 1.2377, 'grad_norm': 8.596918106079102, 'learning_rate': 2.625e-06, 'epoch': 0.95}
2025-05-20 00:06:12 {'loss': 1.2702, 'grad_norm': 14.302099227905273, 'learning_rate': 1.3750000000000002e-06, 'epoch': 0.975}
2025-05-20 00:06:26 {'loss': 0.804, 'grad_norm': 6.725569725036621, 'learning_rate': 1.2500000000000002e-07, 'epoch': 1.0}
2025-05-20 00:06:26 {'train_runtime': 438.2718, 'train_samples_per_second': 1.825, 'train_steps_per_second': 0.913, 'train_loss': 1.112543511390686, 'epoch': 1.0}
2025-05-20 00:07:01 {'eval_loss': 1.3495062589645386, 'eval_runtime': 8.9747, 'eval_samples_per_second': 22.285, 'eval_steps_per_second': 2.786, 'epoch': 1.0}
2025-05-20 00:07:25 {'loss': 0.5129, 'grad_norm': 11.489021301269531, 'learning_rate': 4.8875e-05, 'epoch': 0.025}
2025-05-20 00:07:35 {'loss': 0.6408, 'grad_norm': 12.111141204833984, 'learning_rate': 4.7625000000000006e-05, 'epoch': 0.05}
2025-05-20 00:07:45 {'loss': 0.6628, 'grad_norm': 7.799410343170166, 'learning_rate': 4.6375e-05, 'epoch': 0.075}
2025-05-20 00:07:59 {'loss': 0.7399, 'grad_norm': 11.530773162841797, 'learning_rate': 4.5125e-05, 'epoch': 0.1}
2025-05-20 00:08:09 {'loss': 0.6975, 'grad_norm': 9.437570571899414, 'learning_rate': 4.3875e-05, 'epoch': 0.125}
2025-05-20 00:08:19 {'loss': 0.7828, 'grad_norm': 10.842164993286133, 'learning_rate': 4.2625000000000006e-05, 'epoch': 0.15}
2025-05-20 00:08:29 {'loss': 0.807, 'grad_norm': 21.333032608032227, 'learning_rate': 4.1375e-05, 'epoch': 0.175}
2025-05-20 00:08:42 {'loss': 0.8255, 'grad_norm': 11.621583938598633, 'learning_rate': 4.0125e-05, 'epoch': 0.2}
2025-05-20 00:08:52 {'loss': 0.8404, 'grad_norm': 9.87329387664795, 'learning_rate': 3.8875e-05, 'epoch': 0.225}
2025-05-20 00:09:07 {'loss': 0.7833, 'grad_norm': 11.872110366821289, 'learning_rate': 3.7625e-05, 'epoch': 0.25}
2025-05-20 00:09:17 {'loss': 0.8379, 'grad_norm': 9.557924270629883, 'learning_rate': 3.6375e-05, 'epoch': 0.275}
2025-05-20 00:09:31 {'loss': 0.7593, 'grad_norm': 9.520954132080078, 'learning_rate': 3.5125e-05, 'epoch': 0.3}
2025-05-20 00:09:41 {'loss': 0.8848, 'grad_norm': 12.031746864318848, 'learning_rate': 3.3875000000000003e-05, 'epoch': 0.325}
2025-05-20 00:09:55 {'loss': 0.8438, 'grad_norm': 8.632894515991211, 'learning_rate': 3.2625e-05, 'epoch': 0.35}
2025-05-20 00:10:05 {'loss': 0.8316, 'grad_norm': 8.869159698486328, 'learning_rate': 3.1375e-05, 'epoch': 0.375}
2025-05-20 00:10:19 {'loss': 0.831, 'grad_norm': 10.425603866577148, 'learning_rate': 3.0125000000000004e-05, 'epoch': 0.4}
2025-05-20 00:10:29 {'loss': 0.9293, 'grad_norm': 11.153143882751465, 'learning_rate': 2.8875e-05, 'epoch': 0.425}
2025-05-20 00:10:43 {'loss': 1.0058, 'grad_norm': 9.411910057067871, 'learning_rate': 2.7625e-05, 'epoch': 0.45}
2025-05-20 00:10:53 {'loss': 0.9103, 'grad_norm': 8.1295166015625, 'learning_rate': 2.6375e-05, 'epoch': 0.475}
2025-05-20 00:11:07 {'loss': 0.9401, 'grad_norm': 11.864034652709961, 'learning_rate': 2.5124999999999997e-05, 'epoch': 0.5}
2025-05-20 00:11:17 {'loss': 0.8389, 'grad_norm': 11.455558776855469, 'learning_rate': 2.3875e-05, 'epoch': 0.525}
2025-05-20 00:11:31 {'loss': 0.9354, 'grad_norm': 13.176097869873047, 'learning_rate': 2.2625e-05, 'epoch': 0.55}
2025-05-20 00:11:41 {'loss': 0.8542, 'grad_norm': 7.5193986892700195, 'learning_rate': 2.1375e-05, 'epoch': 0.575}
2025-05-20 00:11:55 {'loss': 0.968, 'grad_norm': 14.493233680725098, 'learning_rate': 2.0125e-05, 'epoch': 0.6}
2025-05-20 00:12:05 {'loss': 1.0249, 'grad_norm': 11.097898483276367, 'learning_rate': 1.8875e-05, 'epoch': 0.625}
2025-05-20 00:12:19 {'loss': 1.0094, 'grad_norm': 10.568965911865234, 'learning_rate': 1.7625e-05, 'epoch': 0.65}
2025-05-20 00:12:29 {'loss': 1.0685, 'grad_norm': 9.882952690124512, 'learning_rate': 1.6375e-05, 'epoch': 0.675}
2025-05-20 00:12:43 {'loss': 0.9961, 'grad_norm': 13.931929588317871, 'learning_rate': 1.5125e-05, 'epoch': 0.7}
2025-05-20 00:12:53 {'loss': 1.1642, 'grad_norm': 10.143982887268066, 'learning_rate': 1.3875000000000002e-05, 'epoch': 0.725}
2025-05-20 00:13:07 {'loss': 0.9696, 'grad_norm': 8.962944984436035, 'learning_rate': 1.2625e-05, 'epoch': 0.75}
2025-05-20 00:13:17 {'loss': 1.1531, 'grad_norm': 11.717218399047852, 'learning_rate': 1.1375e-05, 'epoch': 0.775}
2025-05-20 00:13:30 {'loss': 1.0981, 'grad_norm': 15.267848014831543, 'learning_rate': 1.0125e-05, 'epoch': 0.8}
2025-05-20 00:13:41 {'loss': 1.0743, 'grad_norm': 9.388564109802246, 'learning_rate': 8.875e-06, 'epoch': 0.825}
2025-05-20 00:13:55 {'loss': 1.1634, 'grad_norm': 15.241008758544922, 'learning_rate': 7.625e-06, 'epoch': 0.85}
2025-05-20 00:14:04 {'loss': 1.2061, 'grad_norm': 10.806334495544434, 'learning_rate': 6.375000000000001e-06, 'epoch': 0.875}
2025-05-20 00:14:18 {'loss': 1.1703, 'grad_norm': 11.667875289916992, 'learning_rate': 5.125e-06, 'epoch': 0.9}
2025-05-20 00:14:28 {'loss': 1.1796, 'grad_norm': 12.930315017700195, 'learning_rate': 3.875e-06, 'epoch': 0.925}
2025-05-20 00:14:34 {'loss': 1.1929, 'grad_norm': 8.165425300598145, 'learning_rate': 2.625e-06, 'epoch': 0.95}
2025-05-20 00:14:36 {'loss': 1.1421, 'grad_norm': 13.912237167358398, 'learning_rate': 1.3750000000000002e-06, 'epoch': 0.975}
2025-05-20 00:14:38 {'loss': 0.6309, 'grad_norm': 7.848698616027832, 'learning_rate': 1.2500000000000002e-07, 'epoch': 1.0}
2025-05-20 00:14:38 {'train_runtime': 448.858, 'train_samples_per_second': 1.782, 'train_steps_per_second': 0.891, 'train_loss': 0.922669174671173, 'epoch': 1.0}
2025-05-20 00:15:18 {'eval_loss': 1.3855903148651123, 'eval_runtime': 10.9008, 'eval_samples_per_second': 18.347, 'eval_steps_per_second': 2.293, 'epoch': 1.0}
2025-05-20 00:15:42 {'loss': 0.3083, 'grad_norm': 9.12144660949707, 'learning_rate': 4.8875e-05, 'epoch': 0.025}
2025-05-20 00:15:52 {'loss': 0.4039, 'grad_norm': 9.76573371887207, 'learning_rate': 4.7625000000000006e-05, 'epoch': 0.05}
2025-05-20 00:16:06 {'loss': 0.432, 'grad_norm': 7.550611972808838, 'learning_rate': 4.6375e-05, 'epoch': 0.075}
2025-05-20 00:16:16 {'loss': 0.5213, 'grad_norm': 11.353484153747559, 'learning_rate': 4.5125e-05, 'epoch': 0.1}
2025-05-20 00:16:30 {'loss': 0.5035, 'grad_norm': 10.659130096435547, 'learning_rate': 4.3875e-05, 'epoch': 0.125}
2025-05-20 00:16:44 {'loss': 0.5897, 'grad_norm': 10.192403793334961, 'learning_rate': 4.2625000000000006e-05, 'epoch': 0.15}
2025-05-20 00:16:54 {'loss': 0.5927, 'grad_norm': 14.808211326599121, 'learning_rate': 4.1375e-05, 'epoch': 0.175}
2025-05-20 00:17:08 {'loss': 0.6136, 'grad_norm': 12.15475845336914, 'learning_rate': 4.0125e-05, 'epoch': 0.2}
2025-05-20 00:17:19 {'loss': 0.6242, 'grad_norm': 9.392136573791504, 'learning_rate': 3.8875e-05, 'epoch': 0.225}
2025-05-20 00:17:32 {'loss': 0.5799, 'grad_norm': 8.492496490478516, 'learning_rate': 3.7625e-05, 'epoch': 0.25}
2025-05-20 00:17:42 {'loss': 0.6359, 'grad_norm': 10.487098693847656, 'learning_rate': 3.6375e-05, 'epoch': 0.275}
2025-05-20 00:17:56 {'loss': 0.5777, 'grad_norm': 9.010303497314453, 'learning_rate': 3.5125e-05, 'epoch': 0.3}
2025-05-20 00:18:06 {'loss': 0.7082, 'grad_norm': 10.826310157775879, 'learning_rate': 3.3875000000000003e-05, 'epoch': 0.325}
2025-05-20 00:18:19 {'loss': 0.6697, 'grad_norm': 7.811765193939209, 'learning_rate': 3.2625e-05, 'epoch': 0.35}
2025-05-20 00:18:29 {'loss': 0.6703, 'grad_norm': 8.054680824279785, 'learning_rate': 3.1375e-05, 'epoch': 0.375}
2025-05-20 00:18:43 {'loss': 0.659, 'grad_norm': 9.787676811218262, 'learning_rate': 3.0125000000000004e-05, 'epoch': 0.4}
2025-05-20 00:18:53 {'loss': 0.7445, 'grad_norm': 10.730347633361816, 'learning_rate': 2.8875e-05, 'epoch': 0.425}
2025-05-20 00:19:07 {'loss': 0.8154, 'grad_norm': 12.571490287780762, 'learning_rate': 2.7625e-05, 'epoch': 0.45}
2025-05-20 00:19:17 {'loss': 0.7502, 'grad_norm': 7.6857805252075195, 'learning_rate': 2.6375e-05, 'epoch': 0.475}
2025-05-20 00:19:30 {'loss': 0.7708, 'grad_norm': 8.001895904541016, 'learning_rate': 2.5124999999999997e-05, 'epoch': 0.5}
2025-05-20 00:19:40 {'loss': 0.6922, 'grad_norm': 10.570931434631348, 'learning_rate': 2.3875e-05, 'epoch': 0.525}
2025-05-20 00:19:54 {'loss': 0.7687, 'grad_norm': 12.032798767089844, 'learning_rate': 2.2625e-05, 'epoch': 0.55}
2025-05-20 00:20:04 {'loss': 0.7276, 'grad_norm': 8.112418174743652, 'learning_rate': 2.1375e-05, 'epoch': 0.575}
2025-05-20 00:06:32 INFO :      Sent reply
2025-05-20 00:06:49 INFO :      
2025-05-20 00:06:49 INFO :      Received: evaluate message e0acc389-a2be-4eec-984e-ac2b61c28e1d
2025-05-20 00:07:01 INFO :      Sent reply
2025-05-20 00:07:06 INFO :      
2025-05-20 00:07:06 INFO :      Received: train message bef2177a-5b49-487e-ae6e-a7cc4abff78b
2025-05-20 00:14:42 INFO :      Sent reply
2025-05-20 00:15:06 INFO :      
2025-05-20 00:15:06 INFO :      Received: evaluate message aa769739-3709-427c-8584-6be4198ff946
2025-05-20 00:15:18 INFO :      Sent reply
2025-05-20 00:15:23 INFO :      
2025-05-20 00:15:23 INFO :      Received: train message 90f08921-be61-4753-b295-a8938997b118
2025-05-20 00:23:04 INFO :      Sent reply
2025-05-20 00:23:22 INFO :      
2025-05-20 00:23:22 INFO :      Received: evaluate message 28ecc87a-5b07-452b-95a3-87004773325c
2025-05-20 00:23:34 INFO :      Sent reply
2025-05-20 00:23:42 INFO :      
2025-05-20 00:23:42 INFO :      Received: train message d200708b-3aac-4b00-8e26-693dea9e94e5
2025-05-20 00:31:06 INFO :      Sent reply
2025-05-20 00:31:34 INFO :      
2025-05-20 00:31:34 INFO :      Received: evaluate message c7e36379-7cc5-4df0-9cf0-0c6aa8882d17
2025-05-20 00:31:47 INFO :      Sent reply
2025-05-20 00:31:47 INFO :      
2025-05-20 00:31:47 INFO :      Received: reconnect message c569152b-6a8b-432f-a2fd-d4d23504ba41
2025-05-20 00:31:47 INFO :      Disconnect and shut down
2025-05-20 00:20:14 {'loss': 0.811, 'grad_norm': 9.612454414367676, 'learning_rate': 2.0125e-05, 'epoch': 0.6}
2025-05-20 00:20:24 {'loss': 0.8712, 'grad_norm': 11.079358100891113, 'learning_rate': 1.8875e-05, 'epoch': 0.625}
2025-05-20 00:20:38 {'loss': 0.8757, 'grad_norm': 10.978432655334473, 'learning_rate': 1.7625e-05, 'epoch': 0.65}
2025-05-20 00:20:51 {'loss': 0.947, 'grad_norm': 10.154748916625977, 'learning_rate': 1.6375e-05, 'epoch': 0.675}
2025-05-20 00:21:02 {'loss': 0.8988, 'grad_norm': 14.704151153564453, 'learning_rate': 1.5125e-05, 'epoch': 0.7}
2025-05-20 00:21:15 {'loss': 1.0429, 'grad_norm': 9.538503646850586, 'learning_rate': 1.3875000000000002e-05, 'epoch': 0.725}
2025-05-20 00:21:29 {'loss': 0.8713, 'grad_norm': 10.000080108642578, 'learning_rate': 1.2625e-05, 'epoch': 0.75}
2025-05-20 00:21:40 {'loss': 1.0657, 'grad_norm': 12.189724922180176, 'learning_rate': 1.1375e-05, 'epoch': 0.775}
2025-05-20 00:21:54 {'loss': 1.0046, 'grad_norm': 16.866321563720703, 'learning_rate': 1.0125e-05, 'epoch': 0.8}
2025-05-20 00:22:08 {'loss': 1.0099, 'grad_norm': 11.09628677368164, 'learning_rate': 8.875e-06, 'epoch': 0.825}
2025-05-20 00:22:18 {'loss': 1.102, 'grad_norm': 12.98698902130127, 'learning_rate': 7.625e-06, 'epoch': 0.85}
2025-05-20 00:22:32 {'loss': 1.1294, 'grad_norm': 11.79552173614502, 'learning_rate': 6.375000000000001e-06, 'epoch': 0.875}
2025-05-20 00:22:46 {'loss': 1.1203, 'grad_norm': 13.294706344604492, 'learning_rate': 5.125e-06, 'epoch': 0.9}
2025-05-20 00:22:52 {'loss': 1.1493, 'grad_norm': 12.664468765258789, 'learning_rate': 3.875e-06, 'epoch': 0.925}
2025-05-20 00:22:55 {'loss': 1.1185, 'grad_norm': 11.293660163879395, 'learning_rate': 2.625e-06, 'epoch': 0.95}
2025-05-20 00:22:58 {'loss': 1.0267, 'grad_norm': 13.432673454284668, 'learning_rate': 1.3750000000000002e-06, 'epoch': 0.975}
2025-05-20 00:23:00 {'loss': 0.5189, 'grad_norm': 4.824779510498047, 'learning_rate': 1.2500000000000002e-07, 'epoch': 1.0}
2025-05-20 00:23:00 {'train_runtime': 453.8323, 'train_samples_per_second': 1.763, 'train_steps_per_second': 0.881, 'train_loss': 0.7730578720569611, 'epoch': 1.0}
2025-05-20 00:23:34 {'eval_loss': 1.4242509603500366, 'eval_runtime': 11.6956, 'eval_samples_per_second': 17.1, 'eval_steps_per_second': 2.138, 'epoch': 1.0}
2025-05-20 00:23:54 {'loss': 0.2069, 'grad_norm': 5.809207916259766, 'learning_rate': 4.8875e-05, 'epoch': 0.025}
2025-05-20 00:24:08 {'loss': 0.2719, 'grad_norm': 11.527735710144043, 'learning_rate': 4.7625000000000006e-05, 'epoch': 0.05}
2025-05-20 00:24:18 {'loss': 0.2962, 'grad_norm': 7.591919422149658, 'learning_rate': 4.6375e-05, 'epoch': 0.075}
2025-05-20 00:24:28 {'loss': 0.3466, 'grad_norm': 9.304315567016602, 'learning_rate': 4.5125e-05, 'epoch': 0.1}
2025-05-20 00:24:41 {'loss': 0.3612, 'grad_norm': 7.825960636138916, 'learning_rate': 4.3875e-05, 'epoch': 0.125}
2025-05-20 00:24:51 {'loss': 0.4054, 'grad_norm': 8.235962867736816, 'learning_rate': 4.2625000000000006e-05, 'epoch': 0.15}
2025-05-20 00:25:01 {'loss': 0.421, 'grad_norm': 16.013750076293945, 'learning_rate': 4.1375e-05, 'epoch': 0.175}
2025-05-20 00:25:11 {'loss': 0.4708, 'grad_norm': 10.811594009399414, 'learning_rate': 4.0125e-05, 'epoch': 0.2}
2025-05-20 00:25:25 {'loss': 0.4691, 'grad_norm': 9.472826957702637, 'learning_rate': 3.8875e-05, 'epoch': 0.225}
2025-05-20 00:25:34 {'loss': 0.4413, 'grad_norm': 10.31852912902832, 'learning_rate': 3.7625e-05, 'epoch': 0.25}
2025-05-20 00:25:44 {'loss': 0.464, 'grad_norm': 8.358574867248535, 'learning_rate': 3.6375e-05, 'epoch': 0.275}
2025-05-20 00:25:54 {'loss': 0.4616, 'grad_norm': 8.233656883239746, 'learning_rate': 3.5125e-05, 'epoch': 0.3}
2025-05-20 00:26:07 {'loss': 0.5509, 'grad_norm': 10.386112213134766, 'learning_rate': 3.3875000000000003e-05, 'epoch': 0.325}
2025-05-20 00:26:17 {'loss': 0.5168, 'grad_norm': 6.408851623535156, 'learning_rate': 3.2625e-05, 'epoch': 0.35}
2025-05-20 00:26:27 {'loss': 0.5243, 'grad_norm': 7.9726386070251465, 'learning_rate': 3.1375e-05, 'epoch': 0.375}
2025-05-20 00:26:40 {'loss': 0.5138, 'grad_norm': 9.44582748413086, 'learning_rate': 3.0125000000000004e-05, 'epoch': 0.4}
2025-05-20 00:26:50 {'loss': 0.5793, 'grad_norm': 11.578957557678223, 'learning_rate': 2.8875e-05, 'epoch': 0.425}
2025-05-20 00:27:00 {'loss': 0.6668, 'grad_norm': 10.000980377197266, 'learning_rate': 2.7625e-05, 'epoch': 0.45}
2025-05-20 00:27:09 {'loss': 0.5801, 'grad_norm': 8.269977569580078, 'learning_rate': 2.6375e-05, 'epoch': 0.475}
2025-05-20 00:27:23 {'loss': 0.6218, 'grad_norm': 8.788226127624512, 'learning_rate': 2.5124999999999997e-05, 'epoch': 0.5}
2025-05-20 00:27:33 {'loss': 0.5787, 'grad_norm': 11.002134323120117, 'learning_rate': 2.3875e-05, 'epoch': 0.525}
2025-05-20 00:27:43 {'loss': 0.6473, 'grad_norm': 12.760671615600586, 'learning_rate': 2.2625e-05, 'epoch': 0.55}
2025-05-20 00:27:53 {'loss': 0.6157, 'grad_norm': 6.5994367599487305, 'learning_rate': 2.1375e-05, 'epoch': 0.575}
2025-05-20 00:28:06 {'loss': 0.6875, 'grad_norm': 10.284135818481445, 'learning_rate': 2.0125e-05, 'epoch': 0.6}
2025-05-20 00:28:16 {'loss': 0.7631, 'grad_norm': 10.411426544189453, 'learning_rate': 1.8875e-05, 'epoch': 0.625}
2025-05-20 00:28:26 {'loss': 0.7664, 'grad_norm': 11.550947189331055, 'learning_rate': 1.7625e-05, 'epoch': 0.65}
2025-05-20 00:28:39 {'loss': 0.8345, 'grad_norm': 10.682953834533691, 'learning_rate': 1.6375e-05, 'epoch': 0.675}
2025-05-20 00:28:49 {'loss': 0.7945, 'grad_norm': 14.124815940856934, 'learning_rate': 1.5125e-05, 'epoch': 0.7}
2025-05-20 00:28:59 {'loss': 0.9275, 'grad_norm': 9.276603698730469, 'learning_rate': 1.3875000000000002e-05, 'epoch': 0.725}
2025-05-20 00:29:12 {'loss': 0.7896, 'grad_norm': 9.71500301361084, 'learning_rate': 1.2625e-05, 'epoch': 0.75}
2025-05-20 00:29:22 {'loss': 0.9621, 'grad_norm': 12.013324737548828, 'learning_rate': 1.1375e-05, 'epoch': 0.775}
2025-05-20 00:29:32 {'loss': 0.9405, 'grad_norm': 15.092470169067383, 'learning_rate': 1.0125e-05, 'epoch': 0.8}
2025-05-20 00:29:42 {'loss': 0.953, 'grad_norm': 8.870141983032227, 'learning_rate': 8.875e-06, 'epoch': 0.825}
2025-05-20 00:29:55 {'loss': 1.0629, 'grad_norm': 13.449620246887207, 'learning_rate': 7.625e-06, 'epoch': 0.85}
2025-05-20 00:30:05 {'loss': 1.0854, 'grad_norm': 13.941655158996582, 'learning_rate': 6.375000000000001e-06, 'epoch': 0.875}
2025-05-20 00:30:15 {'loss': 1.0953, 'grad_norm': 12.295432090759277, 'learning_rate': 5.125e-06, 'epoch': 0.9}
2025-05-20 00:30:29 {'loss': 1.1155, 'grad_norm': 13.191505432128906, 'learning_rate': 3.875e-06, 'epoch': 0.925}
2025-05-20 00:30:38 {'loss': 1.0839, 'grad_norm': 8.998685836791992, 'learning_rate': 2.625e-06, 'epoch': 0.95}
2025-05-20 00:30:48 {'loss': 0.9643, 'grad_norm': 14.03757381439209, 'learning_rate': 1.3750000000000002e-06, 'epoch': 0.975}
2025-05-20 00:30:58 {'loss': 0.4119, 'grad_norm': 7.479173183441162, 'learning_rate': 1.2500000000000002e-07, 'epoch': 1.0}
2025-05-20 00:30:58 {'train_runtime': 434.4984, 'train_samples_per_second': 1.841, 'train_steps_per_second': 0.921, 'train_loss': 0.656239498257637, 'epoch': 1.0}
2025-05-20 00:31:47 {'eval_loss': 1.46854567527771, 'eval_runtime': 11.4427, 'eval_samples_per_second': 17.478, 'eval_steps_per_second': 2.185, 'epoch': 1.0}
