2025-05-19 19:52:24 
Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]
Generating train split:  82%|████████▏ | 98000/120000 [00:00<00:00, 974973.00 examples/s]
Generating train split: 100%|██████████| 120000/120000 [00:00<00:00, 1039847.78 examples/s]
2025-05-19 19:52:24 
Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]
Generating test split: 100%|██████████| 7600/7600 [00:00<00:00, 994965.68 examples/s]
2025-05-19 19:52:26 
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 1133.55 examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 1127.32 examples/s]
2025-05-19 19:52:27 
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]
Map:  25%|██▌       | 254/1000 [00:00<00:00, 2516.12 examples/s]
Map:  54%|█████▎    | 535/1000 [00:00<00:00, 2073.19 examples/s]
Map:  82%|████████▏ | 816/1000 [00:00<00:00, 1966.70 examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 1933.18 examples/s]
2025-05-19 19:52:27 /app/client.py:49: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
2025-05-19 19:52:27   trainer = Trainer(
2025-05-19 19:52:28 WARNING :   DEPRECATED FEATURE: flwr.client.start_numpy_client() is deprecated. 
2025-05-19 19:52:28 Instead, use `flwr.client.start_client()` by ensuring you first call the `.to_client()` method as shown below: 
2025-05-19 19:52:28 flwr.client.start_client(
2025-05-19 19:52:28 server_address='<IP>:<PORT>',
2025-05-19 19:52:28 client=FlowerClient().to_client(), # <-- where FlowerClient is of type flwr.client.NumPyClient object
2025-05-19 19:52:28 )
2025-05-19 19:52:28 Using `start_numpy_client()` is deprecated.
2025-05-19 19:52:28 
2025-05-19 19:52:28             This is a deprecated feature. It will be removed
2025-05-19 19:52:28             entirely in future versions of Flower.
2025-05-19 19:52:28         
2025-05-19 19:52:28 WARNING :   DEPRECATED FEATURE: flwr.client.start_client() is deprecated.
2025-05-19 19:52:28 Instead, use the `flower-supernode` CLI command to start a SuperNode as shown below:
2025-05-19 19:52:28 
2025-05-19 19:52:28 $ flower-supernode --insecure --superlink='<IP>:<PORT>'
2025-05-19 19:52:28 
2025-05-19 19:52:28 To view all available options, run:
2025-05-19 19:52:28 
2025-05-19 19:52:28 $ flower-supernode --help
2025-05-19 19:52:28 
2025-05-19 19:52:28 Using `start_client()` is deprecated.
2025-05-19 19:52:28 
2025-05-19 19:52:28             This is a deprecated feature. It will be removed
2025-05-19 19:52:28             entirely in future versions of Flower.
2025-05-19 19:52:28         
2025-05-19 19:52:28 INFO :      
2025-05-19 19:52:28 INFO :      Received: get_parameters message c44b2d8d-4a1b-448a-b117-183ead547cef
2025-05-19 19:52:32 INFO :      Sent reply
2025-05-19 19:52:57 INFO :      
2025-05-19 19:52:57 INFO :      Received: train message 360737be-1907-411d-95ae-98184aded80e
2025-05-19 19:53:25 {'loss': 2.779, 'grad_norm': 15.33484172821045, 'learning_rate': 4.8875e-05, 'epoch': 0.025}
2025-05-19 19:53:35 {'loss': 1.6937, 'grad_norm': 24.514694213867188, 'learning_rate': 4.7625000000000006e-05, 'epoch': 0.05}
2025-05-19 19:53:45 {'loss': 1.3988, 'grad_norm': 18.37212371826172, 'learning_rate': 4.6375e-05, 'epoch': 0.075}
2025-05-19 19:53:55 {'loss': 1.5413, 'grad_norm': 12.579385757446289, 'learning_rate': 4.5125e-05, 'epoch': 0.1}
2025-05-19 19:54:10 {'loss': 1.5633, 'grad_norm': 13.248905181884766, 'learning_rate': 4.3875e-05, 'epoch': 0.125}
2025-05-19 19:54:20 {'loss': 1.7557, 'grad_norm': 17.898086547851562, 'learning_rate': 4.2625000000000006e-05, 'epoch': 0.15}
2025-05-19 19:54:30 {'loss': 1.5492, 'grad_norm': 15.867897987365723, 'learning_rate': 4.1375e-05, 'epoch': 0.175}
2025-05-19 19:54:40 {'loss': 1.5249, 'grad_norm': 10.537339210510254, 'learning_rate': 4.0125e-05, 'epoch': 0.2}
2025-05-19 19:54:54 {'loss': 1.4984, 'grad_norm': 11.082659721374512, 'learning_rate': 3.8875e-05, 'epoch': 0.225}
2025-05-19 19:55:05 {'loss': 1.3876, 'grad_norm': 11.777989387512207, 'learning_rate': 3.7625e-05, 'epoch': 0.25}
2025-05-19 19:55:15 {'loss': 1.3508, 'grad_norm': 10.944311141967773, 'learning_rate': 3.6375e-05, 'epoch': 0.275}
2025-05-19 19:55:25 {'loss': 1.5242, 'grad_norm': 11.815328598022461, 'learning_rate': 3.5125e-05, 'epoch': 0.3}
2025-05-19 19:55:39 {'loss': 1.4819, 'grad_norm': 10.80756664276123, 'learning_rate': 3.3875000000000003e-05, 'epoch': 0.325}
2025-05-19 19:55:50 {'loss': 1.6342, 'grad_norm': 12.772502899169922, 'learning_rate': 3.2625e-05, 'epoch': 0.35}
2025-05-19 19:56:00 {'loss': 1.3705, 'grad_norm': 9.417085647583008, 'learning_rate': 3.1375e-05, 'epoch': 0.375}
2025-05-19 19:56:14 {'loss': 1.3622, 'grad_norm': 13.629802703857422, 'learning_rate': 3.0125000000000004e-05, 'epoch': 0.4}
2025-05-19 19:56:24 {'loss': 1.5365, 'grad_norm': 12.325438499450684, 'learning_rate': 2.8875e-05, 'epoch': 0.425}
2025-05-19 19:56:34 {'loss': 1.4302, 'grad_norm': 9.572359085083008, 'learning_rate': 2.7625e-05, 'epoch': 0.45}
2025-05-19 19:56:44 {'loss': 1.5359, 'grad_norm': 17.035903930664062, 'learning_rate': 2.6375e-05, 'epoch': 0.475}
2025-05-19 19:56:59 {'loss': 1.3919, 'grad_norm': 14.22335433959961, 'learning_rate': 2.5124999999999997e-05, 'epoch': 0.5}
2025-05-19 19:57:09 {'loss': 1.3973, 'grad_norm': 13.49631118774414, 'learning_rate': 2.3875e-05, 'epoch': 0.525}
2025-05-19 19:57:19 {'loss': 1.3348, 'grad_norm': 9.85044002532959, 'learning_rate': 2.2625e-05, 'epoch': 0.55}
2025-05-19 19:57:29 {'loss': 1.3042, 'grad_norm': 12.208086013793945, 'learning_rate': 2.1375e-05, 'epoch': 0.575}
2025-05-19 19:57:42 {'loss': 1.4083, 'grad_norm': 12.212202072143555, 'learning_rate': 2.0125e-05, 'epoch': 0.6}
2025-05-19 19:57:52 {'loss': 1.3802, 'grad_norm': 15.282905578613281, 'learning_rate': 1.8875e-05, 'epoch': 0.625}
2025-05-19 19:58:02 {'loss': 1.4742, 'grad_norm': 12.77016544342041, 'learning_rate': 1.7625e-05, 'epoch': 0.65}
2025-05-19 19:58:12 {'loss': 1.3258, 'grad_norm': 12.3956880569458, 'learning_rate': 1.6375e-05, 'epoch': 0.675}
2025-05-19 19:58:26 {'loss': 1.3029, 'grad_norm': 14.919289588928223, 'learning_rate': 1.5125e-05, 'epoch': 0.7}
2025-05-19 19:58:36 {'loss': 1.4238, 'grad_norm': 12.70351791381836, 'learning_rate': 1.3875000000000002e-05, 'epoch': 0.725}
2025-05-19 19:58:46 {'loss': 1.391, 'grad_norm': 14.199056625366211, 'learning_rate': 1.2625e-05, 'epoch': 0.75}
2025-05-19 19:58:56 {'loss': 1.5552, 'grad_norm': 12.598148345947266, 'learning_rate': 1.1375e-05, 'epoch': 0.775}
2025-05-19 19:59:06 {'loss': 1.3609, 'grad_norm': 12.959821701049805, 'learning_rate': 1.0125e-05, 'epoch': 0.8}
2025-05-19 19:59:20 {'loss': 1.293, 'grad_norm': 13.00887393951416, 'learning_rate': 8.875e-06, 'epoch': 0.825}
2025-05-19 19:59:30 {'loss': 1.3681, 'grad_norm': 14.860785484313965, 'learning_rate': 7.625e-06, 'epoch': 0.85}
2025-05-19 19:59:40 {'loss': 1.2628, 'grad_norm': 12.191110610961914, 'learning_rate': 6.375000000000001e-06, 'epoch': 0.875}
2025-05-19 19:59:50 {'loss': 1.3197, 'grad_norm': 17.6649227142334, 'learning_rate': 5.125e-06, 'epoch': 0.9}
2025-05-19 20:00:03 {'loss': 1.4387, 'grad_norm': 13.553362846374512, 'learning_rate': 3.875e-06, 'epoch': 0.925}
2025-05-19 20:00:13 {'loss': 1.4165, 'grad_norm': 8.898282051086426, 'learning_rate': 2.625e-06, 'epoch': 0.95}
2025-05-19 20:00:23 {'loss': 1.4283, 'grad_norm': 13.59922981262207, 'learning_rate': 1.3750000000000002e-06, 'epoch': 0.975}
2025-05-19 20:00:33 {'loss': 1.3802, 'grad_norm': 11.823150634765625, 'learning_rate': 1.2500000000000002e-07, 'epoch': 1.0}
2025-05-19 20:00:33 {'train_runtime': 455.0892, 'train_samples_per_second': 1.758, 'train_steps_per_second': 0.879, 'train_loss': 1.4719064927101135, 'epoch': 1.0}
2025-05-19 20:01:27 {'eval_loss': 1.3516159057617188, 'eval_runtime': 11.7794, 'eval_samples_per_second': 16.979, 'eval_steps_per_second': 2.122, 'epoch': 1.0}
2025-05-19 20:01:55 {'loss': 0.889, 'grad_norm': 11.474447250366211, 'learning_rate': 4.8875e-05, 'epoch': 0.025}
2025-05-19 20:02:05 {'loss': 1.0528, 'grad_norm': 17.615478515625, 'learning_rate': 4.7625000000000006e-05, 'epoch': 0.05}
2025-05-19 20:02:15 {'loss': 0.9224, 'grad_norm': 11.755194664001465, 'learning_rate': 4.6375e-05, 'epoch': 0.075}
2025-05-19 20:02:29 {'loss': 1.0564, 'grad_norm': 10.245782852172852, 'learning_rate': 4.5125e-05, 'epoch': 0.1}
2025-05-19 20:02:39 {'loss': 1.0761, 'grad_norm': 11.406681060791016, 'learning_rate': 4.3875e-05, 'epoch': 0.125}
2025-05-19 20:02:49 {'loss': 1.1875, 'grad_norm': 15.759106636047363, 'learning_rate': 4.2625000000000006e-05, 'epoch': 0.15}
2025-05-19 20:02:58 {'loss': 1.0506, 'grad_norm': 13.805329322814941, 'learning_rate': 4.1375e-05, 'epoch': 0.175}
2025-05-19 20:03:12 {'loss': 1.0979, 'grad_norm': 11.872097969055176, 'learning_rate': 4.0125e-05, 'epoch': 0.2}
2025-05-19 20:03:22 {'loss': 1.0663, 'grad_norm': 10.724918365478516, 'learning_rate': 3.8875e-05, 'epoch': 0.225}
2025-05-19 20:03:32 {'loss': 1.0045, 'grad_norm': 7.902745723724365, 'learning_rate': 3.7625e-05, 'epoch': 0.25}
2025-05-19 20:03:42 {'loss': 0.9859, 'grad_norm': 20.100479125976562, 'learning_rate': 3.6375e-05, 'epoch': 0.275}
2025-05-19 20:03:55 {'loss': 1.158, 'grad_norm': 13.429862976074219, 'learning_rate': 3.5125e-05, 'epoch': 0.3}
2025-05-19 20:04:05 {'loss': 1.0316, 'grad_norm': 9.925966262817383, 'learning_rate': 3.3875000000000003e-05, 'epoch': 0.325}
2025-05-19 20:04:15 {'loss': 1.2458, 'grad_norm': 12.445505142211914, 'learning_rate': 3.2625e-05, 'epoch': 0.35}
2025-05-19 20:04:25 {'loss': 1.0059, 'grad_norm': 8.499706268310547, 'learning_rate': 3.1375e-05, 'epoch': 0.375}
2025-05-19 20:04:39 {'loss': 1.0024, 'grad_norm': 13.033760070800781, 'learning_rate': 3.0125000000000004e-05, 'epoch': 0.4}
2025-05-19 20:04:49 {'loss': 1.1712, 'grad_norm': 10.523344039916992, 'learning_rate': 2.8875e-05, 'epoch': 0.425}
2025-05-19 20:04:58 {'loss': 1.1207, 'grad_norm': 8.748915672302246, 'learning_rate': 2.7625e-05, 'epoch': 0.45}
2025-05-19 20:05:08 {'loss': 1.2173, 'grad_norm': 16.68496322631836, 'learning_rate': 2.6375e-05, 'epoch': 0.475}
2025-05-19 20:05:22 {'loss': 1.0867, 'grad_norm': 13.52774429321289, 'learning_rate': 2.5124999999999997e-05, 'epoch': 0.5}
2025-05-19 20:05:32 {'loss': 1.1004, 'grad_norm': 11.776573181152344, 'learning_rate': 2.3875e-05, 'epoch': 0.525}
2025-05-19 20:05:41 {'loss': 1.0547, 'grad_norm': 8.750810623168945, 'learning_rate': 2.2625e-05, 'epoch': 0.55}
2025-05-19 20:05:51 {'loss': 1.0397, 'grad_norm': 12.49250602722168, 'learning_rate': 2.1375e-05, 'epoch': 0.575}
2025-05-19 20:06:05 {'loss': 1.1305, 'grad_norm': 9.895142555236816, 'learning_rate': 2.0125e-05, 'epoch': 0.6}
2025-05-19 20:06:15 {'loss': 1.1205, 'grad_norm': 12.354031562805176, 'learning_rate': 1.8875e-05, 'epoch': 0.625}
2025-05-19 20:06:25 {'loss': 1.2402, 'grad_norm': 9.963717460632324, 'learning_rate': 1.7625e-05, 'epoch': 0.65}
2025-05-19 20:06:38 {'loss': 1.0816, 'grad_norm': 11.23161506652832, 'learning_rate': 1.6375e-05, 'epoch': 0.675}
2025-05-19 20:06:48 {'loss': 1.0966, 'grad_norm': 12.594035148620605, 'learning_rate': 1.5125e-05, 'epoch': 0.7}
2025-05-19 20:06:58 {'loss': 1.2173, 'grad_norm': 9.256046295166016, 'learning_rate': 1.3875000000000002e-05, 'epoch': 0.725}
2025-05-19 20:07:08 {'loss': 1.1866, 'grad_norm': 11.075396537780762, 'learning_rate': 1.2625e-05, 'epoch': 0.75}
2025-05-19 20:07:18 {'loss': 1.3504, 'grad_norm': 14.2737455368042, 'learning_rate': 1.1375e-05, 'epoch': 0.775}
2025-05-19 20:07:32 {'loss': 1.1932, 'grad_norm': 11.889867782592773, 'learning_rate': 1.0125e-05, 'epoch': 0.8}
2025-05-19 20:07:42 {'loss': 1.1522, 'grad_norm': 12.33627700805664, 'learning_rate': 8.875e-06, 'epoch': 0.825}
2025-05-19 20:07:51 {'loss': 1.2332, 'grad_norm': 14.203442573547363, 'learning_rate': 7.625e-06, 'epoch': 0.85}
2025-05-19 20:08:01 {'loss': 1.1436, 'grad_norm': 11.181139945983887, 'learning_rate': 6.375000000000001e-06, 'epoch': 0.875}
2025-05-19 20:08:15 {'loss': 1.1847, 'grad_norm': 14.25092887878418, 'learning_rate': 5.125e-06, 'epoch': 0.9}
2025-05-19 20:08:25 {'loss': 1.3163, 'grad_norm': 12.033960342407227, 'learning_rate': 3.875e-06, 'epoch': 0.925}
2025-05-19 20:08:35 {'loss': 1.2432, 'grad_norm': 8.480753898620605, 'learning_rate': 2.625e-06, 'epoch': 0.95}
2025-05-19 20:08:45 {'loss': 1.145, 'grad_norm': 13.094032287597656, 'learning_rate': 1.3750000000000002e-06, 'epoch': 0.975}
2025-05-19 20:08:58 {'loss': 0.836, 'grad_norm': 10.012494087219238, 'learning_rate': 1.2500000000000002e-07, 'epoch': 1.0}
2025-05-19 20:08:58 {'train_runtime': 439.6451, 'train_samples_per_second': 1.82, 'train_steps_per_second': 0.91, 'train_loss': 1.1123775053024292, 'epoch': 1.0}
2025-05-19 20:09:47 {'eval_loss': 1.3698089122772217, 'eval_runtime': 11.3321, 'eval_samples_per_second': 17.649, 'eval_steps_per_second': 2.206, 'epoch': 1.0}
2025-05-19 20:10:13 {'loss': 0.5546, 'grad_norm': 11.02843952178955, 'learning_rate': 4.8875e-05, 'epoch': 0.025}
2025-05-19 20:10:23 {'loss': 0.6736, 'grad_norm': 13.009527206420898, 'learning_rate': 4.7625000000000006e-05, 'epoch': 0.05}
2025-05-19 20:10:37 {'loss': 0.6435, 'grad_norm': 10.909767150878906, 'learning_rate': 4.6375e-05, 'epoch': 0.075}
2025-05-19 20:10:47 {'loss': 0.7321, 'grad_norm': 10.189830780029297, 'learning_rate': 4.5125e-05, 'epoch': 0.1}
2025-05-19 20:10:57 {'loss': 0.7604, 'grad_norm': 13.245781898498535, 'learning_rate': 4.3875e-05, 'epoch': 0.125}
2025-05-19 20:11:07 {'loss': 0.8635, 'grad_norm': 12.217531204223633, 'learning_rate': 4.2625000000000006e-05, 'epoch': 0.15}
2025-05-19 20:11:21 {'loss': 0.7656, 'grad_norm': 12.122151374816895, 'learning_rate': 4.1375e-05, 'epoch': 0.175}
2025-05-19 20:11:31 {'loss': 0.8461, 'grad_norm': 10.231795310974121, 'learning_rate': 4.0125e-05, 'epoch': 0.2}
2025-05-19 20:11:40 {'loss': 0.8376, 'grad_norm': 10.500654220581055, 'learning_rate': 3.8875e-05, 'epoch': 0.225}
2025-05-19 20:11:50 {'loss': 0.7377, 'grad_norm': 6.31170654296875, 'learning_rate': 3.7625e-05, 'epoch': 0.25}
2025-05-19 20:12:04 {'loss': 0.7446, 'grad_norm': 9.77391242980957, 'learning_rate': 3.6375e-05, 'epoch': 0.275}
2025-05-19 20:12:14 {'loss': 0.8659, 'grad_norm': 13.466057777404785, 'learning_rate': 3.5125e-05, 'epoch': 0.3}
2025-05-19 20:12:24 {'loss': 0.7873, 'grad_norm': 13.312376976013184, 'learning_rate': 3.3875000000000003e-05, 'epoch': 0.325}
2025-05-19 20:12:34 {'loss': 0.9732, 'grad_norm': 11.330785751342773, 'learning_rate': 3.2625e-05, 'epoch': 0.35}
2025-05-19 20:12:48 {'loss': 0.7984, 'grad_norm': 7.4205641746521, 'learning_rate': 3.1375e-05, 'epoch': 0.375}
2025-05-19 20:12:58 {'loss': 0.7889, 'grad_norm': 11.371676445007324, 'learning_rate': 3.0125000000000004e-05, 'epoch': 0.4}
2025-05-19 20:13:08 {'loss': 0.9626, 'grad_norm': 9.928948402404785, 'learning_rate': 2.8875e-05, 'epoch': 0.425}
2025-05-19 20:13:18 {'loss': 0.9106, 'grad_norm': 7.714757442474365, 'learning_rate': 2.7625e-05, 'epoch': 0.45}
2025-05-19 20:13:31 {'loss': 1.0013, 'grad_norm': 15.685127258300781, 'learning_rate': 2.6375e-05, 'epoch': 0.475}
2025-05-19 20:13:41 {'loss': 0.882, 'grad_norm': 12.763917922973633, 'learning_rate': 2.5124999999999997e-05, 'epoch': 0.5}
2025-05-19 20:13:51 {'loss': 0.8976, 'grad_norm': 11.483081817626953, 'learning_rate': 2.3875e-05, 'epoch': 0.525}
2025-05-19 20:14:01 {'loss': 0.8722, 'grad_norm': 9.243993759155273, 'learning_rate': 2.2625e-05, 'epoch': 0.55}
2025-05-19 20:14:15 {'loss': 0.875, 'grad_norm': 8.172551155090332, 'learning_rate': 2.1375e-05, 'epoch': 0.575}
2025-05-19 20:14:25 {'loss': 0.9528, 'grad_norm': 9.64354133605957, 'learning_rate': 2.0125e-05, 'epoch': 0.6}
2025-05-19 20:14:35 {'loss': 0.9533, 'grad_norm': 15.424835205078125, 'learning_rate': 1.8875e-05, 'epoch': 0.625}
2025-05-19 20:14:45 {'loss': 1.0637, 'grad_norm': 9.882534980773926, 'learning_rate': 1.7625e-05, 'epoch': 0.65}
2025-05-19 20:14:58 {'loss': 0.9382, 'grad_norm': 9.864739418029785, 'learning_rate': 1.6375e-05, 'epoch': 0.675}
2025-05-19 20:15:08 {'loss': 0.943, 'grad_norm': 11.402226448059082, 'learning_rate': 1.5125e-05, 'epoch': 0.7}
2025-05-19 20:15:18 {'loss': 1.0821, 'grad_norm': 11.410287857055664, 'learning_rate': 1.3875000000000002e-05, 'epoch': 0.725}
2025-05-19 20:15:29 {'loss': 1.0757, 'grad_norm': 12.148334503173828, 'learning_rate': 1.2625e-05, 'epoch': 0.75}
2025-05-19 20:15:42 {'loss': 1.2489, 'grad_norm': 12.371475219726562, 'learning_rate': 1.1375e-05, 'epoch': 0.775}
2025-05-19 20:15:52 {'loss': 1.0767, 'grad_norm': 11.361482620239258, 'learning_rate': 1.0125e-05, 'epoch': 0.8}
2025-05-19 20:16:02 {'loss': 1.0553, 'grad_norm': 11.43899154663086, 'learning_rate': 8.875e-06, 'epoch': 0.825}
2025-05-19 20:16:15 {'loss': 1.1613, 'grad_norm': 14.552059173583984, 'learning_rate': 7.625e-06, 'epoch': 0.85}
2025-05-19 20:16:25 {'loss': 1.0971, 'grad_norm': 11.597261428833008, 'learning_rate': 6.375000000000001e-06, 'epoch': 0.875}
2025-05-19 20:16:35 {'loss': 1.1309, 'grad_norm': 15.12843132019043, 'learning_rate': 5.125e-06, 'epoch': 0.9}
2025-05-19 20:16:44 {'loss': 1.2661, 'grad_norm': 12.56824779510498, 'learning_rate': 3.875e-06, 'epoch': 0.925}
2025-05-19 20:16:58 {'loss': 1.1801, 'grad_norm': 8.919990539550781, 'learning_rate': 2.625e-06, 'epoch': 0.95}
2025-05-19 20:17:09 {'loss': 1.0226, 'grad_norm': 10.887499809265137, 'learning_rate': 1.3750000000000002e-06, 'epoch': 0.975}
2025-05-19 20:17:19 {'loss': 0.6615, 'grad_norm': 12.714032173156738, 'learning_rate': 1.2500000000000002e-07, 'epoch': 1.0}
2025-05-19 20:17:19 {'train_runtime': 441.2143, 'train_samples_per_second': 1.813, 'train_steps_per_second': 0.907, 'train_loss': 0.9170981669425964, 'epoch': 1.0}
2025-05-19 20:18:04 {'eval_loss': 1.4117273092269897, 'eval_runtime': 11.5415, 'eval_samples_per_second': 17.329, 'eval_steps_per_second': 2.166, 'epoch': 1.0}
2025-05-19 20:18:29 {'loss': 0.3507, 'grad_norm': 7.961800575256348, 'learning_rate': 4.8875e-05, 'epoch': 0.025}
2025-05-19 20:18:39 {'loss': 0.4386, 'grad_norm': 14.627103805541992, 'learning_rate': 4.7625000000000006e-05, 'epoch': 0.05}
2025-05-19 20:18:49 {'loss': 0.464, 'grad_norm': 9.979926109313965, 'learning_rate': 4.6375e-05, 'epoch': 0.075}
2025-05-19 20:19:03 {'loss': 0.5033, 'grad_norm': 8.008667945861816, 'learning_rate': 4.5125e-05, 'epoch': 0.1}
2025-05-19 20:19:13 {'loss': 0.5353, 'grad_norm': 11.534815788269043, 'learning_rate': 4.3875e-05, 'epoch': 0.125}
2025-05-19 20:19:23 {'loss': 0.6212, 'grad_norm': 11.883872985839844, 'learning_rate': 4.2625000000000006e-05, 'epoch': 0.15}
2025-05-19 20:19:33 {'loss': 0.5319, 'grad_norm': 13.870086669921875, 'learning_rate': 4.1375e-05, 'epoch': 0.175}
2025-05-19 20:19:46 {'loss': 0.6253, 'grad_norm': 9.337309837341309, 'learning_rate': 4.0125e-05, 'epoch': 0.2}
2025-05-19 20:19:56 {'loss': 0.6269, 'grad_norm': 9.318618774414062, 'learning_rate': 3.8875e-05, 'epoch': 0.225}
2025-05-19 20:20:06 {'loss': 0.557, 'grad_norm': 6.250629901885986, 'learning_rate': 3.7625e-05, 'epoch': 0.25}
2025-05-19 20:20:16 {'loss': 0.5805, 'grad_norm': 9.781439781188965, 'learning_rate': 3.6375e-05, 'epoch': 0.275}
2025-05-19 20:20:29 {'loss': 0.6994, 'grad_norm': 11.798791885375977, 'learning_rate': 3.5125e-05, 'epoch': 0.3}
2025-05-19 20:20:39 {'loss': 0.5905, 'grad_norm': 9.781699180603027, 'learning_rate': 3.3875000000000003e-05, 'epoch': 0.325}
2025-05-19 20:20:49 {'loss': 0.745, 'grad_norm': 11.379271507263184, 'learning_rate': 3.2625e-05, 'epoch': 0.35}
2025-05-19 20:20:59 {'loss': 0.603, 'grad_norm': 8.12216854095459, 'learning_rate': 3.1375e-05, 'epoch': 0.375}
2025-05-19 20:21:13 {'loss': 0.6178, 'grad_norm': 10.302294731140137, 'learning_rate': 3.0125000000000004e-05, 'epoch': 0.4}
2025-05-19 20:21:22 {'loss': 0.7881, 'grad_norm': 10.484752655029297, 'learning_rate': 2.8875e-05, 'epoch': 0.425}
2025-05-19 20:21:33 {'loss': 0.746, 'grad_norm': 7.9548821449279785, 'learning_rate': 2.7625e-05, 'epoch': 0.45}
2025-05-19 20:21:42 {'loss': 0.8033, 'grad_norm': 14.324920654296875, 'learning_rate': 2.6375e-05, 'epoch': 0.475}
2025-05-19 20:21:56 {'loss': 0.7063, 'grad_norm': 10.747191429138184, 'learning_rate': 2.5124999999999997e-05, 'epoch': 0.5}
2025-05-19 20:22:06 {'loss': 0.7375, 'grad_norm': 11.80844497680664, 'learning_rate': 2.3875e-05, 'epoch': 0.525}
2025-05-19 20:22:16 {'loss': 0.7133, 'grad_norm': 9.2686185836792, 'learning_rate': 2.2625e-05, 'epoch': 0.55}
2025-05-19 20:22:25 {'loss': 0.7053, 'grad_norm': 8.263036727905273, 'learning_rate': 2.1375e-05, 'epoch': 0.575}
2025-05-19 20:22:35 {'loss': 0.796, 'grad_norm': 8.32235336303711, 'learning_rate': 2.0125e-05, 'epoch': 0.6}
2025-05-19 20:22:49 {'loss': 0.8086, 'grad_norm': 13.98997688293457, 'learning_rate': 1.8875e-05, 'epoch': 0.625}
2025-05-19 20:22:59 {'loss': 0.9264, 'grad_norm': 10.203826904296875, 'learning_rate': 1.7625e-05, 'epoch': 0.65}
2025-05-19 20:23:09 {'loss': 0.8045, 'grad_norm': 10.619465827941895, 'learning_rate': 1.6375e-05, 'epoch': 0.675}
2025-05-19 20:23:19 {'loss': 0.8382, 'grad_norm': 12.874154090881348, 'learning_rate': 1.5125e-05, 'epoch': 0.7}
2025-05-19 20:23:33 {'loss': 0.9757, 'grad_norm': 9.890647888183594, 'learning_rate': 1.3875000000000002e-05, 'epoch': 0.725}
2025-05-19 20:23:43 {'loss': 0.9741, 'grad_norm': 11.650824546813965, 'learning_rate': 1.2625e-05, 'epoch': 0.75}
2025-05-19 20:23:53 {'loss': 1.1366, 'grad_norm': 12.572630882263184, 'learning_rate': 1.1375e-05, 'epoch': 0.775}
2025-05-19 20:24:03 {'loss': 0.9994, 'grad_norm': 11.165063858032227, 'learning_rate': 1.0125e-05, 'epoch': 0.8}
2025-05-19 20:24:16 {'loss': 1.0091, 'grad_norm': 11.01586627960205, 'learning_rate': 8.875e-06, 'epoch': 0.825}
2025-05-19 20:24:26 {'loss': 1.1096, 'grad_norm': 14.08565616607666, 'learning_rate': 7.625e-06, 'epoch': 0.85}
2025-05-19 20:24:36 {'loss': 1.0216, 'grad_norm': 13.160332679748535, 'learning_rate': 6.375000000000001e-06, 'epoch': 0.875}
2025-05-19 20:24:50 {'loss': 1.1005, 'grad_norm': 17.489940643310547, 'learning_rate': 5.125e-06, 'epoch': 0.9}
2025-05-19 20:25:00 {'loss': 1.2252, 'grad_norm': 13.50774097442627, 'learning_rate': 3.875e-06, 'epoch': 0.925}
2025-05-19 20:25:10 {'loss': 1.1379, 'grad_norm': 9.212684631347656, 'learning_rate': 2.625e-06, 'epoch': 0.95}
2025-05-19 20:25:20 {'loss': 0.9315, 'grad_norm': 10.441457748413086, 'learning_rate': 1.3750000000000002e-06, 'epoch': 0.975}
2025-05-19 20:25:30 {'loss': 0.5132, 'grad_norm': 7.5773024559021, 'learning_rate': 1.2500000000000002e-07, 'epoch': 1.0}
2025-05-19 20:25:30 {'train_runtime': 436.5705, 'train_samples_per_second': 1.832, 'train_steps_per_second': 0.916, 'train_loss': 0.7649610769748688, 'epoch': 1.0}
2025-05-19 20:26:15 {'eval_loss': 1.4510772228240967, 'eval_runtime': 9.1993, 'eval_samples_per_second': 21.741, 'eval_steps_per_second': 2.718, 'epoch': 1.0}
2025-05-19 20:26:44 {'loss': 0.2064, 'grad_norm': 7.625996112823486, 'learning_rate': 4.8875e-05, 'epoch': 0.025}
2025-05-19 20:26:54 {'loss': 0.2771, 'grad_norm': 12.264768600463867, 'learning_rate': 4.7625000000000006e-05, 'epoch': 0.05}
2025-05-19 20:27:04 {'loss': 0.3336, 'grad_norm': 8.960967063903809, 'learning_rate': 4.6375e-05, 'epoch': 0.075}
2025-05-19 20:27:14 {'loss': 0.3468, 'grad_norm': 8.636549949645996, 'learning_rate': 4.5125e-05, 'epoch': 0.1}
2025-05-19 20:27:28 {'loss': 0.3878, 'grad_norm': 10.00275707244873, 'learning_rate': 4.3875e-05, 'epoch': 0.125}
2025-05-19 20:27:38 {'loss': 0.4537, 'grad_norm': 10.824951171875, 'learning_rate': 4.2625000000000006e-05, 'epoch': 0.15}
2025-05-19 20:27:48 {'loss': 0.3951, 'grad_norm': 10.804141998291016, 'learning_rate': 4.1375e-05, 'epoch': 0.175}
2025-05-19 20:27:58 {'loss': 0.4807, 'grad_norm': 7.563554286956787, 'learning_rate': 4.0125e-05, 'epoch': 0.2}
2025-05-19 20:28:11 {'loss': 0.4499, 'grad_norm': 9.246800422668457, 'learning_rate': 3.8875e-05, 'epoch': 0.225}
2025-05-19 20:28:21 {'loss': 0.4119, 'grad_norm': 5.9660258293151855, 'learning_rate': 3.7625e-05, 'epoch': 0.25}
2025-05-19 20:28:31 {'loss': 0.4371, 'grad_norm': 8.687769889831543, 'learning_rate': 3.6375e-05, 'epoch': 0.275}
2025-05-19 20:28:41 {'loss': 0.5097, 'grad_norm': 9.607014656066895, 'learning_rate': 3.5125e-05, 'epoch': 0.3}
2025-05-19 20:28:54 {'loss': 0.4449, 'grad_norm': 9.033117294311523, 'learning_rate': 3.3875000000000003e-05, 'epoch': 0.325}
2025-05-19 20:29:04 {'loss': 0.607, 'grad_norm': 12.879858016967773, 'learning_rate': 3.2625e-05, 'epoch': 0.35}
2025-05-19 20:29:14 {'loss': 0.4851, 'grad_norm': 7.195497989654541, 'learning_rate': 3.1375e-05, 'epoch': 0.375}
2025-05-19 20:29:28 {'loss': 0.5097, 'grad_norm': 9.465287208557129, 'learning_rate': 3.0125000000000004e-05, 'epoch': 0.4}
2025-05-19 20:29:37 {'loss': 0.6247, 'grad_norm': 10.95999526977539, 'learning_rate': 2.8875e-05, 'epoch': 0.425}
2025-05-19 20:29:47 {'loss': 0.6071, 'grad_norm': 9.626545906066895, 'learning_rate': 2.7625e-05, 'epoch': 0.45}
2025-05-19 20:29:57 {'loss': 0.6658, 'grad_norm': 19.069875717163086, 'learning_rate': 2.6375e-05, 'epoch': 0.475}
2025-05-19 20:30:11 {'loss': 0.5801, 'grad_norm': 11.397224426269531, 'learning_rate': 2.5124999999999997e-05, 'epoch': 0.5}
2025-05-19 20:30:21 {'loss': 0.6017, 'grad_norm': 11.192805290222168, 'learning_rate': 2.3875e-05, 'epoch': 0.525}
2025-05-19 20:30:31 {'loss': 0.5898, 'grad_norm': 8.583730697631836, 'learning_rate': 2.2625e-05, 'epoch': 0.55}
2025-05-19 20:30:41 {'loss': 0.5862, 'grad_norm': 8.670289993286133, 'learning_rate': 2.1375e-05, 'epoch': 0.575}
2025-05-19 20:30:55 {'loss': 0.6762, 'grad_norm': 10.272116661071777, 'learning_rate': 2.0125e-05, 'epoch': 0.6}
2025-05-19 20:31:05 {'loss': 0.6982, 'grad_norm': 12.860069274902344, 'learning_rate': 1.8875e-05, 'epoch': 0.625}
2025-05-19 20:31:15 {'loss': 0.8208, 'grad_norm': 10.884089469909668, 'learning_rate': 1.7625e-05, 'epoch': 0.65}
2025-05-19 20:31:24 {'loss': 0.6976, 'grad_norm': 8.901874542236328, 'learning_rate': 1.6375e-05, 'epoch': 0.675}
2025-05-19 20:31:34 {'loss': 0.724, 'grad_norm': 12.351400375366211, 'learning_rate': 1.5125e-05, 'epoch': 0.7}
2025-05-19 20:31:48 {'loss': 0.8725, 'grad_norm': 10.029245376586914, 'learning_rate': 1.3875000000000002e-05, 'epoch': 0.725}
2025-05-19 20:31:58 {'loss': 0.8704, 'grad_norm': 11.482064247131348, 'learning_rate': 1.2625e-05, 'epoch': 0.75}
2025-05-19 20:32:08 {'loss': 1.0411, 'grad_norm': 14.843254089355469, 'learning_rate': 1.1375e-05, 'epoch': 0.775}
2025-05-19 20:32:21 {'loss': 0.9184, 'grad_norm': 10.634461402893066, 'learning_rate': 1.0125e-05, 'epoch': 0.8}
2025-05-19 20:32:31 {'loss': 0.9435, 'grad_norm': 11.141656875610352, 'learning_rate': 8.875e-06, 'epoch': 0.825}
2025-05-19 20:32:41 {'loss': 1.0547, 'grad_norm': 14.56884479522705, 'learning_rate': 7.625e-06, 'epoch': 0.85}
2025-05-19 20:32:51 {'loss': 0.9875, 'grad_norm': 11.894054412841797, 'learning_rate': 6.375000000000001e-06, 'epoch': 0.875}
2025-05-19 20:33:05 {'loss': 1.038, 'grad_norm': 14.791433334350586, 'learning_rate': 5.125e-06, 'epoch': 0.9}
2025-05-19 20:33:15 {'loss': 1.1936, 'grad_norm': 13.18530559539795, 'learning_rate': 3.875e-06, 'epoch': 0.925}
2025-05-19 20:33:25 {'loss': 1.1133, 'grad_norm': 8.994257926940918, 'learning_rate': 2.625e-06, 'epoch': 0.95}
2025-05-19 20:33:34 {'loss': 0.8543, 'grad_norm': 11.730727195739746, 'learning_rate': 1.3750000000000002e-06, 'epoch': 0.975}
2025-05-19 20:33:48 {'loss': 0.4093, 'grad_norm': 6.11272668838501, 'learning_rate': 1.2500000000000002e-07, 'epoch': 1.0}
2025-05-19 20:33:48 {'train_runtime': 439.3073, 'train_samples_per_second': 1.821, 'train_steps_per_second': 0.911, 'train_loss': 0.6476362150907516, 'epoch': 1.0}
2025-05-19 20:34:32 {'eval_loss': 1.4900485277175903, 'eval_runtime': 11.3926, 'eval_samples_per_second': 17.555, 'eval_steps_per_second': 2.194, 'epoch': 1.0}
2025-05-19 20:00:44 INFO :      Sent reply
2025-05-19 20:01:14 INFO :      
2025-05-19 20:01:14 INFO :      Received: evaluate message bba16a3c-9871-4cd9-8574-ffc39c0d0109
2025-05-19 20:01:27 INFO :      Sent reply
2025-05-19 20:01:35 INFO :      
2025-05-19 20:01:35 INFO :      Received: train message 25fc6709-cac2-48f3-9ea7-a57a0d33df41
2025-05-19 20:09:04 INFO :      Sent reply
2025-05-19 20:09:34 INFO :      
2025-05-19 20:09:34 INFO :      Received: evaluate message 58a7d0c8-76f3-42e3-8a1d-027bf60f0c13
2025-05-19 20:09:47 INFO :      Sent reply
2025-05-19 20:09:53 INFO :      
2025-05-19 20:09:53 INFO :      Received: train message 937c25c0-eeb2-4505-ab94-c1f052fad424
2025-05-19 20:17:24 INFO :      Sent reply
2025-05-19 20:17:52 INFO :      
2025-05-19 20:17:52 INFO :      Received: evaluate message 3222661f-65b5-4b9e-99da-039ddd7a3e8d
2025-05-19 20:18:04 INFO :      Sent reply
2025-05-19 20:18:11 INFO :      
2025-05-19 20:18:11 INFO :      Received: train message bb7278ef-3010-410b-8617-9b9f227f9cb0
2025-05-19 20:25:36 INFO :      Sent reply
2025-05-19 20:26:03 INFO :      
2025-05-19 20:26:03 INFO :      Received: evaluate message d0bed0a6-cb66-4f61-8a1e-c630b7c79964
2025-05-19 20:26:15 INFO :      Sent reply
2025-05-19 20:26:25 INFO :      
2025-05-19 20:26:25 INFO :      Received: train message e03b5b7c-e6f8-4654-bf0a-6678dbd03aad
2025-05-19 20:33:52 INFO :      Sent reply
2025-05-19 20:34:20 INFO :      
2025-05-19 20:34:20 INFO :      Received: evaluate message 1aa1a843-5246-4b77-b491-af297d667f4a
2025-05-19 20:34:32 INFO :      Sent reply
2025-05-19 20:34:33 INFO :      
2025-05-19 20:34:33 INFO :      Received: reconnect message 0a8185ad-a973-4fbc-a3b1-b1eda76f04df
2025-05-19 20:34:33 INFO :      Disconnect and shut down
