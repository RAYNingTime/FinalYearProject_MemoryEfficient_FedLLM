2025-05-21 17:16:08 
Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]
Generating train split:  94%|█████████▍| 113000/120000 [00:00<00:00, 1117036.50 examples/s]
Generating train split: 100%|██████████| 120000/120000 [00:00<00:00, 1138153.42 examples/s]
2025-05-21 17:16:08 
Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]
Generating test split: 100%|██████████| 7600/7600 [00:00<00:00, 1023362.24 examples/s]
2025-05-21 17:16:10 
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 1050.93 examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 1045.31 examples/s]
2025-05-21 17:16:11 
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]
Map:  22%|██▏       | 217/1000 [00:00<00:00, 2121.52 examples/s]
Map:  48%|████▊     | 477/1000 [00:00<00:00, 2388.71 examples/s]
Map:  72%|███████▏  | 722/1000 [00:00<00:00, 2412.97 examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 2354.29 examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 2328.74 examples/s]
2025-05-21 17:16:11 /app/client.py:56: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
2025-05-21 17:16:11   trainer = Trainer(
2025-05-21 17:16:11 WARNING :   DEPRECATED FEATURE: flwr.client.start_numpy_client() is deprecated. 
2025-05-21 17:16:11 Instead, use `flwr.client.start_client()` by ensuring you first call the `.to_client()` method as shown below: 
2025-05-21 17:16:11 flwr.client.start_client(
2025-05-21 17:16:11 server_address='<IP>:<PORT>',
2025-05-21 17:16:11 client=FlowerClient().to_client(), # <-- where FlowerClient is of type flwr.client.NumPyClient object
2025-05-21 17:16:11 )
2025-05-21 17:16:11 Using `start_numpy_client()` is deprecated.
2025-05-21 17:16:11 
2025-05-21 17:16:11             This is a deprecated feature. It will be removed
2025-05-21 17:16:11             entirely in future versions of Flower.
2025-05-21 17:16:11         
2025-05-21 17:16:11 WARNING :   DEPRECATED FEATURE: flwr.client.start_client() is deprecated.
2025-05-21 17:16:11 Instead, use the `flower-supernode` CLI command to start a SuperNode as shown below:
2025-05-21 17:16:11 
2025-05-21 17:16:11 $ flower-supernode --insecure --superlink='<IP>:<PORT>'
2025-05-21 17:16:11 
2025-05-21 17:16:11 To view all available options, run:
2025-05-21 17:16:11 
2025-05-21 17:16:11 $ flower-supernode --help
2025-05-21 17:16:11 
2025-05-21 17:16:11 Using `start_client()` is deprecated.
2025-05-21 17:16:11 
2025-05-21 17:16:11             This is a deprecated feature. It will be removed
2025-05-21 17:16:11             entirely in future versions of Flower.
2025-05-21 17:16:11         
2025-05-21 17:16:11 INFO :      
2025-05-21 17:16:11 INFO :      Received: get_parameters message be285aea-adab-4da6-b5b3-20208b06c940
2025-05-21 17:16:15 INFO :      Sent reply
2025-05-21 17:16:47 INFO :      
2025-05-21 17:16:47 INFO :      Received: train message 475744d9-2535-4c6c-9267-6c3481d5fe2e
2025-05-21 17:17:09 {'loss': 4.208, 'grad_norm': 10.104744911193848, 'learning_rate': 4.8875e-05, 'epoch': 0.025}
2025-05-21 17:17:30 {'loss': 2.6599, 'grad_norm': 12.822298049926758, 'learning_rate': 4.7625000000000006e-05, 'epoch': 0.05}
2025-05-21 17:17:45 {'loss': 2.3049, 'grad_norm': 14.194560050964355, 'learning_rate': 4.6375e-05, 'epoch': 0.075}
2025-05-21 17:18:00 {'loss': 2.4306, 'grad_norm': 9.967127799987793, 'learning_rate': 4.5125e-05, 'epoch': 0.1}
2025-05-21 17:18:15 {'loss': 1.9986, 'grad_norm': 14.009085655212402, 'learning_rate': 4.3875e-05, 'epoch': 0.125}
2025-05-21 17:18:30 {'loss': 2.1657, 'grad_norm': 13.355198860168457, 'learning_rate': 4.2625000000000006e-05, 'epoch': 0.15}
2025-05-21 17:18:45 {'loss': 1.8071, 'grad_norm': 12.851582527160645, 'learning_rate': 4.1375e-05, 'epoch': 0.175}
2025-05-21 17:18:59 {'loss': 1.8988, 'grad_norm': 11.356367111206055, 'learning_rate': 4.0125e-05, 'epoch': 0.2}
2025-05-21 17:19:14 {'loss': 2.1155, 'grad_norm': 10.730032920837402, 'learning_rate': 3.8875e-05, 'epoch': 0.225}
2025-05-21 17:19:35 {'loss': 1.9519, 'grad_norm': 10.320252418518066, 'learning_rate': 3.7625e-05, 'epoch': 0.25}
2025-05-21 17:19:50 {'loss': 1.9538, 'grad_norm': 10.981802940368652, 'learning_rate': 3.6375e-05, 'epoch': 0.275}
2025-05-21 17:20:05 {'loss': 2.184, 'grad_norm': 12.203413009643555, 'learning_rate': 3.5125e-05, 'epoch': 0.3}
2025-05-21 17:20:20 {'loss': 1.8272, 'grad_norm': 8.3193359375, 'learning_rate': 3.3875000000000003e-05, 'epoch': 0.325}
2025-05-21 17:20:35 {'loss': 2.0594, 'grad_norm': 14.879190444946289, 'learning_rate': 3.2625e-05, 'epoch': 0.35}
2025-05-21 17:20:49 {'loss': 1.9338, 'grad_norm': 15.397518157958984, 'learning_rate': 3.1375e-05, 'epoch': 0.375}
2025-05-21 17:21:04 {'loss': 1.9048, 'grad_norm': 12.24506664276123, 'learning_rate': 3.0125000000000004e-05, 'epoch': 0.4}
2025-05-21 17:21:19 {'loss': 1.7798, 'grad_norm': 13.736392974853516, 'learning_rate': 2.8875e-05, 'epoch': 0.425}
2025-05-21 17:21:33 {'loss': 2.0523, 'grad_norm': 10.379700660705566, 'learning_rate': 2.7625e-05, 'epoch': 0.45}
2025-05-21 17:21:55 {'loss': 1.9869, 'grad_norm': 11.48332691192627, 'learning_rate': 2.6375e-05, 'epoch': 0.475}
2025-05-21 17:22:09 {'loss': 1.7121, 'grad_norm': 10.231863021850586, 'learning_rate': 2.5124999999999997e-05, 'epoch': 0.5}
2025-05-21 17:22:24 {'loss': 1.6858, 'grad_norm': 8.638924598693848, 'learning_rate': 2.3875e-05, 'epoch': 0.525}
2025-05-21 17:22:39 {'loss': 1.8469, 'grad_norm': 8.401350021362305, 'learning_rate': 2.2625e-05, 'epoch': 0.55}
2025-05-21 17:22:54 {'loss': 1.9971, 'grad_norm': 11.020670890808105, 'learning_rate': 2.1375e-05, 'epoch': 0.575}
2025-05-21 17:23:09 {'loss': 1.5395, 'grad_norm': 8.10565185546875, 'learning_rate': 2.0125e-05, 'epoch': 0.6}
2025-05-21 17:23:24 {'loss': 1.6959, 'grad_norm': 9.15926742553711, 'learning_rate': 1.8875e-05, 'epoch': 0.625}
2025-05-21 17:23:45 {'loss': 1.9559, 'grad_norm': 11.926215171813965, 'learning_rate': 1.7625e-05, 'epoch': 0.65}
2025-05-21 17:23:59 {'loss': 1.7471, 'grad_norm': 10.76102352142334, 'learning_rate': 1.6375e-05, 'epoch': 0.675}
2025-05-21 17:24:15 {'loss': 1.7073, 'grad_norm': 12.357460021972656, 'learning_rate': 1.5125e-05, 'epoch': 0.7}
2025-05-21 17:24:30 {'loss': 1.9098, 'grad_norm': 9.912742614746094, 'learning_rate': 1.3875000000000002e-05, 'epoch': 0.725}
2025-05-21 17:24:44 {'loss': 1.5679, 'grad_norm': 11.620748519897461, 'learning_rate': 1.2625e-05, 'epoch': 0.75}
2025-05-21 17:24:59 {'loss': 1.7288, 'grad_norm': 10.822535514831543, 'learning_rate': 1.1375e-05, 'epoch': 0.775}
2025-05-21 17:25:14 {'loss': 1.4948, 'grad_norm': 10.573762893676758, 'learning_rate': 1.0125e-05, 'epoch': 0.8}
2025-05-21 17:25:29 {'loss': 1.7499, 'grad_norm': 10.137906074523926, 'learning_rate': 8.875e-06, 'epoch': 0.825}
2025-05-21 17:25:44 {'loss': 1.5527, 'grad_norm': 12.89706802368164, 'learning_rate': 7.625e-06, 'epoch': 0.85}
2025-05-21 17:26:05 {'loss': 1.5855, 'grad_norm': 11.109420776367188, 'learning_rate': 6.375000000000001e-06, 'epoch': 0.875}
2025-05-21 17:26:20 {'loss': 1.4924, 'grad_norm': 8.80353832244873, 'learning_rate': 5.125e-06, 'epoch': 0.9}
2025-05-21 17:26:35 {'loss': 1.7938, 'grad_norm': 10.938332557678223, 'learning_rate': 3.875e-06, 'epoch': 0.925}
2025-05-21 17:26:50 {'loss': 2.0224, 'grad_norm': 12.793330192565918, 'learning_rate': 2.625e-06, 'epoch': 0.95}
2025-05-21 17:27:05 {'loss': 1.7975, 'grad_norm': 8.842324256896973, 'learning_rate': 1.3750000000000002e-06, 'epoch': 0.975}
2025-05-21 17:27:20 {'loss': 1.5337, 'grad_norm': 10.450824737548828, 'learning_rate': 1.2500000000000002e-07, 'epoch': 1.0}
2025-05-21 17:27:20 {'train_runtime': 630.5431, 'train_samples_per_second': 1.269, 'train_steps_per_second': 0.634, 'train_loss': 1.9334967613220215, 'epoch': 1.0}
2025-05-21 17:28:36 {'eval_loss': 1.5096261501312256, 'eval_runtime': 14.2425, 'eval_samples_per_second': 14.042, 'eval_steps_per_second': 1.755, 'epoch': 1.0}
2025-05-21 17:29:06 {'loss': 1.5309, 'grad_norm': 9.099725723266602, 'learning_rate': 4.8875e-05, 'epoch': 0.025}
2025-05-21 17:29:21 {'loss': 1.6338, 'grad_norm': 10.919231414794922, 'learning_rate': 4.7625000000000006e-05, 'epoch': 0.05}
2025-05-21 17:29:35 {'loss': 1.5263, 'grad_norm': 11.164735794067383, 'learning_rate': 4.6375e-05, 'epoch': 0.075}
2025-05-21 17:29:56 {'loss': 1.6147, 'grad_norm': 8.288593292236328, 'learning_rate': 4.5125e-05, 'epoch': 0.1}
2025-05-21 17:30:11 {'loss': 1.4515, 'grad_norm': 10.797696113586426, 'learning_rate': 4.3875e-05, 'epoch': 0.125}
2025-05-21 17:30:26 {'loss': 1.5328, 'grad_norm': 11.012202262878418, 'learning_rate': 4.2625000000000006e-05, 'epoch': 0.15}
2025-05-21 17:30:41 {'loss': 1.2982, 'grad_norm': 11.279581069946289, 'learning_rate': 4.1375e-05, 'epoch': 0.175}
2025-05-21 17:30:55 {'loss': 1.3946, 'grad_norm': 9.794670104980469, 'learning_rate': 4.0125e-05, 'epoch': 0.2}
2025-05-21 17:31:16 {'loss': 1.5188, 'grad_norm': 9.930113792419434, 'learning_rate': 3.8875e-05, 'epoch': 0.225}
2025-05-21 17:31:31 {'loss': 1.5141, 'grad_norm': 10.441512107849121, 'learning_rate': 3.7625e-05, 'epoch': 0.25}
2025-05-21 17:31:46 {'loss': 1.4702, 'grad_norm': 9.359715461730957, 'learning_rate': 3.6375e-05, 'epoch': 0.275}
2025-05-21 17:32:01 {'loss': 1.6726, 'grad_norm': 11.811548233032227, 'learning_rate': 3.5125e-05, 'epoch': 0.3}
2025-05-21 17:32:16 {'loss': 1.376, 'grad_norm': 7.200644493103027, 'learning_rate': 3.3875000000000003e-05, 'epoch': 0.325}
2025-05-21 17:32:30 {'loss': 1.545, 'grad_norm': 11.528031349182129, 'learning_rate': 3.2625e-05, 'epoch': 0.35}
2025-05-21 17:32:45 {'loss': 1.4502, 'grad_norm': 11.757255554199219, 'learning_rate': 3.1375e-05, 'epoch': 0.375}
2025-05-21 17:33:05 {'loss': 1.4283, 'grad_norm': 10.625152587890625, 'learning_rate': 3.0125000000000004e-05, 'epoch': 0.4}
2025-05-21 17:33:20 {'loss': 1.4141, 'grad_norm': 10.311325073242188, 'learning_rate': 2.8875e-05, 'epoch': 0.425}
2025-05-21 17:33:34 {'loss': 1.6085, 'grad_norm': 10.399246215820312, 'learning_rate': 2.7625e-05, 'epoch': 0.45}
2025-05-21 17:33:49 {'loss': 1.5902, 'grad_norm': 12.154425621032715, 'learning_rate': 2.6375e-05, 'epoch': 0.475}
2025-05-21 17:34:04 {'loss': 1.3642, 'grad_norm': 10.23400592803955, 'learning_rate': 2.5124999999999997e-05, 'epoch': 0.5}
2025-05-21 17:34:18 {'loss': 1.3556, 'grad_norm': 8.512118339538574, 'learning_rate': 2.3875e-05, 'epoch': 0.525}
2025-05-21 17:34:33 {'loss': 1.5367, 'grad_norm': 8.35689926147461, 'learning_rate': 2.2625e-05, 'epoch': 0.55}
2025-05-21 17:34:47 {'loss': 1.6386, 'grad_norm': 11.660429954528809, 'learning_rate': 2.1375e-05, 'epoch': 0.575}
2025-05-21 17:35:08 {'loss': 1.2581, 'grad_norm': 7.05364990234375, 'learning_rate': 2.0125e-05, 'epoch': 0.6}
2025-05-21 17:35:23 {'loss': 1.4232, 'grad_norm': 7.513388633728027, 'learning_rate': 1.8875e-05, 'epoch': 0.625}
2025-05-21 17:35:38 {'loss': 1.6122, 'grad_norm': 10.23376178741455, 'learning_rate': 1.7625e-05, 'epoch': 0.65}
2025-05-21 17:35:53 {'loss': 1.4533, 'grad_norm': 9.009908676147461, 'learning_rate': 1.6375e-05, 'epoch': 0.675}
2025-05-21 17:36:07 {'loss': 1.4553, 'grad_norm': 12.061803817749023, 'learning_rate': 1.5125e-05, 'epoch': 0.7}
2025-05-21 17:36:22 {'loss': 1.6364, 'grad_norm': 8.727347373962402, 'learning_rate': 1.3875000000000002e-05, 'epoch': 0.725}
2025-05-21 17:36:37 {'loss': 1.3078, 'grad_norm': 10.824244499206543, 'learning_rate': 1.2625e-05, 'epoch': 0.75}
2025-05-21 17:36:52 {'loss': 1.4848, 'grad_norm': 10.810641288757324, 'learning_rate': 1.1375e-05, 'epoch': 0.775}
2025-05-21 17:37:12 {'loss': 1.281, 'grad_norm': 9.890715599060059, 'learning_rate': 1.0125e-05, 'epoch': 0.8}
2025-05-21 17:37:27 {'loss': 1.4194, 'grad_norm': 10.057692527770996, 'learning_rate': 8.875e-06, 'epoch': 0.825}
2025-05-21 17:37:42 {'loss': 1.3223, 'grad_norm': 12.14645767211914, 'learning_rate': 7.625e-06, 'epoch': 0.85}
2025-05-21 17:37:57 {'loss': 1.3843, 'grad_norm': 10.13056468963623, 'learning_rate': 6.375000000000001e-06, 'epoch': 0.875}
2025-05-21 17:38:11 {'loss': 1.3138, 'grad_norm': 9.139432907104492, 'learning_rate': 5.125e-06, 'epoch': 0.9}
2025-05-21 17:38:32 {'loss': 1.6179, 'grad_norm': 10.068380355834961, 'learning_rate': 3.875e-06, 'epoch': 0.925}
2025-05-21 17:38:47 {'loss': 1.6985, 'grad_norm': 11.242310523986816, 'learning_rate': 2.625e-06, 'epoch': 0.95}
2025-05-21 17:38:56 {'loss': 1.3964, 'grad_norm': 6.658202171325684, 'learning_rate': 1.3750000000000002e-06, 'epoch': 0.975}
2025-05-21 17:39:06 {'loss': 0.91, 'grad_norm': 7.413536548614502, 'learning_rate': 1.2500000000000002e-07, 'epoch': 1.0}
2025-05-21 17:39:06 {'train_runtime': 614.4986, 'train_samples_per_second': 1.302, 'train_steps_per_second': 0.651, 'train_loss': 1.4610222864151001, 'epoch': 1.0}
2025-05-21 17:40:13 {'eval_loss': 1.4678747653961182, 'eval_runtime': 9.3474, 'eval_samples_per_second': 21.396, 'eval_steps_per_second': 2.675, 'epoch': 1.0}
2025-05-21 17:40:55 {'loss': 1.0949, 'grad_norm': 8.573458671569824, 'learning_rate': 4.8875e-05, 'epoch': 0.025}
2025-05-21 17:41:09 {'loss': 1.2191, 'grad_norm': 9.200946807861328, 'learning_rate': 4.7625000000000006e-05, 'epoch': 0.05}
2025-05-21 17:41:24 {'loss': 1.1801, 'grad_norm': 8.245063781738281, 'learning_rate': 4.6375e-05, 'epoch': 0.075}
2025-05-21 17:41:38 {'loss': 1.2475, 'grad_norm': 7.5204315185546875, 'learning_rate': 4.5125e-05, 'epoch': 0.1}
2025-05-21 17:41:53 {'loss': 1.1948, 'grad_norm': 9.754060745239258, 'learning_rate': 4.3875e-05, 'epoch': 0.125}
2025-05-21 17:42:14 {'loss': 1.1976, 'grad_norm': 10.615896224975586, 'learning_rate': 4.2625000000000006e-05, 'epoch': 0.15}
2025-05-21 17:42:28 {'loss': 1.0116, 'grad_norm': 9.375036239624023, 'learning_rate': 4.1375e-05, 'epoch': 0.175}
2025-05-21 17:42:43 {'loss': 1.1784, 'grad_norm': 8.249297142028809, 'learning_rate': 4.0125e-05, 'epoch': 0.2}
2025-05-21 17:42:57 {'loss': 1.2391, 'grad_norm': 9.335128784179688, 'learning_rate': 3.8875e-05, 'epoch': 0.225}
2025-05-21 17:43:12 {'loss': 1.2398, 'grad_norm': 9.329742431640625, 'learning_rate': 3.7625e-05, 'epoch': 0.25}
2025-05-21 17:43:26 {'loss': 1.214, 'grad_norm': 8.500500679016113, 'learning_rate': 3.6375e-05, 'epoch': 0.275}
2025-05-21 17:43:47 {'loss': 1.3952, 'grad_norm': 11.392909049987793, 'learning_rate': 3.5125e-05, 'epoch': 0.3}
2025-05-21 17:44:02 {'loss': 1.1459, 'grad_norm': 7.90590763092041, 'learning_rate': 3.3875000000000003e-05, 'epoch': 0.325}
2025-05-21 17:44:16 {'loss': 1.2918, 'grad_norm': 11.214473724365234, 'learning_rate': 3.2625e-05, 'epoch': 0.35}
2025-05-21 17:44:31 {'loss': 1.1968, 'grad_norm': 10.018423080444336, 'learning_rate': 3.1375e-05, 'epoch': 0.375}
2025-05-21 17:44:46 {'loss': 1.2032, 'grad_norm': 8.184170722961426, 'learning_rate': 3.0125000000000004e-05, 'epoch': 0.4}
2025-05-21 17:45:00 {'loss': 1.2284, 'grad_norm': 9.756196022033691, 'learning_rate': 2.8875e-05, 'epoch': 0.425}
2025-05-21 17:45:15 {'loss': 1.4047, 'grad_norm': 9.470294952392578, 'learning_rate': 2.7625e-05, 'epoch': 0.45}
2025-05-21 17:45:29 {'loss': 1.3674, 'grad_norm': 9.841180801391602, 'learning_rate': 2.6375e-05, 'epoch': 0.475}
2025-05-21 17:45:44 {'loss': 1.1756, 'grad_norm': 9.98120403289795, 'learning_rate': 2.5124999999999997e-05, 'epoch': 0.5}
2025-05-21 17:45:59 {'loss': 1.1618, 'grad_norm': 7.928491115570068, 'learning_rate': 2.3875e-05, 'epoch': 0.525}
2025-05-21 17:46:20 {'loss': 1.3302, 'grad_norm': 8.359090805053711, 'learning_rate': 2.2625e-05, 'epoch': 0.55}
2025-05-21 17:46:35 {'loss': 1.4532, 'grad_norm': 11.792808532714844, 'learning_rate': 2.1375e-05, 'epoch': 0.575}
2025-05-21 17:46:49 {'loss': 1.1077, 'grad_norm': 6.771246433258057, 'learning_rate': 2.0125e-05, 'epoch': 0.6}
2025-05-21 17:47:04 {'loss': 1.2675, 'grad_norm': 8.074199676513672, 'learning_rate': 1.8875e-05, 'epoch': 0.625}
2025-05-21 17:47:19 {'loss': 1.4505, 'grad_norm': 11.068415641784668, 'learning_rate': 1.7625e-05, 'epoch': 0.65}
2025-05-21 17:47:34 {'loss': 1.3223, 'grad_norm': 9.98000717163086, 'learning_rate': 1.6375e-05, 'epoch': 0.675}
2025-05-21 17:47:49 {'loss': 1.3199, 'grad_norm': 12.147781372070312, 'learning_rate': 1.5125e-05, 'epoch': 0.7}
2025-05-21 17:48:10 {'loss': 1.493, 'grad_norm': 9.492140769958496, 'learning_rate': 1.3875000000000002e-05, 'epoch': 0.725}
2025-05-21 17:48:25 {'loss': 1.1972, 'grad_norm': 11.537666320800781, 'learning_rate': 1.2625e-05, 'epoch': 0.75}
2025-05-21 17:48:39 {'loss': 1.3849, 'grad_norm': 8.837778091430664, 'learning_rate': 1.1375e-05, 'epoch': 0.775}
2025-05-21 17:48:55 {'loss': 1.1839, 'grad_norm': 11.279258728027344, 'learning_rate': 1.0125e-05, 'epoch': 0.8}
2025-05-21 17:49:09 {'loss': 1.2969, 'grad_norm': 8.976175308227539, 'learning_rate': 8.875e-06, 'epoch': 0.825}
2025-05-21 17:49:24 {'loss': 1.25, 'grad_norm': 12.754247665405273, 'learning_rate': 7.625e-06, 'epoch': 0.85}
2025-05-21 17:49:39 {'loss': 1.3042, 'grad_norm': 10.20521068572998, 'learning_rate': 6.375000000000001e-06, 'epoch': 0.875}
2025-05-21 17:50:00 {'loss': 1.2636, 'grad_norm': 8.585806846618652, 'learning_rate': 5.125e-06, 'epoch': 0.9}
2025-05-21 17:50:15 {'loss': 1.5476, 'grad_norm': 9.617477416992188, 'learning_rate': 3.875e-06, 'epoch': 0.925}
2025-05-21 17:50:30 {'loss': 1.6198, 'grad_norm': 12.090703964233398, 'learning_rate': 2.625e-06, 'epoch': 0.95}
2025-05-21 17:50:45 {'loss': 1.2687, 'grad_norm': 7.129505157470703, 'learning_rate': 1.3750000000000002e-06, 'epoch': 0.975}
2025-05-21 17:50:59 {'loss': 0.7543, 'grad_norm': 7.306861877441406, 'learning_rate': 1.2500000000000002e-07, 'epoch': 1.0}
2025-05-21 17:50:59 {'train_runtime': 623.8425, 'train_samples_per_second': 1.282, 'train_steps_per_second': 0.641, 'train_loss': 1.2600689029693604, 'epoch': 1.0}
2025-05-21 17:52:21 {'eval_loss': 1.4745087623596191, 'eval_runtime': 13.5357, 'eval_samples_per_second': 14.776, 'eval_steps_per_second': 1.847, 'epoch': 1.0}
2025-05-21 17:53:05 {'loss': 0.8034, 'grad_norm': 8.420910835266113, 'learning_rate': 4.8875e-05, 'epoch': 0.025}
2025-05-21 17:53:19 {'loss': 0.9676, 'grad_norm': 9.265372276306152, 'learning_rate': 4.7625000000000006e-05, 'epoch': 0.05}
2025-05-21 17:53:34 {'loss': 0.9258, 'grad_norm': 9.960258483886719, 'learning_rate': 4.6375e-05, 'epoch': 0.075}
2025-05-21 17:53:49 {'loss': 0.9916, 'grad_norm': 7.160867691040039, 'learning_rate': 4.5125e-05, 'epoch': 0.1}
2025-05-21 17:54:10 {'loss': 0.9783, 'grad_norm': 9.009163856506348, 'learning_rate': 4.3875e-05, 'epoch': 0.125}
2025-05-21 17:54:25 {'loss': 0.9688, 'grad_norm': 9.431229591369629, 'learning_rate': 4.2625000000000006e-05, 'epoch': 0.15}
2025-05-21 17:54:39 {'loss': 0.8338, 'grad_norm': 11.585589408874512, 'learning_rate': 4.1375e-05, 'epoch': 0.175}
2025-05-21 17:54:54 {'loss': 0.9898, 'grad_norm': 8.411323547363281, 'learning_rate': 4.0125e-05, 'epoch': 0.2}
2025-05-21 17:55:09 {'loss': 1.0373, 'grad_norm': 9.523872375488281, 'learning_rate': 3.8875e-05, 'epoch': 0.225}
2025-05-21 17:55:23 {'loss': 1.034, 'grad_norm': 10.016279220581055, 'learning_rate': 3.7625e-05, 'epoch': 0.25}
2025-05-21 17:55:44 {'loss': 1.0127, 'grad_norm': 8.106352806091309, 'learning_rate': 3.6375e-05, 'epoch': 0.275}
2025-05-21 17:55:59 {'loss': 1.1685, 'grad_norm': 12.212920188903809, 'learning_rate': 3.5125e-05, 'epoch': 0.3}
2025-05-21 17:56:14 {'loss': 0.9583, 'grad_norm': 7.644230365753174, 'learning_rate': 3.3875000000000003e-05, 'epoch': 0.325}
2025-05-21 17:56:28 {'loss': 1.0875, 'grad_norm': 9.541053771972656, 'learning_rate': 3.2625e-05, 'epoch': 0.35}
2025-05-21 17:56:43 {'loss': 1.0518, 'grad_norm': 9.525766372680664, 'learning_rate': 3.1375e-05, 'epoch': 0.375}
2025-05-21 17:56:58 {'loss': 1.0382, 'grad_norm': 8.143584251403809, 'learning_rate': 3.0125000000000004e-05, 'epoch': 0.4}
2025-05-21 17:57:12 {'loss': 1.0762, 'grad_norm': 9.341533660888672, 'learning_rate': 2.8875e-05, 'epoch': 0.425}
2025-05-21 17:57:27 {'loss': 1.2097, 'grad_norm': 9.236954689025879, 'learning_rate': 2.7625e-05, 'epoch': 0.45}
2025-05-21 17:57:48 {'loss': 1.1911, 'grad_norm': 9.77398681640625, 'learning_rate': 2.6375e-05, 'epoch': 0.475}
2025-05-21 17:58:02 {'loss': 1.0435, 'grad_norm': 8.316080093383789, 'learning_rate': 2.5124999999999997e-05, 'epoch': 0.5}
2025-05-21 17:58:17 {'loss': 1.0401, 'grad_norm': 8.47297191619873, 'learning_rate': 2.3875e-05, 'epoch': 0.525}
2025-05-21 17:58:31 {'loss': 1.1668, 'grad_norm': 7.582188606262207, 'learning_rate': 2.2625e-05, 'epoch': 0.55}
2025-05-21 17:58:46 {'loss': 1.2898, 'grad_norm': 11.110459327697754, 'learning_rate': 2.1375e-05, 'epoch': 0.575}
2025-05-21 17:59:00 {'loss': 0.9933, 'grad_norm': 6.397116184234619, 'learning_rate': 2.0125e-05, 'epoch': 0.6}
2025-05-21 17:59:15 {'loss': 1.1508, 'grad_norm': 7.640918254852295, 'learning_rate': 1.8875e-05, 'epoch': 0.625}
2025-05-21 17:59:36 {'loss': 1.328, 'grad_norm': 10.4454927444458, 'learning_rate': 1.7625e-05, 'epoch': 0.65}
2025-05-21 17:59:51 {'loss': 1.2198, 'grad_norm': 8.872220993041992, 'learning_rate': 1.6375e-05, 'epoch': 0.675}
2025-05-21 18:00:05 {'loss': 1.2009, 'grad_norm': 10.648439407348633, 'learning_rate': 1.5125e-05, 'epoch': 0.7}
2025-05-21 18:00:20 {'loss': 1.3963, 'grad_norm': 10.045294761657715, 'learning_rate': 1.3875000000000002e-05, 'epoch': 0.725}
2025-05-21 18:00:35 {'loss': 1.1221, 'grad_norm': 10.614605903625488, 'learning_rate': 1.2625e-05, 'epoch': 0.75}
2025-05-21 18:00:49 {'loss': 1.283, 'grad_norm': 8.037344932556152, 'learning_rate': 1.1375e-05, 'epoch': 0.775}
2025-05-21 18:01:10 {'loss': 1.1081, 'grad_norm': 10.083134651184082, 'learning_rate': 1.0125e-05, 'epoch': 0.8}
2025-05-21 18:01:25 {'loss': 1.2142, 'grad_norm': 9.91692066192627, 'learning_rate': 8.875e-06, 'epoch': 0.825}
2025-05-21 18:01:40 {'loss': 1.191, 'grad_norm': 12.120384216308594, 'learning_rate': 7.625e-06, 'epoch': 0.85}
2025-05-21 18:01:55 {'loss': 1.2676, 'grad_norm': 10.302796363830566, 'learning_rate': 6.375000000000001e-06, 'epoch': 0.875}
2025-05-21 18:02:09 {'loss': 1.2255, 'grad_norm': 10.20496940612793, 'learning_rate': 5.125e-06, 'epoch': 0.9}
2025-05-21 18:02:24 {'loss': 1.522, 'grad_norm': 11.761357307434082, 'learning_rate': 3.875e-06, 'epoch': 0.925}
2025-05-21 18:02:39 {'loss': 1.5673, 'grad_norm': 12.868289947509766, 'learning_rate': 2.625e-06, 'epoch': 0.95}
2025-05-21 18:02:54 {'loss': 1.1889, 'grad_norm': 7.294971942901611, 'learning_rate': 1.3750000000000002e-06, 'epoch': 0.975}
2025-05-21 18:03:03 {'loss': 0.6554, 'grad_norm': 8.016361236572266, 'learning_rate': 1.2500000000000002e-07, 'epoch': 1.0}
2025-05-21 18:03:03 {'train_runtime': 619.2753, 'train_samples_per_second': 1.292, 'train_steps_per_second': 0.646, 'train_loss': 1.112468627691269, 'epoch': 1.0}
2025-05-21 18:03:59 {'eval_loss': 1.4852486848831177, 'eval_runtime': 8.0949, 'eval_samples_per_second': 24.707, 'eval_steps_per_second': 3.088, 'epoch': 1.0}
2025-05-21 18:04:36 {'loss': 0.5841, 'grad_norm': 6.857686996459961, 'learning_rate': 4.8875e-05, 'epoch': 0.025}
2025-05-21 18:04:51 {'loss': 0.7239, 'grad_norm': 10.050824165344238, 'learning_rate': 4.7625000000000006e-05, 'epoch': 0.05}
2025-05-21 18:05:12 {'loss': 0.7405, 'grad_norm': 7.802554607391357, 'learning_rate': 4.6375e-05, 'epoch': 0.075}
2025-05-21 18:05:27 {'loss': 0.8022, 'grad_norm': 6.078256607055664, 'learning_rate': 4.5125e-05, 'epoch': 0.1}
2025-05-21 18:05:41 {'loss': 0.7805, 'grad_norm': 7.794881343841553, 'learning_rate': 4.3875e-05, 'epoch': 0.125}
2025-05-21 18:05:56 {'loss': 0.79, 'grad_norm': 10.287562370300293, 'learning_rate': 4.2625000000000006e-05, 'epoch': 0.15}
2025-05-21 18:06:11 {'loss': 0.6646, 'grad_norm': 10.04893684387207, 'learning_rate': 4.1375e-05, 'epoch': 0.175}
2025-05-21 18:06:25 {'loss': 0.821, 'grad_norm': 9.238807678222656, 'learning_rate': 4.0125e-05, 'epoch': 0.2}
2025-05-21 18:06:40 {'loss': 0.8719, 'grad_norm': 8.442605972290039, 'learning_rate': 3.8875e-05, 'epoch': 0.225}
2025-05-21 18:07:01 {'loss': 0.8625, 'grad_norm': 8.753212928771973, 'learning_rate': 3.7625e-05, 'epoch': 0.25}
2025-05-21 18:07:16 {'loss': 0.8561, 'grad_norm': 8.464815139770508, 'learning_rate': 3.6375e-05, 'epoch': 0.275}
2025-05-21 18:07:31 {'loss': 0.9892, 'grad_norm': 9.502264976501465, 'learning_rate': 3.5125e-05, 'epoch': 0.3}
2025-05-21 18:07:45 {'loss': 0.8083, 'grad_norm': 6.949486255645752, 'learning_rate': 3.3875000000000003e-05, 'epoch': 0.325}
2025-05-21 18:08:00 {'loss': 0.9213, 'grad_norm': 9.405105590820312, 'learning_rate': 3.2625e-05, 'epoch': 0.35}
2025-05-21 18:08:14 {'loss': 0.8688, 'grad_norm': 10.394392967224121, 'learning_rate': 3.1375e-05, 'epoch': 0.375}
2025-05-21 18:08:29 {'loss': 0.9094, 'grad_norm': 6.951852321624756, 'learning_rate': 3.0125000000000004e-05, 'epoch': 0.4}
2025-05-21 18:08:50 {'loss': 0.9276, 'grad_norm': 9.41880989074707, 'learning_rate': 2.8875e-05, 'epoch': 0.425}
2025-05-21 18:09:05 {'loss': 1.055, 'grad_norm': 9.644485473632812, 'learning_rate': 2.7625e-05, 'epoch': 0.45}
2025-05-21 18:09:20 {'loss': 1.0476, 'grad_norm': 9.452999114990234, 'learning_rate': 2.6375e-05, 'epoch': 0.475}
2025-05-21 18:09:35 {'loss': 0.9198, 'grad_norm': 9.165353775024414, 'learning_rate': 2.5124999999999997e-05, 'epoch': 0.5}
2025-05-21 18:09:49 {'loss': 0.9236, 'grad_norm': 7.962134838104248, 'learning_rate': 2.3875e-05, 'epoch': 0.525}
2025-05-21 18:10:04 {'loss': 1.0296, 'grad_norm': 7.635830402374268, 'learning_rate': 2.2625e-05, 'epoch': 0.55}
2025-05-21 18:10:19 {'loss': 1.1414, 'grad_norm': 10.826729774475098, 'learning_rate': 2.1375e-05, 'epoch': 0.575}
2025-05-21 18:10:34 {'loss': 0.8931, 'grad_norm': 7.406645774841309, 'learning_rate': 2.0125e-05, 'epoch': 0.6}
2025-05-21 18:10:55 {'loss': 1.0604, 'grad_norm': 7.7753753662109375, 'learning_rate': 1.8875e-05, 'epoch': 0.625}
2025-05-21 18:11:09 {'loss': 1.2285, 'grad_norm': 10.28802490234375, 'learning_rate': 1.7625e-05, 'epoch': 0.65}
2025-05-21 18:11:24 {'loss': 1.1243, 'grad_norm': 9.691778182983398, 'learning_rate': 1.6375e-05, 'epoch': 0.675}
2025-05-21 18:11:39 {'loss': 1.1169, 'grad_norm': 10.243146896362305, 'learning_rate': 1.5125e-05, 'epoch': 0.7}
2025-05-21 18:11:53 {'loss': 1.3043, 'grad_norm': 10.627593040466309, 'learning_rate': 1.3875000000000002e-05, 'epoch': 0.725}
2025-05-21 18:12:08 {'loss': 1.0416, 'grad_norm': 9.376628875732422, 'learning_rate': 1.2625e-05, 'epoch': 0.75}
2025-05-21 18:12:23 {'loss': 1.2132, 'grad_norm': 7.986268997192383, 'learning_rate': 1.1375e-05, 'epoch': 0.775}
2025-05-21 18:12:38 {'loss': 1.0618, 'grad_norm': 11.123740196228027, 'learning_rate': 1.0125e-05, 'epoch': 0.8}
2025-05-21 18:12:52 {'loss': 1.1444, 'grad_norm': 10.454724311828613, 'learning_rate': 8.875e-06, 'epoch': 0.825}
2025-05-21 18:13:07 {'loss': 1.1364, 'grad_norm': 11.98070240020752, 'learning_rate': 7.625e-06, 'epoch': 0.85}
2025-05-21 18:13:22 {'loss': 1.2208, 'grad_norm': 10.494217872619629, 'learning_rate': 6.375000000000001e-06, 'epoch': 0.875}
2025-05-21 18:13:44 {'loss': 1.1724, 'grad_norm': 12.189960479736328, 'learning_rate': 5.125e-06, 'epoch': 0.9}
2025-05-21 18:13:58 {'loss': 1.4761, 'grad_norm': 10.118169784545898, 'learning_rate': 3.875e-06, 'epoch': 0.925}
2025-05-21 18:14:13 {'loss': 1.5297, 'grad_norm': 13.849287986755371, 'learning_rate': 2.625e-06, 'epoch': 0.95}
2025-05-21 18:14:28 {'loss': 1.114, 'grad_norm': 7.404979705810547, 'learning_rate': 1.3750000000000002e-06, 'epoch': 0.975}
2025-05-21 18:14:40 {'loss': 0.5907, 'grad_norm': 7.128823757171631, 'learning_rate': 1.2500000000000002e-07, 'epoch': 1.0}
2025-05-21 18:14:40 {'train_runtime': 618.246, 'train_samples_per_second': 1.294, 'train_steps_per_second': 0.647, 'train_loss': 0.9866857695579528, 'epoch': 1.0}
2025-05-21 18:15:47 {'eval_loss': 1.5039160251617432, 'eval_runtime': 11.3979, 'eval_samples_per_second': 17.547, 'eval_steps_per_second': 2.193, 'epoch': 1.0}
2025-05-21 17:27:33 INFO :      Sent reply
2025-05-21 17:28:20 INFO :      
2025-05-21 17:28:20 INFO :      Received: evaluate message 5fafb0b9-4046-4575-aeea-10be230cad2d
2025-05-21 17:28:36 INFO :      Sent reply
2025-05-21 17:28:50 INFO :      
2025-05-21 17:28:50 INFO :      Received: train message 5703ff5d-27c8-4d2c-b6d8-fe0422499395
2025-05-21 17:39:16 INFO :      Sent reply
2025-05-21 17:40:02 INFO :      
2025-05-21 17:40:02 INFO :      Received: evaluate message eee3b0c4-d4d0-475b-8f78-db906f9fd085
2025-05-21 17:40:13 INFO :      Sent reply
2025-05-21 17:40:33 INFO :      
2025-05-21 17:40:33 INFO :      Received: train message 24bfca40-3da5-4e53-8b69-4c65a1c16dfb
2025-05-21 17:51:13 INFO :      Sent reply
2025-05-21 17:52:04 INFO :      
2025-05-21 17:52:04 INFO :      Received: evaluate message cb603ea7-cfb1-4a76-9b5d-a2f485397206
2025-05-21 17:52:21 INFO :      Sent reply
2025-05-21 17:52:38 INFO :      
2025-05-21 17:52:38 INFO :      Received: train message dad5fb3e-8c30-48c9-83c3-48fa5d249ff8
2025-05-21 18:03:09 INFO :      Sent reply
2025-05-21 18:03:50 INFO :      
2025-05-21 18:03:50 INFO :      Received: evaluate message 4b6f0604-5625-459f-924c-3bee4b5fefd9
2025-05-21 18:03:59 INFO :      Sent reply
2025-05-21 18:04:20 INFO :      
2025-05-21 18:04:20 INFO :      Received: train message 4bfbc04d-3d75-4272-ad20-d7ed92256ccd
2025-05-21 18:14:50 INFO :      Sent reply
2025-05-21 18:15:34 INFO :      
2025-05-21 18:15:34 INFO :      Received: evaluate message d870f629-9843-4382-a524-b54ea157b633
2025-05-21 18:15:47 INFO :      Sent reply
2025-05-21 18:15:48 INFO :      
2025-05-21 18:15:48 INFO :      Received: reconnect message b7183f55-7264-49b7-afa7-beff5d7e7a48
2025-05-21 18:15:48 INFO :      Disconnect and shut down
