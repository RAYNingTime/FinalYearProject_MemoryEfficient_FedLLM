2025-05-01 16:56:20 
Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]
Generating train split:  72%|███████▎  | 87000/120000 [00:00<00:00, 858723.97 examples/s]
Generating train split: 100%|██████████| 120000/120000 [00:00<00:00, 974918.85 examples/s]
2025-05-01 16:56:20 
Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]
Generating test split: 100%|██████████| 7600/7600 [00:00<00:00, 993848.92 examples/s]
2025-05-01 16:56:22 
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 1036.84 examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 1031.92 examples/s]
2025-05-01 16:56:22 
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]
Map:  17%|█▋        | 167/1000 [00:00<00:00, 1641.19 examples/s]
Map:  44%|████▎     | 435/1000 [00:00<00:00, 2242.42 examples/s]
Map:  68%|██████▊   | 678/1000 [00:00<00:00, 2327.13 examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 2502.06 examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 2364.63 examples/s]
2025-05-01 16:56:22 /app/client.py:45: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
2025-05-01 16:56:22   trainer = Trainer(
2025-05-01 16:56:22 WARNING :   DEPRECATED FEATURE: flwr.client.start_numpy_client() is deprecated. 
2025-05-01 16:56:22 Instead, use `flwr.client.start_client()` by ensuring you first call the `.to_client()` method as shown below: 
2025-05-01 16:56:22 flwr.client.start_client(
2025-05-01 16:56:22 server_address='<IP>:<PORT>',
2025-05-01 16:56:22 client=FlowerClient().to_client(), # <-- where FlowerClient is of type flwr.client.NumPyClient object
2025-05-01 16:56:22 )
2025-05-01 16:56:22 Using `start_numpy_client()` is deprecated.
2025-05-01 16:56:22 
2025-05-01 16:56:22             This is a deprecated feature. It will be removed
2025-05-01 16:56:22             entirely in future versions of Flower.
2025-05-01 16:56:22         
2025-05-01 16:56:22 WARNING :   DEPRECATED FEATURE: flwr.client.start_client() is deprecated.
2025-05-01 16:56:22 Instead, use the `flower-supernode` CLI command to start a SuperNode as shown below:
2025-05-01 16:56:22 
2025-05-01 16:56:22 $ flower-supernode --insecure --superlink='<IP>:<PORT>'
2025-05-01 16:56:22 
2025-05-01 16:56:22 To view all available options, run:
2025-05-01 16:56:22 
2025-05-01 16:56:22 $ flower-supernode --help
2025-05-01 16:56:22 
2025-05-01 16:56:22 Using `start_client()` is deprecated.
2025-05-01 16:56:22 
2025-05-01 16:56:22             This is a deprecated feature. It will be removed
2025-05-01 16:56:22             entirely in future versions of Flower.
2025-05-01 16:56:22         
2025-05-01 16:56:22 INFO :      
2025-05-01 16:56:22 INFO :      Received: get_parameters message 950eadc1-52fd-4de6-93a0-4afdff2a39bb
2025-05-01 16:56:29 INFO :      Sent reply
2025-05-01 16:56:47 INFO :      
2025-05-01 16:56:47 INFO :      Received: train message dd52fb7a-f57c-42d6-993f-645388a8d867
2025-05-01 16:56:48 /usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.
2025-05-01 16:56:48   warnings.warn(warn_msg)
2025-05-01 16:57:43 {'loss': 2.6662, 'grad_norm': 12.391962051391602, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-01 16:58:22 {'loss': 1.4508, 'grad_norm': 14.513712882995605, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-01 16:59:01 {'loss': 1.5937, 'grad_norm': 13.39852237701416, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-01 16:59:38 {'loss': 1.629, 'grad_norm': 13.995201110839844, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-01 17:00:15 {'loss': 1.5512, 'grad_norm': 13.600688934326172, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-01 17:00:52 {'loss': 1.509, 'grad_norm': 14.088199615478516, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-01 17:01:35 {'loss': 1.6351, 'grad_norm': 18.110595703125, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-01 17:02:17 {'loss': 1.359, 'grad_norm': 11.39002799987793, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-01 17:02:56 {'loss': 1.405, 'grad_norm': 12.682809829711914, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-01 17:03:35 {'loss': 1.5175, 'grad_norm': 14.580314636230469, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-01 17:04:13 {'loss': 1.4927, 'grad_norm': 13.662179946899414, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-01 17:04:51 {'loss': 1.5636, 'grad_norm': 17.04266357421875, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-01 17:05:30 {'loss': 1.409, 'grad_norm': 10.997357368469238, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-01 17:06:09 {'loss': 1.4555, 'grad_norm': 10.532724380493164, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-01 17:06:49 {'loss': 1.7753, 'grad_norm': 18.49554443359375, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-01 17:07:28 {'loss': 1.5404, 'grad_norm': 12.469453811645508, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-01 17:08:09 {'loss': 1.362, 'grad_norm': 12.122188568115234, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-01 17:08:50 {'loss': 1.4101, 'grad_norm': 12.094453811645508, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-01 17:09:29 {'loss': 1.4549, 'grad_norm': 11.1176118850708, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-01 17:10:08 {'loss': 1.5574, 'grad_norm': 11.998001098632812, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-01 17:10:48 {'loss': 1.3598, 'grad_norm': 12.752941131591797, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-01 17:11:25 {'loss': 1.556, 'grad_norm': 16.37736701965332, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-01 17:12:05 {'loss': 1.5748, 'grad_norm': 12.573687553405762, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-01 17:12:44 {'loss': 1.2963, 'grad_norm': 9.365303993225098, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-01 17:13:24 {'loss': 1.3681, 'grad_norm': 15.273015022277832, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-01 17:14:04 {'loss': 1.598, 'grad_norm': 15.911567687988281, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-01 17:14:43 {'loss': 1.4107, 'grad_norm': 10.422139167785645, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-01 17:15:22 {'loss': 1.4404, 'grad_norm': 15.662635803222656, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-01 17:16:02 {'loss': 1.4199, 'grad_norm': 12.161609649658203, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-01 17:16:42 {'loss': 1.4338, 'grad_norm': 11.970267295837402, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-01 17:17:22 {'loss': 1.4529, 'grad_norm': 7.49110746383667, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-01 17:18:02 {'loss': 1.4126, 'grad_norm': 11.438653945922852, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-01 17:18:41 {'loss': 1.2018, 'grad_norm': 9.389483451843262, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-01 17:19:22 {'loss': 1.4139, 'grad_norm': 11.02976131439209, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-01 17:20:00 {'loss': 1.446, 'grad_norm': 13.41610050201416, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-01 17:20:38 {'loss': 1.4773, 'grad_norm': 11.167899131774902, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-01 17:21:17 {'loss': 1.4216, 'grad_norm': 11.896980285644531, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-01 17:21:56 {'loss': 1.3564, 'grad_norm': 13.350176811218262, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-01 17:22:35 {'loss': 1.3969, 'grad_norm': 12.774413108825684, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-01 17:23:13 {'loss': 1.2674, 'grad_norm': 11.842689514160156, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-01 17:23:52 {'loss': 1.4189, 'grad_norm': 10.729519844055176, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-01 17:24:31 {'loss': 1.4777, 'grad_norm': 12.171703338623047, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-01 17:25:10 {'loss': 1.3306, 'grad_norm': 11.175091743469238, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-01 17:25:50 {'loss': 1.5243, 'grad_norm': 13.880160331726074, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-01 17:26:27 {'loss': 1.3041, 'grad_norm': 12.880005836486816, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-01 17:27:06 {'loss': 1.4324, 'grad_norm': 11.675780296325684, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-01 17:27:43 {'loss': 1.4524, 'grad_norm': 11.932045936584473, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-01 17:28:21 {'loss': 1.455, 'grad_norm': 10.952859878540039, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-01 17:28:58 {'loss': 1.536, 'grad_norm': 11.670601844787598, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-01 17:29:40 {'loss': 1.3449, 'grad_norm': 12.07453441619873, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-01 17:29:40 {'train_runtime': 1971.737, 'train_samples_per_second': 0.507, 'train_steps_per_second': 0.254, 'train_loss': 1.4783621101379394, 'epoch': 1.0}
