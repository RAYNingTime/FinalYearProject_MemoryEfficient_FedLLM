2025-05-01 18:55:36 
Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]
Generating train split:  93%|█████████▎| 112000/120000 [00:00<00:00, 1102536.29 examples/s]
Generating train split: 100%|██████████| 120000/120000 [00:00<00:00, 1120927.18 examples/s]
2025-05-01 18:55:36 
Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]
Generating test split: 100%|██████████| 7600/7600 [00:00<00:00, 953108.40 examples/s]
2025-05-01 18:55:38 
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 1114.94 examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 1110.10 examples/s]
2025-05-01 18:55:38 
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]
Map:  20%|█▉        | 195/1000 [00:00<00:00, 1923.86 examples/s]
Map:  40%|███▉      | 398/1000 [00:00<00:00, 1975.54 examples/s]
Map:  64%|██████▎   | 636/1000 [00:00<00:00, 1753.42 examples/s]
Map:  82%|████████▏ | 821/1000 [00:00<00:00, 1784.03 examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 1705.89 examples/s]
2025-05-01 18:55:38 /app/client.py:45: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
2025-05-01 18:55:38   trainer = Trainer(
2025-05-01 18:55:39 WARNING :   DEPRECATED FEATURE: flwr.client.start_numpy_client() is deprecated. 
2025-05-01 18:55:39 Instead, use `flwr.client.start_client()` by ensuring you first call the `.to_client()` method as shown below: 
2025-05-01 18:55:39 flwr.client.start_client(
2025-05-01 18:55:39 server_address='<IP>:<PORT>',
2025-05-01 18:55:39 client=FlowerClient().to_client(), # <-- where FlowerClient is of type flwr.client.NumPyClient object
2025-05-01 18:55:39 )
2025-05-01 18:55:39 Using `start_numpy_client()` is deprecated.
2025-05-01 18:55:39 
2025-05-01 18:55:39             This is a deprecated feature. It will be removed
2025-05-01 18:55:39             entirely in future versions of Flower.
2025-05-01 18:55:39         
2025-05-01 18:55:39 WARNING :   DEPRECATED FEATURE: flwr.client.start_client() is deprecated.
2025-05-01 18:55:39 Instead, use the `flower-supernode` CLI command to start a SuperNode as shown below:
2025-05-01 18:55:39 
2025-05-01 18:55:39 $ flower-supernode --insecure --superlink='<IP>:<PORT>'
2025-05-01 18:55:39 
2025-05-01 18:55:39 To view all available options, run:
2025-05-01 18:55:39 
2025-05-01 18:55:39 $ flower-supernode --help
2025-05-01 18:55:39 
2025-05-01 18:55:39 Using `start_client()` is deprecated.
2025-05-01 18:55:39 
2025-05-01 18:55:39             This is a deprecated feature. It will be removed
2025-05-01 18:55:39             entirely in future versions of Flower.
2025-05-01 18:55:39         
2025-05-01 18:56:01 INFO :      
2025-05-01 18:56:01 INFO :      Received: train message d57217c6-93fd-4e2e-82a0-c60577868787
2025-05-01 18:56:16 {'loss': 2.6476, 'grad_norm': 18.667423248291016, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-01 18:56:26 {'loss': 1.6533, 'grad_norm': 14.597430229187012, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-01 18:56:35 {'loss': 1.6346, 'grad_norm': 19.806570053100586, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-01 18:56:49 {'loss': 1.5147, 'grad_norm': 11.533321380615234, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-01 18:56:59 {'loss': 1.5607, 'grad_norm': 16.239545822143555, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-01 18:57:09 {'loss': 1.4235, 'grad_norm': 14.735560417175293, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-01 18:57:18 {'loss': 1.3801, 'grad_norm': 11.47992992401123, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-01 18:57:28 {'loss': 1.4928, 'grad_norm': 12.755566596984863, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-01 18:57:42 {'loss': 1.2775, 'grad_norm': 13.867303848266602, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-01 18:57:51 {'loss': 1.413, 'grad_norm': 13.377055168151855, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-01 18:58:01 {'loss': 1.4638, 'grad_norm': 11.14098834991455, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-01 18:58:11 {'loss': 1.5317, 'grad_norm': 16.719707489013672, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-01 18:58:20 {'loss': 1.5412, 'grad_norm': 14.699211120605469, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-01 18:58:34 {'loss': 1.34, 'grad_norm': 11.303004264831543, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-01 18:58:43 {'loss': 1.4992, 'grad_norm': 11.791444778442383, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-01 18:58:53 {'loss': 1.4179, 'grad_norm': 12.727545738220215, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-01 18:59:03 {'loss': 1.6515, 'grad_norm': 13.811737060546875, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-01 18:59:13 {'loss': 1.4676, 'grad_norm': 11.954639434814453, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-01 18:59:22 {'loss': 1.4366, 'grad_norm': 14.865253448486328, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-01 18:59:36 {'loss': 1.3679, 'grad_norm': 12.745451927185059, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-01 18:59:46 {'loss': 1.583, 'grad_norm': 13.52482795715332, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-01 18:59:56 {'loss': 1.5815, 'grad_norm': 20.672588348388672, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-01 19:00:05 {'loss': 1.448, 'grad_norm': 16.254541397094727, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-01 19:00:15 {'loss': 1.5469, 'grad_norm': 10.708218574523926, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-01 19:00:29 {'loss': 1.4196, 'grad_norm': 10.960878372192383, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-01 19:00:39 {'loss': 1.3361, 'grad_norm': 11.715882301330566, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-01 19:00:48 {'loss': 1.4753, 'grad_norm': 14.1637544631958, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-01 19:00:58 {'loss': 1.5778, 'grad_norm': 10.979082107543945, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-01 19:01:08 {'loss': 1.4089, 'grad_norm': 11.89306926727295, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-01 19:01:21 {'loss': 1.4044, 'grad_norm': 10.345044136047363, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-01 19:01:31 {'loss': 1.4182, 'grad_norm': 12.881041526794434, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-01 19:01:41 {'loss': 1.381, 'grad_norm': 12.148990631103516, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-01 19:01:51 {'loss': 1.3378, 'grad_norm': 11.297174453735352, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-01 19:02:01 {'loss': 1.4548, 'grad_norm': 10.685917854309082, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-01 19:02:10 {'loss': 1.4016, 'grad_norm': 11.256150245666504, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-01 19:02:24 {'loss': 1.2591, 'grad_norm': 13.242323875427246, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-01 19:02:34 {'loss': 1.5684, 'grad_norm': 12.140288352966309, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-01 19:02:44 {'loss': 1.2984, 'grad_norm': 13.39657974243164, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-01 19:02:53 {'loss': 1.4199, 'grad_norm': 10.896543502807617, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-01 19:03:03 {'loss': 1.4087, 'grad_norm': 12.410995483398438, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-01 19:03:13 {'loss': 1.4482, 'grad_norm': 10.810933113098145, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-01 19:03:27 {'loss': 1.3652, 'grad_norm': 12.634051322937012, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-01 19:03:36 {'loss': 1.2963, 'grad_norm': 12.784635543823242, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-01 19:03:46 {'loss': 1.3526, 'grad_norm': 12.187051773071289, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-01 19:03:56 {'loss': 1.2595, 'grad_norm': 11.523948669433594, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-01 19:04:06 {'loss': 1.3866, 'grad_norm': 11.84868335723877, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-01 19:04:19 {'loss': 1.5414, 'grad_norm': 16.074962615966797, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-01 19:04:29 {'loss': 1.1848, 'grad_norm': 8.580780982971191, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-01 19:04:39 {'loss': 1.4945, 'grad_norm': 13.382322311401367, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-01 19:04:49 {'loss': 1.3534, 'grad_norm': 10.782709121704102, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-01 19:04:49 {'train_runtime': 526.251, 'train_samples_per_second': 1.9, 'train_steps_per_second': 0.95, 'train_loss': 1.462536033630371, 'epoch': 1.0}
2025-05-01 19:06:33 {'loss': 0.8739, 'grad_norm': 11.30894660949707, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-01 19:06:47 {'loss': 1.0113, 'grad_norm': 9.593485832214355, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-01 19:06:57 {'loss': 1.051, 'grad_norm': 13.092449188232422, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-01 19:07:07 {'loss': 1.0439, 'grad_norm': 9.071171760559082, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-01 19:07:17 {'loss': 1.0474, 'grad_norm': 11.812043190002441, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-01 19:07:30 {'loss': 1.0289, 'grad_norm': 12.439757347106934, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-01 19:07:40 {'loss': 0.9452, 'grad_norm': 9.283509254455566, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-01 19:07:50 {'loss': 1.0687, 'grad_norm': 12.011503219604492, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-01 19:08:00 {'loss': 0.8833, 'grad_norm': 10.743570327758789, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-01 19:08:09 {'loss': 0.9881, 'grad_norm': 10.292394638061523, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-01 19:08:23 {'loss': 1.0531, 'grad_norm': 10.581761360168457, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-01 19:08:33 {'loss': 1.0985, 'grad_norm': 10.361001014709473, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-01 19:08:42 {'loss': 1.1379, 'grad_norm': 10.928037643432617, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-01 19:08:52 {'loss': 0.966, 'grad_norm': 10.203673362731934, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-01 19:09:02 {'loss': 1.1106, 'grad_norm': 11.386584281921387, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-01 19:09:15 {'loss': 1.0251, 'grad_norm': 12.09245777130127, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-01 19:09:25 {'loss': 1.2262, 'grad_norm': 12.2351713180542, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-01 19:09:35 {'loss': 1.0906, 'grad_norm': 10.547967910766602, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-01 19:09:44 {'loss': 1.0968, 'grad_norm': 9.298036575317383, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-01 19:09:54 {'loss': 1.0346, 'grad_norm': 10.861795425415039, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-01 19:10:08 {'loss': 1.1896, 'grad_norm': 12.054854393005371, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-01 19:10:17 {'loss': 1.2176, 'grad_norm': 14.12172794342041, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-01 19:10:27 {'loss': 1.1147, 'grad_norm': 14.609519004821777, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-01 19:10:37 {'loss': 1.2034, 'grad_norm': 9.627419471740723, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-01 19:10:47 {'loss': 1.1096, 'grad_norm': 8.864710807800293, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-01 19:10:57 {'loss': 1.0485, 'grad_norm': 10.565980911254883, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-01 19:11:10 {'loss': 1.1882, 'grad_norm': 13.36424732208252, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-01 19:11:20 {'loss': 1.295, 'grad_norm': 10.292628288269043, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-01 19:11:30 {'loss': 1.1101, 'grad_norm': 11.676753997802734, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-01 19:11:39 {'loss': 1.1556, 'grad_norm': 9.864084243774414, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-01 19:11:49 {'loss': 1.1647, 'grad_norm': 12.172998428344727, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-01 19:11:58 {'loss': 1.1573, 'grad_norm': 9.832629203796387, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-01 19:12:12 {'loss': 1.1346, 'grad_norm': 11.136321067810059, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-01 19:12:22 {'loss': 1.2329, 'grad_norm': 11.884288787841797, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-01 19:12:31 {'loss': 1.1909, 'grad_norm': 11.301447868347168, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-01 19:12:41 {'loss': 1.0801, 'grad_norm': 11.23210334777832, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-01 19:12:51 {'loss': 1.3506, 'grad_norm': 14.105032920837402, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-01 19:13:04 {'loss': 1.092, 'grad_norm': 12.359619140625, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-01 19:13:14 {'loss': 1.2541, 'grad_norm': 11.557358741760254, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-01 19:13:24 {'loss': 1.2532, 'grad_norm': 12.775272369384766, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-01 19:13:33 {'loss': 1.311, 'grad_norm': 11.84224796295166, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-01 19:13:43 {'loss': 1.2282, 'grad_norm': 12.477914810180664, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-01 19:13:53 {'loss': 1.1709, 'grad_norm': 11.336602210998535, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-01 19:14:06 {'loss': 1.2002, 'grad_norm': 8.816797256469727, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-01 19:14:16 {'loss': 1.1477, 'grad_norm': 10.560884475708008, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-01 19:14:25 {'loss': 1.3021, 'grad_norm': 11.791546821594238, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-01 19:14:35 {'loss': 1.4354, 'grad_norm': 11.809879302978516, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-01 19:14:44 {'loss': 1.0537, 'grad_norm': 8.311447143554688, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-01 19:14:58 {'loss': 1.1961, 'grad_norm': 11.19583797454834, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-01 19:15:01 {'loss': 0.8148, 'grad_norm': 7.8339996337890625, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-01 19:15:01 {'train_runtime': 519.9748, 'train_samples_per_second': 1.923, 'train_steps_per_second': 0.962, 'train_loss': 1.1236793117523194, 'epoch': 1.0}
2025-05-01 19:05:05 INFO :      Sent reply
2025-05-01 19:06:05 INFO :      
2025-05-01 19:06:05 INFO :      Received: train message 58cc2c89-f2d2-46a4-83db-1cfe997a26ba
2025-05-01 19:15:06 INFO :      Sent reply
2025-05-01 19:15:07 Traceback (most recent call last):
2025-05-01 19:15:07   File "/app/client.py", line 89, in <module>
2025-05-01 19:15:07     fl.client.start_numpy_client(server_address=server_ip, client=FlowerClient())
2025-05-01 19:15:07   File "/usr/local/lib/python3.10/dist-packages/flwr/client/app.py", line 731, in start_numpy_client
2025-05-01 19:15:07     start_client(
2025-05-01 19:15:07   File "/usr/local/lib/python3.10/dist-packages/flwr/client/app.py", line 201, in start_client
2025-05-01 19:15:07     start_client_internal(
2025-05-01 19:15:07   File "/usr/local/lib/python3.10/dist-packages/flwr/client/app.py", line 438, in start_client_internal
2025-05-01 19:15:07     message = receive()
2025-05-01 19:15:07   File "/usr/local/lib/python3.10/dist-packages/flwr/client/grpc_client/connection.py", line 142, in receive
2025-05-01 19:15:07     proto = next(server_message_iterator)
2025-05-01 19:15:07   File "/usr/local/lib/python3.10/dist-packages/grpc/_channel.py", line 543, in __next__
2025-05-01 19:15:07     return self._next()
2025-05-01 19:15:07   File "/usr/local/lib/python3.10/dist-packages/grpc/_channel.py", line 952, in _next
2025-05-01 19:15:07     raise self
2025-05-01 19:15:07 grpc._channel._MultiThreadedRendezvous: <_MultiThreadedRendezvous of RPC that terminated with:
2025-05-01 19:15:07 status = StatusCode.UNAVAILABLE
2025-05-01 19:15:07 details = "Socket closed"
2025-05-01 19:15:07 debug_error_string = "UNKNOWN:Error received from peer ipv4:172.18.0.2:8080 {created_time:"2025-05-01T16:15:01.675807696+00:00", grpc_status:14, grpc_message:"Socket closed"}"
2025-05-01 19:15:07 >
