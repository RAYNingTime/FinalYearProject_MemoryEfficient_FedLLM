2025-05-01 17:57:02 
Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]
Generating train split:  99%|█████████▉| 119000/120000 [00:00<00:00, 1182204.89 examples/s]
Generating train split: 100%|██████████| 120000/120000 [00:00<00:00, 1176564.12 examples/s]
2025-05-01 17:57:02 
Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]
Generating test split: 100%|██████████| 7600/7600 [00:00<00:00, 1068290.17 examples/s]
2025-05-01 17:57:04 
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 1222.56 examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 1216.79 examples/s]
2025-05-01 17:57:05 
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]
Map:  17%|█▋        | 170/1000 [00:00<00:00, 1673.05 examples/s]
Map:  35%|███▍      | 348/1000 [00:00<00:00, 1731.82 examples/s]
Map:  54%|█████▎    | 535/1000 [00:00<00:00, 1788.89 examples/s]
Map:  79%|███████▉  | 791/1000 [00:00<00:00, 1744.96 examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 1576.30 examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 1631.82 examples/s]
2025-05-01 17:57:05 /app/client.py:45: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
2025-05-01 17:57:05   trainer = Trainer(
2025-05-01 17:57:06 WARNING :   DEPRECATED FEATURE: flwr.client.start_numpy_client() is deprecated. 
2025-05-01 17:57:06 Instead, use `flwr.client.start_client()` by ensuring you first call the `.to_client()` method as shown below: 
2025-05-01 17:57:06 flwr.client.start_client(
2025-05-01 17:57:06 server_address='<IP>:<PORT>',
2025-05-01 17:57:06 client=FlowerClient().to_client(), # <-- where FlowerClient is of type flwr.client.NumPyClient object
2025-05-01 17:57:06 )
2025-05-01 17:57:06 Using `start_numpy_client()` is deprecated.
2025-05-01 17:57:06 
2025-05-01 17:57:06             This is a deprecated feature. It will be removed
2025-05-01 17:57:06             entirely in future versions of Flower.
2025-05-01 17:57:06         
2025-05-01 17:57:06 WARNING :   DEPRECATED FEATURE: flwr.client.start_client() is deprecated.
2025-05-01 17:57:06 Instead, use the `flower-supernode` CLI command to start a SuperNode as shown below:
2025-05-01 17:57:06 
2025-05-01 17:57:06 $ flower-supernode --insecure --superlink='<IP>:<PORT>'
2025-05-01 17:57:06 
2025-05-01 17:57:06 To view all available options, run:
2025-05-01 17:57:06 
2025-05-01 17:57:06 $ flower-supernode --help
2025-05-01 17:57:06 
2025-05-01 17:57:06 Using `start_client()` is deprecated.
2025-05-01 17:57:06 
2025-05-01 17:57:06             This is a deprecated feature. It will be removed
2025-05-01 17:57:06             entirely in future versions of Flower.
2025-05-01 17:57:06         
2025-05-01 17:57:06 INFO :      
2025-05-01 17:57:06 INFO :      Received: get_parameters message 85302a1b-606b-4f5a-afe1-594748cc20ff
2025-05-01 17:57:12 INFO :      Sent reply
2025-05-01 17:57:37 INFO :      
2025-05-01 17:57:37 INFO :      Received: train message cf5064c1-bae6-45ce-ac9c-4546d763a7d9
2025-05-01 17:58:10 {'loss': 2.6476, 'grad_norm': 18.667423248291016, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-01 17:58:20 {'loss': 1.6533, 'grad_norm': 14.597430229187012, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-01 17:58:30 {'loss': 1.6346, 'grad_norm': 19.806570053100586, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-01 17:58:40 {'loss': 1.5147, 'grad_norm': 11.533321380615234, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-01 17:58:54 {'loss': 1.5607, 'grad_norm': 16.239545822143555, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-01 17:59:05 {'loss': 1.4235, 'grad_norm': 14.735560417175293, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-01 17:59:15 {'loss': 1.3801, 'grad_norm': 11.47992992401123, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-01 17:59:29 {'loss': 1.4928, 'grad_norm': 12.755566596984863, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-01 17:59:40 {'loss': 1.2775, 'grad_norm': 13.867303848266602, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-01 17:59:54 {'loss': 1.413, 'grad_norm': 13.377055168151855, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-01 18:00:04 {'loss': 1.4638, 'grad_norm': 11.14098834991455, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-01 18:00:14 {'loss': 1.5317, 'grad_norm': 16.719707489013672, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-01 18:00:24 {'loss': 1.5412, 'grad_norm': 14.699211120605469, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-01 18:00:39 {'loss': 1.34, 'grad_norm': 11.303004264831543, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-01 18:00:49 {'loss': 1.4992, 'grad_norm': 11.791444778442383, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-01 18:00:59 {'loss': 1.4179, 'grad_norm': 12.727545738220215, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-01 18:01:09 {'loss': 1.6515, 'grad_norm': 13.811737060546875, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-01 18:01:19 {'loss': 1.4676, 'grad_norm': 11.954639434814453, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-01 18:01:32 {'loss': 1.4366, 'grad_norm': 14.865253448486328, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-01 18:01:42 {'loss': 1.3679, 'grad_norm': 12.745451927185059, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-01 18:01:52 {'loss': 1.583, 'grad_norm': 13.52482795715332, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-01 18:02:02 {'loss': 1.5815, 'grad_norm': 20.672588348388672, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-01 18:02:16 {'loss': 1.448, 'grad_norm': 16.254541397094727, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-01 18:02:26 {'loss': 1.5469, 'grad_norm': 10.708218574523926, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-01 18:02:36 {'loss': 1.4196, 'grad_norm': 10.960878372192383, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-01 18:02:46 {'loss': 1.3361, 'grad_norm': 11.715882301330566, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-01 18:03:00 {'loss': 1.4753, 'grad_norm': 14.1637544631958, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-01 18:03:10 {'loss': 1.5778, 'grad_norm': 10.979082107543945, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-01 18:03:20 {'loss': 1.4089, 'grad_norm': 11.89306926727295, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-01 18:03:30 {'loss': 1.4044, 'grad_norm': 10.345044136047363, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-01 18:03:44 {'loss': 1.4182, 'grad_norm': 12.881041526794434, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-01 18:03:54 {'loss': 1.381, 'grad_norm': 12.148990631103516, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-01 18:04:04 {'loss': 1.3378, 'grad_norm': 11.297174453735352, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-01 18:04:14 {'loss': 1.4548, 'grad_norm': 10.685917854309082, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-01 18:04:24 {'loss': 1.4016, 'grad_norm': 11.256150245666504, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-01 18:04:37 {'loss': 1.2591, 'grad_norm': 13.242323875427246, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-01 18:04:47 {'loss': 1.5684, 'grad_norm': 12.140288352966309, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-01 18:04:57 {'loss': 1.2984, 'grad_norm': 13.39657974243164, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-01 18:05:07 {'loss': 1.4199, 'grad_norm': 10.896543502807617, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-01 18:05:17 {'loss': 1.4087, 'grad_norm': 12.410995483398438, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-01 18:05:31 {'loss': 1.4482, 'grad_norm': 10.810933113098145, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-01 18:05:42 {'loss': 1.3652, 'grad_norm': 12.634051322937012, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-01 18:05:52 {'loss': 1.2963, 'grad_norm': 12.784635543823242, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-01 18:06:01 {'loss': 1.3526, 'grad_norm': 12.187051773071289, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-01 18:06:15 {'loss': 1.2595, 'grad_norm': 11.523948669433594, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-01 18:06:25 {'loss': 1.3866, 'grad_norm': 11.84868335723877, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-01 18:06:35 {'loss': 1.5414, 'grad_norm': 16.074962615966797, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-01 18:06:45 {'loss': 1.1848, 'grad_norm': 8.580780982971191, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-01 18:06:55 {'loss': 1.4945, 'grad_norm': 13.382322311401367, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-01 18:06:57 {'loss': 1.3534, 'grad_norm': 10.782709121704102, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-01 18:06:57 {'train_runtime': 558.6907, 'train_samples_per_second': 1.79, 'train_steps_per_second': 0.895, 'train_loss': 1.462536033630371, 'epoch': 1.0}
