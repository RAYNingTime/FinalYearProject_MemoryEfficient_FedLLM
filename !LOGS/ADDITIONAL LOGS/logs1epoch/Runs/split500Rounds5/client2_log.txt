2025-05-13 13:13:45 
Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]
Generating train split: 100%|██████████| 120000/120000 [00:00<00:00, 1224670.92 examples/s]
2025-05-13 13:13:45 
Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]
Generating test split: 100%|██████████| 7600/7600 [00:00<00:00, 1013535.67 examples/s]
2025-05-13 13:13:47 
Map:   0%|          | 0/500 [00:00<?, ? examples/s]
Map: 100%|██████████| 500/500 [00:00<00:00, 942.20 examples/s]
Map: 100%|██████████| 500/500 [00:00<00:00, 936.61 examples/s]
2025-05-13 13:13:47 
Map:   0%|          | 0/500 [00:00<?, ? examples/s]
Map:  41%|████▏     | 207/500 [00:00<00:00, 2032.99 examples/s]
Map:  85%|████████▍ | 424/500 [00:00<00:00, 2105.15 examples/s]
Map: 100%|██████████| 500/500 [00:00<00:00, 1923.22 examples/s]
2025-05-13 13:13:48 /app/client.py:45: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
2025-05-13 13:13:48   trainer = Trainer(
2025-05-13 13:13:48 WARNING :   DEPRECATED FEATURE: flwr.client.start_numpy_client() is deprecated. 
2025-05-13 13:13:48 Instead, use `flwr.client.start_client()` by ensuring you first call the `.to_client()` method as shown below: 
2025-05-13 13:13:48 flwr.client.start_client(
2025-05-13 13:13:48 server_address='<IP>:<PORT>',
2025-05-13 13:13:48 client=FlowerClient().to_client(), # <-- where FlowerClient is of type flwr.client.NumPyClient object
2025-05-13 13:13:48 )
2025-05-13 13:13:48 Using `start_numpy_client()` is deprecated.
2025-05-13 13:13:48 
2025-05-13 13:13:48             This is a deprecated feature. It will be removed
2025-05-13 13:13:48             entirely in future versions of Flower.
2025-05-13 13:13:48         
2025-05-13 13:13:48 WARNING :   DEPRECATED FEATURE: flwr.client.start_client() is deprecated.
2025-05-13 13:13:48 Instead, use the `flower-supernode` CLI command to start a SuperNode as shown below:
2025-05-13 13:13:48 
2025-05-13 13:13:48 $ flower-supernode --insecure --superlink='<IP>:<PORT>'
2025-05-13 13:13:48 
2025-05-13 13:13:48 To view all available options, run:
2025-05-13 13:13:48 
2025-05-13 13:13:48 $ flower-supernode --help
2025-05-13 13:13:48 
2025-05-13 13:13:48 Using `start_client()` is deprecated.
2025-05-13 13:13:48 
2025-05-13 13:13:48             This is a deprecated feature. It will be removed
2025-05-13 13:13:48             entirely in future versions of Flower.
2025-05-13 13:13:48         
2025-05-13 13:14:00 INFO :      
2025-05-13 13:14:00 INFO :      Received: train message 8b2548f5-97b7-4211-942c-219dcc61a129
2025-05-13 13:18:33 INFO :      Sent reply
2025-05-13 13:18:47 INFO :      
2025-05-13 13:18:47 INFO :      Received: train message 7276b8fe-b6fa-4bbc-940f-8042f11c4505
2025-05-13 13:14:16 {'loss': 2.7751, 'grad_norm': 18.519489288330078, 'learning_rate': 4.82e-05, 'epoch': 0.04}
2025-05-13 13:14:26 {'loss': 1.6029, 'grad_norm': 14.751036643981934, 'learning_rate': 4.6200000000000005e-05, 'epoch': 0.08}
2025-05-13 13:14:35 {'loss': 1.5794, 'grad_norm': 14.19046688079834, 'learning_rate': 4.4200000000000004e-05, 'epoch': 0.12}
2025-05-13 13:14:45 {'loss': 1.5974, 'grad_norm': 12.10661792755127, 'learning_rate': 4.22e-05, 'epoch': 0.16}
2025-05-13 13:14:55 {'loss': 1.5889, 'grad_norm': 13.332625389099121, 'learning_rate': 4.02e-05, 'epoch': 0.2}
2025-05-13 13:15:08 {'loss': 1.5412, 'grad_norm': 10.970870971679688, 'learning_rate': 3.82e-05, 'epoch': 0.24}
2025-05-13 13:15:18 {'loss': 1.5192, 'grad_norm': 13.312793731689453, 'learning_rate': 3.62e-05, 'epoch': 0.28}
2025-05-13 13:15:27 {'loss': 1.3967, 'grad_norm': 12.499846458435059, 'learning_rate': 3.4200000000000005e-05, 'epoch': 0.32}
2025-05-13 13:15:37 {'loss': 1.4871, 'grad_norm': 12.355472564697266, 'learning_rate': 3.2200000000000003e-05, 'epoch': 0.36}
2025-05-13 13:15:47 {'loss': 1.4265, 'grad_norm': 11.46533203125, 'learning_rate': 3.02e-05, 'epoch': 0.4}
2025-05-13 13:15:56 {'loss': 1.6482, 'grad_norm': 14.148241996765137, 'learning_rate': 2.8199999999999998e-05, 'epoch': 0.44}
2025-05-13 13:16:10 {'loss': 1.426, 'grad_norm': 13.335575103759766, 'learning_rate': 2.6200000000000003e-05, 'epoch': 0.48}
2025-05-13 13:16:20 {'loss': 1.3205, 'grad_norm': 16.550350189208984, 'learning_rate': 2.4200000000000002e-05, 'epoch': 0.52}
2025-05-13 13:16:29 {'loss': 1.4765, 'grad_norm': 16.085912704467773, 'learning_rate': 2.22e-05, 'epoch': 0.56}
2025-05-13 13:16:39 {'loss': 1.3923, 'grad_norm': 13.222654342651367, 'learning_rate': 2.0200000000000003e-05, 'epoch': 0.6}
2025-05-13 13:16:49 {'loss': 1.3074, 'grad_norm': 11.937178611755371, 'learning_rate': 1.8200000000000002e-05, 'epoch': 0.64}
2025-05-13 13:16:58 {'loss': 1.5856, 'grad_norm': 15.577046394348145, 'learning_rate': 1.62e-05, 'epoch': 0.68}
2025-05-13 13:17:12 {'loss': 1.4143, 'grad_norm': 14.039474487304688, 'learning_rate': 1.42e-05, 'epoch': 0.72}
2025-05-13 13:17:22 {'loss': 1.3453, 'grad_norm': 11.653090476989746, 'learning_rate': 1.22e-05, 'epoch': 0.76}
2025-05-13 13:17:32 {'loss': 1.4974, 'grad_norm': 13.129425048828125, 'learning_rate': 1.02e-05, 'epoch': 0.8}
2025-05-13 13:17:46 {'loss': 1.4242, 'grad_norm': 14.055017471313477, 'learning_rate': 8.200000000000001e-06, 'epoch': 0.84}
2025-05-13 13:17:56 {'loss': 1.4881, 'grad_norm': 12.582931518554688, 'learning_rate': 6.2e-06, 'epoch': 0.88}
2025-05-13 13:18:10 {'loss': 1.3035, 'grad_norm': 11.899556159973145, 'learning_rate': 4.2000000000000004e-06, 'epoch': 0.92}
2025-05-13 13:18:20 {'loss': 1.3543, 'grad_norm': 13.913609504699707, 'learning_rate': 2.2e-06, 'epoch': 0.96}
2025-05-13 13:18:29 {'loss': 1.519, 'grad_norm': 10.157051086425781, 'learning_rate': 2.0000000000000002e-07, 'epoch': 1.0}
2025-05-13 13:18:29 {'train_runtime': 267.9929, 'train_samples_per_second': 1.866, 'train_steps_per_second': 0.933, 'train_loss': 1.5206789207458495, 'epoch': 1.0}
2025-05-13 13:19:01 {'loss': 0.927, 'grad_norm': 14.398917198181152, 'learning_rate': 4.82e-05, 'epoch': 0.04}
2025-05-13 13:19:11 {'loss': 1.0103, 'grad_norm': 10.946357727050781, 'learning_rate': 4.6200000000000005e-05, 'epoch': 0.08}
2025-05-13 13:19:25 {'loss': 1.0826, 'grad_norm': 10.463777542114258, 'learning_rate': 4.4200000000000004e-05, 'epoch': 0.12}
2025-05-13 13:19:35 {'loss': 1.016, 'grad_norm': 9.921194076538086, 'learning_rate': 4.22e-05, 'epoch': 0.16}
2025-05-13 13:19:48 {'loss': 1.0946, 'grad_norm': 10.77833080291748, 'learning_rate': 4.02e-05, 'epoch': 0.2}
2025-05-13 13:19:59 {'loss': 1.0775, 'grad_norm': 8.912487983703613, 'learning_rate': 3.82e-05, 'epoch': 0.24}
2025-05-13 13:20:08 {'loss': 1.0681, 'grad_norm': 10.213648796081543, 'learning_rate': 3.62e-05, 'epoch': 0.28}
2025-05-13 13:20:18 {'loss': 0.9769, 'grad_norm': 11.028637886047363, 'learning_rate': 3.4200000000000005e-05, 'epoch': 0.32}
2025-05-13 13:20:31 {'loss': 1.0888, 'grad_norm': 12.657425880432129, 'learning_rate': 3.2200000000000003e-05, 'epoch': 0.36}
2025-05-13 13:20:41 {'loss': 1.0599, 'grad_norm': 10.635690689086914, 'learning_rate': 3.02e-05, 'epoch': 0.4}
2025-05-13 13:20:50 {'loss': 1.245, 'grad_norm': 12.613212585449219, 'learning_rate': 2.8199999999999998e-05, 'epoch': 0.44}
2025-05-13 13:21:00 {'loss': 1.0757, 'grad_norm': 9.699553489685059, 'learning_rate': 2.6200000000000003e-05, 'epoch': 0.48}
2025-05-13 13:21:09 {'loss': 1.0275, 'grad_norm': 13.292228698730469, 'learning_rate': 2.4200000000000002e-05, 'epoch': 0.52}
2025-05-13 13:21:19 {'loss': 1.1421, 'grad_norm': 11.359539031982422, 'learning_rate': 2.22e-05, 'epoch': 0.56}
2025-05-13 13:21:32 {'loss': 1.1209, 'grad_norm': 12.35815715789795, 'learning_rate': 2.0200000000000003e-05, 'epoch': 0.6}
2025-05-13 13:21:41 {'loss': 1.0469, 'grad_norm': 14.3265962600708, 'learning_rate': 1.8200000000000002e-05, 'epoch': 0.64}
2025-05-13 13:21:51 {'loss': 1.3107, 'grad_norm': 14.92988109588623, 'learning_rate': 1.62e-05, 'epoch': 0.68}
2025-05-13 13:22:00 {'loss': 1.1942, 'grad_norm': 14.031665802001953, 'learning_rate': 1.42e-05, 'epoch': 0.72}
2025-05-13 13:22:10 {'loss': 1.1448, 'grad_norm': 10.113167762756348, 'learning_rate': 1.22e-05, 'epoch': 0.76}
2025-05-13 13:22:20 {'loss': 1.3005, 'grad_norm': 12.26799201965332, 'learning_rate': 1.02e-05, 'epoch': 0.8}
2025-05-13 13:22:29 {'loss': 1.2649, 'grad_norm': 11.295190811157227, 'learning_rate': 8.200000000000001e-06, 'epoch': 0.84}
2025-05-13 13:22:43 {'loss': 1.3616, 'grad_norm': 14.211376190185547, 'learning_rate': 6.2e-06, 'epoch': 0.88}
2025-05-13 13:22:52 {'loss': 1.121, 'grad_norm': 10.666187286376953, 'learning_rate': 4.2000000000000004e-06, 'epoch': 0.92}
2025-05-13 13:23:02 {'loss': 1.0592, 'grad_norm': 12.649394989013672, 'learning_rate': 2.2e-06, 'epoch': 0.96}
2025-05-13 13:23:11 {'loss': 0.927, 'grad_norm': 6.515467643737793, 'learning_rate': 2.0000000000000002e-07, 'epoch': 1.0}
2025-05-13 13:23:11 {'train_runtime': 262.9574, 'train_samples_per_second': 1.901, 'train_steps_per_second': 0.951, 'train_loss': 1.109750831604004, 'epoch': 1.0}
2025-05-13 13:23:44 {'loss': 0.5564, 'grad_norm': 12.045516967773438, 'learning_rate': 4.82e-05, 'epoch': 0.04}
2025-05-13 13:23:54 {'loss': 0.6712, 'grad_norm': 10.750245094299316, 'learning_rate': 4.6200000000000005e-05, 'epoch': 0.08}
2025-05-13 13:24:03 {'loss': 0.7404, 'grad_norm': 9.6331205368042, 'learning_rate': 4.4200000000000004e-05, 'epoch': 0.12}
2025-05-13 13:24:13 {'loss': 0.7099, 'grad_norm': 10.865588188171387, 'learning_rate': 4.22e-05, 'epoch': 0.16}
2025-05-13 13:24:26 {'loss': 0.7777, 'grad_norm': 10.009559631347656, 'learning_rate': 4.02e-05, 'epoch': 0.2}
2025-05-13 13:24:36 {'loss': 0.8018, 'grad_norm': 7.867875576019287, 'learning_rate': 3.82e-05, 'epoch': 0.24}
2025-05-13 13:24:46 {'loss': 0.8017, 'grad_norm': 11.011390686035156, 'learning_rate': 3.62e-05, 'epoch': 0.28}
2025-05-13 13:24:55 {'loss': 0.7309, 'grad_norm': 9.100639343261719, 'learning_rate': 3.4200000000000005e-05, 'epoch': 0.32}
2025-05-13 13:25:05 {'loss': 0.8415, 'grad_norm': 11.949930191040039, 'learning_rate': 3.2200000000000003e-05, 'epoch': 0.36}
2025-05-13 13:25:14 {'loss': 0.8333, 'grad_norm': 9.531172752380371, 'learning_rate': 3.02e-05, 'epoch': 0.4}
2025-05-13 13:25:28 {'loss': 0.9751, 'grad_norm': 12.308521270751953, 'learning_rate': 2.8199999999999998e-05, 'epoch': 0.44}
2025-05-13 13:25:37 {'loss': 0.8443, 'grad_norm': 12.42241382598877, 'learning_rate': 2.6200000000000003e-05, 'epoch': 0.48}
2025-05-13 13:25:47 {'loss': 0.8207, 'grad_norm': 9.650876998901367, 'learning_rate': 2.4200000000000002e-05, 'epoch': 0.52}
2025-05-13 13:25:56 {'loss': 0.9546, 'grad_norm': 10.771954536437988, 'learning_rate': 2.22e-05, 'epoch': 0.56}
2025-05-13 13:26:06 {'loss': 0.9391, 'grad_norm': 11.402926445007324, 'learning_rate': 2.0200000000000003e-05, 'epoch': 0.6}
2025-05-13 13:26:16 {'loss': 0.8908, 'grad_norm': 11.00220012664795, 'learning_rate': 1.8200000000000002e-05, 'epoch': 0.64}
2025-05-13 13:26:29 {'loss': 1.1173, 'grad_norm': 12.502324104309082, 'learning_rate': 1.62e-05, 'epoch': 0.68}
2025-05-13 13:26:38 {'loss': 1.0492, 'grad_norm': 13.56574535369873, 'learning_rate': 1.42e-05, 'epoch': 0.72}
2025-05-13 13:26:48 {'loss': 1.0242, 'grad_norm': 11.613231658935547, 'learning_rate': 1.22e-05, 'epoch': 0.76}
2025-05-13 13:26:58 {'loss': 1.1895, 'grad_norm': 12.482230186462402, 'learning_rate': 1.02e-05, 'epoch': 0.8}
2025-05-13 13:27:07 {'loss': 1.183, 'grad_norm': 11.444426536560059, 'learning_rate': 8.200000000000001e-06, 'epoch': 0.84}
2025-05-13 13:27:17 {'loss': 1.2824, 'grad_norm': 13.326251983642578, 'learning_rate': 6.2e-06, 'epoch': 0.88}
2025-05-13 13:27:26 {'loss': 1.0248, 'grad_norm': 10.344125747680664, 'learning_rate': 4.2000000000000004e-06, 'epoch': 0.92}
2025-05-13 13:27:40 {'loss': 0.9414, 'grad_norm': 11.998185157775879, 'learning_rate': 2.2e-06, 'epoch': 0.96}
2025-05-13 13:27:49 {'loss': 0.6997, 'grad_norm': 6.047473907470703, 'learning_rate': 2.0000000000000002e-07, 'epoch': 1.0}
2025-05-13 13:27:49 {'train_runtime': 258.1399, 'train_samples_per_second': 1.937, 'train_steps_per_second': 0.968, 'train_loss': 0.8960392169952393, 'epoch': 1.0}
2025-05-13 13:28:24 {'loss': 0.3411, 'grad_norm': 10.702852249145508, 'learning_rate': 4.82e-05, 'epoch': 0.04}
2025-05-13 13:28:37 {'loss': 0.4554, 'grad_norm': 11.465157508850098, 'learning_rate': 4.6200000000000005e-05, 'epoch': 0.08}
2025-05-13 13:28:47 {'loss': 0.5426, 'grad_norm': 8.220775604248047, 'learning_rate': 4.4200000000000004e-05, 'epoch': 0.12}
2025-05-13 13:28:56 {'loss': 0.5174, 'grad_norm': 13.977864265441895, 'learning_rate': 4.22e-05, 'epoch': 0.16}
2025-05-13 13:29:06 {'loss': 0.5362, 'grad_norm': 8.938958168029785, 'learning_rate': 4.02e-05, 'epoch': 0.2}
2025-05-13 13:29:15 {'loss': 0.6039, 'grad_norm': 10.085184097290039, 'learning_rate': 3.82e-05, 'epoch': 0.24}
2025-05-13 13:29:25 {'loss': 0.602, 'grad_norm': 10.956184387207031, 'learning_rate': 3.62e-05, 'epoch': 0.28}
2025-05-13 13:29:35 {'loss': 0.5562, 'grad_norm': 9.516735076904297, 'learning_rate': 3.4200000000000005e-05, 'epoch': 0.32}
2025-05-13 13:29:48 {'loss': 0.6514, 'grad_norm': 10.44924545288086, 'learning_rate': 3.2200000000000003e-05, 'epoch': 0.36}
2025-05-13 13:29:58 {'loss': 0.6416, 'grad_norm': 8.998805046081543, 'learning_rate': 3.02e-05, 'epoch': 0.4}
2025-05-13 13:30:07 {'loss': 0.7798, 'grad_norm': 12.369564056396484, 'learning_rate': 2.8199999999999998e-05, 'epoch': 0.44}
2025-05-13 13:30:17 {'loss': 0.694, 'grad_norm': 10.320508003234863, 'learning_rate': 2.6200000000000003e-05, 'epoch': 0.48}
2025-05-13 13:30:27 {'loss': 0.6569, 'grad_norm': 10.351908683776855, 'learning_rate': 2.4200000000000002e-05, 'epoch': 0.52}
2025-05-13 13:30:36 {'loss': 0.7761, 'grad_norm': 10.485098838806152, 'learning_rate': 2.22e-05, 'epoch': 0.56}
2025-05-13 13:30:50 {'loss': 0.8191, 'grad_norm': 11.104731559753418, 'learning_rate': 2.0200000000000003e-05, 'epoch': 0.6}
2025-05-13 13:30:59 {'loss': 0.7405, 'grad_norm': 10.578235626220703, 'learning_rate': 1.8200000000000002e-05, 'epoch': 0.64}
2025-05-13 13:31:09 {'loss': 0.9573, 'grad_norm': 13.046738624572754, 'learning_rate': 1.62e-05, 'epoch': 0.68}
2025-05-13 13:31:19 {'loss': 0.9235, 'grad_norm': 12.371805191040039, 'learning_rate': 1.42e-05, 'epoch': 0.72}
2025-05-13 13:31:28 {'loss': 0.9109, 'grad_norm': 10.197039604187012, 'learning_rate': 1.22e-05, 'epoch': 0.76}
2025-05-13 13:31:38 {'loss': 1.1089, 'grad_norm': 13.661616325378418, 'learning_rate': 1.02e-05, 'epoch': 0.8}
2025-05-13 13:31:51 {'loss': 1.121, 'grad_norm': 13.17540168762207, 'learning_rate': 8.200000000000001e-06, 'epoch': 0.84}
2025-05-13 13:32:01 {'loss': 1.2272, 'grad_norm': 13.352509498596191, 'learning_rate': 6.2e-06, 'epoch': 0.88}
2025-05-13 13:32:11 {'loss': 0.9737, 'grad_norm': 10.9821195602417, 'learning_rate': 4.2000000000000004e-06, 'epoch': 0.92}
2025-05-13 13:32:20 {'loss': 0.8392, 'grad_norm': 13.71068000793457, 'learning_rate': 2.2e-06, 'epoch': 0.96}
2025-05-13 13:32:30 {'loss': 0.5639, 'grad_norm': 6.582391262054443, 'learning_rate': 2.0000000000000002e-07, 'epoch': 1.0}
2025-05-13 13:32:30 {'train_runtime': 256.7584, 'train_samples_per_second': 1.947, 'train_steps_per_second': 0.974, 'train_loss': 0.7415905151367187, 'epoch': 1.0}
2025-05-13 13:33:01 {'loss': 0.2219, 'grad_norm': 7.936019420623779, 'learning_rate': 4.82e-05, 'epoch': 0.04}
2025-05-13 13:33:11 {'loss': 0.303, 'grad_norm': 10.758126258850098, 'learning_rate': 4.6200000000000005e-05, 'epoch': 0.08}
2025-05-13 13:33:21 {'loss': 0.3668, 'grad_norm': 7.805701732635498, 'learning_rate': 4.4200000000000004e-05, 'epoch': 0.12}
2025-05-13 13:33:30 {'loss': 0.3518, 'grad_norm': 9.02733039855957, 'learning_rate': 4.22e-05, 'epoch': 0.16}
2025-05-13 13:33:43 {'loss': 0.3687, 'grad_norm': 8.659722328186035, 'learning_rate': 4.02e-05, 'epoch': 0.2}
2025-05-13 13:33:53 {'loss': 0.4536, 'grad_norm': 7.3897624015808105, 'learning_rate': 3.82e-05, 'epoch': 0.24}
2025-05-13 13:34:03 {'loss': 0.4222, 'grad_norm': 9.347444534301758, 'learning_rate': 3.62e-05, 'epoch': 0.28}
2025-05-13 13:34:12 {'loss': 0.4319, 'grad_norm': 10.563488960266113, 'learning_rate': 3.4200000000000005e-05, 'epoch': 0.32}
2025-05-13 13:34:22 {'loss': 0.4764, 'grad_norm': 9.562846183776855, 'learning_rate': 3.2200000000000003e-05, 'epoch': 0.36}
2025-05-13 13:34:31 {'loss': 0.4903, 'grad_norm': 10.109074592590332, 'learning_rate': 3.02e-05, 'epoch': 0.4}
2025-05-13 13:34:45 {'loss': 0.6038, 'grad_norm': 10.526625633239746, 'learning_rate': 2.8199999999999998e-05, 'epoch': 0.44}
2025-05-13 13:34:54 {'loss': 0.5342, 'grad_norm': 9.286479949951172, 'learning_rate': 2.6200000000000003e-05, 'epoch': 0.48}
2025-05-13 13:35:04 {'loss': 0.5457, 'grad_norm': 10.376466751098633, 'learning_rate': 2.4200000000000002e-05, 'epoch': 0.52}
2025-05-13 13:35:13 {'loss': 0.6373, 'grad_norm': 10.29773235321045, 'learning_rate': 2.22e-05, 'epoch': 0.56}
2025-05-13 13:35:23 {'loss': 0.6941, 'grad_norm': 10.113097190856934, 'learning_rate': 2.0200000000000003e-05, 'epoch': 0.6}
2025-05-13 13:35:33 {'loss': 0.6167, 'grad_norm': 10.643391609191895, 'learning_rate': 1.8200000000000002e-05, 'epoch': 0.64}
2025-05-13 13:35:46 {'loss': 0.8299, 'grad_norm': 11.606522560119629, 'learning_rate': 1.62e-05, 'epoch': 0.68}
2025-05-13 13:35:56 {'loss': 0.8126, 'grad_norm': 12.444034576416016, 'learning_rate': 1.42e-05, 'epoch': 0.72}
2025-05-13 13:36:05 {'loss': 0.8221, 'grad_norm': 10.11982536315918, 'learning_rate': 1.22e-05, 'epoch': 0.76}
2025-05-13 13:36:15 {'loss': 1.0273, 'grad_norm': 14.13768482208252, 'learning_rate': 1.02e-05, 'epoch': 0.8}
2025-05-13 13:36:24 {'loss': 1.0556, 'grad_norm': 14.44654369354248, 'learning_rate': 8.200000000000001e-06, 'epoch': 0.84}
2025-05-13 13:36:34 {'loss': 1.1599, 'grad_norm': 13.447833061218262, 'learning_rate': 6.2e-06, 'epoch': 0.88}
2025-05-13 13:36:47 {'loss': 0.9021, 'grad_norm': 12.90402603149414, 'learning_rate': 4.2000000000000004e-06, 'epoch': 0.92}
2025-05-13 13:36:57 {'loss': 0.7535, 'grad_norm': 12.020132064819336, 'learning_rate': 2.2e-06, 'epoch': 0.96}
2025-05-13 13:37:06 {'loss': 0.4359, 'grad_norm': 4.682199954986572, 'learning_rate': 2.0000000000000002e-07, 'epoch': 1.0}
2025-05-13 13:37:06 {'train_runtime': 257.2887, 'train_samples_per_second': 1.943, 'train_steps_per_second': 0.972, 'train_loss': 0.6126982278823853, 'epoch': 1.0}
2025-05-13 13:23:18 INFO :      Sent reply
2025-05-13 13:23:31 INFO :      
2025-05-13 13:23:31 INFO :      Received: train message 78eaed91-a7f9-4599-99e2-175a572b2fd8
2025-05-13 13:27:55 INFO :      Sent reply
2025-05-13 13:28:10 INFO :      
2025-05-13 13:28:10 INFO :      Received: train message bb8c9cde-23c4-44a0-8996-0d7d374d3e1b
2025-05-13 13:32:35 INFO :      Sent reply
2025-05-13 13:32:48 INFO :      
2025-05-13 13:32:48 INFO :      Received: train message a3c7dab9-956e-44ab-9738-d0b2d2ea66b4
2025-05-13 13:37:13 INFO :      Sent reply
2025-05-13 13:37:19 INFO :      
2025-05-13 13:37:19 INFO :      Received: reconnect message 0732262a-492c-4b1f-baf4-f49e8303dfbe
2025-05-13 13:37:20 INFO :      Disconnect and shut down
