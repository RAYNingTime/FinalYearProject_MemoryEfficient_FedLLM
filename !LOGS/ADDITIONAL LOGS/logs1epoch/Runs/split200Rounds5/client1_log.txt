2025-05-13 13:13:45 
Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]
Generating train split:  87%|████████▋ | 104000/120000 [00:00<00:00, 1024805.92 examples/s]
Generating train split: 100%|██████████| 120000/120000 [00:00<00:00, 1066434.33 examples/s]
2025-05-13 13:13:45 
Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]
Generating test split: 100%|██████████| 7600/7600 [00:00<00:00, 876866.02 examples/s]
2025-05-13 13:13:46 
Map:   0%|          | 0/500 [00:00<?, ? examples/s]
Map: 100%|██████████| 500/500 [00:00<00:00, 1086.25 examples/s]
Map: 100%|██████████| 500/500 [00:00<00:00, 1079.82 examples/s]
2025-05-13 13:13:47 
Map:   0%|          | 0/500 [00:00<?, ? examples/s]
Map:  32%|███▏      | 162/500 [00:00<00:00, 1591.18 examples/s]
Map:  65%|██████▌   | 325/500 [00:00<00:00, 1611.79 examples/s]
Map:  98%|█████████▊| 488/500 [00:00<00:00, 1616.48 examples/s]
Map: 100%|██████████| 500/500 [00:00<00:00, 1507.06 examples/s]
2025-05-13 13:13:47 /app/client.py:45: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
2025-05-13 13:13:47   trainer = Trainer(
2025-05-13 13:13:47 WARNING :   DEPRECATED FEATURE: flwr.client.start_numpy_client() is deprecated. 
2025-05-13 13:13:47 Instead, use `flwr.client.start_client()` by ensuring you first call the `.to_client()` method as shown below: 
2025-05-13 13:13:47 flwr.client.start_client(
2025-05-13 13:13:47 server_address='<IP>:<PORT>',
2025-05-13 13:13:47 client=FlowerClient().to_client(), # <-- where FlowerClient is of type flwr.client.NumPyClient object
2025-05-13 13:13:47 )
2025-05-13 13:13:47 Using `start_numpy_client()` is deprecated.
2025-05-13 13:13:47 
2025-05-13 13:13:47             This is a deprecated feature. It will be removed
2025-05-13 13:13:47             entirely in future versions of Flower.
2025-05-13 13:13:47         
2025-05-13 13:13:47 WARNING :   DEPRECATED FEATURE: flwr.client.start_client() is deprecated.
2025-05-13 13:13:47 Instead, use the `flower-supernode` CLI command to start a SuperNode as shown below:
2025-05-13 13:13:47 
2025-05-13 13:13:47 $ flower-supernode --insecure --superlink='<IP>:<PORT>'
2025-05-13 13:13:47 
2025-05-13 13:13:47 To view all available options, run:
2025-05-13 13:13:47 
2025-05-13 13:13:47 $ flower-supernode --help
2025-05-13 13:13:47 
2025-05-13 13:13:47 Using `start_client()` is deprecated.
2025-05-13 13:13:47 
2025-05-13 13:13:47             This is a deprecated feature. It will be removed
2025-05-13 13:13:47             entirely in future versions of Flower.
2025-05-13 13:13:47         
2025-05-13 13:13:48 INFO :      
2025-05-13 13:13:48 INFO :      Received: get_parameters message 46377a65-583d-4d96-824b-64ef23cd72e3
2025-05-13 13:13:51 INFO :      Sent reply
2025-05-13 13:14:00 INFO :      
2025-05-13 13:14:00 INFO :      Received: train message 4586683f-f896-49f5-9d1f-c43f52a40656
2025-05-13 13:18:31 INFO :      Sent reply
2025-05-13 13:18:47 INFO :      
2025-05-13 13:18:47 INFO :      Received: train message 75ee0493-447b-48e6-97ab-6ebfd64febaa
2025-05-13 13:14:14 {'loss': 2.6879, 'grad_norm': 12.462568283081055, 'learning_rate': 4.82e-05, 'epoch': 0.04}
2025-05-13 13:14:23 {'loss': 1.5461, 'grad_norm': 13.631051063537598, 'learning_rate': 4.6200000000000005e-05, 'epoch': 0.08}
2025-05-13 13:14:33 {'loss': 1.4952, 'grad_norm': 15.123758316040039, 'learning_rate': 4.4200000000000004e-05, 'epoch': 0.12}
2025-05-13 13:14:47 {'loss': 1.6495, 'grad_norm': 20.75948715209961, 'learning_rate': 4.22e-05, 'epoch': 0.16}
2025-05-13 13:14:56 {'loss': 1.5671, 'grad_norm': 16.530641555786133, 'learning_rate': 4.02e-05, 'epoch': 0.2}
2025-05-13 13:15:06 {'loss': 1.5212, 'grad_norm': 17.822240829467773, 'learning_rate': 3.82e-05, 'epoch': 0.24}
2025-05-13 13:15:16 {'loss': 1.5563, 'grad_norm': 15.01745891571045, 'learning_rate': 3.62e-05, 'epoch': 0.28}
2025-05-13 13:15:25 {'loss': 1.5, 'grad_norm': 11.507730484008789, 'learning_rate': 3.4200000000000005e-05, 'epoch': 0.32}
2025-05-13 13:15:35 {'loss': 1.4285, 'grad_norm': 13.050183296203613, 'learning_rate': 3.2200000000000003e-05, 'epoch': 0.36}
2025-05-13 13:15:48 {'loss': 1.4194, 'grad_norm': 11.131627082824707, 'learning_rate': 3.02e-05, 'epoch': 0.4}
2025-05-13 13:15:58 {'loss': 1.4634, 'grad_norm': 13.683835983276367, 'learning_rate': 2.8199999999999998e-05, 'epoch': 0.44}
2025-05-13 13:16:08 {'loss': 1.5254, 'grad_norm': 13.856196403503418, 'learning_rate': 2.6200000000000003e-05, 'epoch': 0.48}
2025-05-13 13:16:17 {'loss': 1.3643, 'grad_norm': 15.318628311157227, 'learning_rate': 2.4200000000000002e-05, 'epoch': 0.52}
2025-05-13 13:16:27 {'loss': 1.4274, 'grad_norm': 21.425397872924805, 'learning_rate': 2.22e-05, 'epoch': 0.56}
2025-05-13 13:16:37 {'loss': 1.296, 'grad_norm': 12.406946182250977, 'learning_rate': 2.0200000000000003e-05, 'epoch': 0.6}
2025-05-13 13:16:46 {'loss': 1.537, 'grad_norm': 10.146235466003418, 'learning_rate': 1.8200000000000002e-05, 'epoch': 0.64}
2025-05-13 13:17:00 {'loss': 1.3386, 'grad_norm': 13.720836639404297, 'learning_rate': 1.62e-05, 'epoch': 0.68}
2025-05-13 13:17:10 {'loss': 1.5644, 'grad_norm': 9.92667007446289, 'learning_rate': 1.42e-05, 'epoch': 0.72}
2025-05-13 13:17:19 {'loss': 1.275, 'grad_norm': 11.887932777404785, 'learning_rate': 1.22e-05, 'epoch': 0.76}
2025-05-13 13:17:29 {'loss': 1.4688, 'grad_norm': 15.31954288482666, 'learning_rate': 1.02e-05, 'epoch': 0.8}
2025-05-13 13:17:39 {'loss': 1.419, 'grad_norm': 11.688867568969727, 'learning_rate': 8.200000000000001e-06, 'epoch': 0.84}
2025-05-13 13:17:53 {'loss': 1.2847, 'grad_norm': 13.761067390441895, 'learning_rate': 6.2e-06, 'epoch': 0.88}
2025-05-13 13:18:03 {'loss': 1.4009, 'grad_norm': 14.512120246887207, 'learning_rate': 4.2000000000000004e-06, 'epoch': 0.92}
2025-05-13 13:18:13 {'loss': 1.3505, 'grad_norm': 12.186603546142578, 'learning_rate': 2.2e-06, 'epoch': 0.96}
2025-05-13 13:18:22 {'loss': 1.4136, 'grad_norm': 12.773447036743164, 'learning_rate': 2.0000000000000002e-07, 'epoch': 1.0}
2025-05-13 13:18:22 {'train_runtime': 260.8353, 'train_samples_per_second': 1.917, 'train_steps_per_second': 0.958, 'train_loss': 1.500004482269287, 'epoch': 1.0}
2025-05-13 13:18:59 {'loss': 0.8904, 'grad_norm': 10.24727725982666, 'learning_rate': 4.82e-05, 'epoch': 0.04}
2025-05-13 13:19:13 {'loss': 0.9551, 'grad_norm': 9.997514724731445, 'learning_rate': 4.6200000000000005e-05, 'epoch': 0.08}
2025-05-13 13:19:22 {'loss': 0.9858, 'grad_norm': 9.317275047302246, 'learning_rate': 4.4200000000000004e-05, 'epoch': 0.12}
2025-05-13 13:19:32 {'loss': 1.0748, 'grad_norm': 14.078439712524414, 'learning_rate': 4.22e-05, 'epoch': 0.16}
2025-05-13 13:19:42 {'loss': 1.0746, 'grad_norm': 13.68847942352295, 'learning_rate': 4.02e-05, 'epoch': 0.2}
2025-05-13 13:19:56 {'loss': 1.0695, 'grad_norm': 11.08474349975586, 'learning_rate': 3.82e-05, 'epoch': 0.24}
2025-05-13 13:20:06 {'loss': 1.1079, 'grad_norm': 14.132729530334473, 'learning_rate': 3.62e-05, 'epoch': 0.28}
2025-05-13 13:20:15 {'loss': 1.0696, 'grad_norm': 11.425318717956543, 'learning_rate': 3.4200000000000005e-05, 'epoch': 0.32}
2025-05-13 13:20:25 {'loss': 1.0645, 'grad_norm': 12.561664581298828, 'learning_rate': 3.2200000000000003e-05, 'epoch': 0.36}
2025-05-13 13:20:34 {'loss': 1.0281, 'grad_norm': 9.622440338134766, 'learning_rate': 3.02e-05, 'epoch': 0.4}
2025-05-13 13:20:44 {'loss': 1.0585, 'grad_norm': 13.399136543273926, 'learning_rate': 2.8199999999999998e-05, 'epoch': 0.44}
2025-05-13 13:20:53 {'loss': 1.1666, 'grad_norm': 13.080180168151855, 'learning_rate': 2.6200000000000003e-05, 'epoch': 0.48}
2025-05-13 13:21:06 {'loss': 1.0339, 'grad_norm': 13.61414623260498, 'learning_rate': 2.4200000000000002e-05, 'epoch': 0.52}
2025-05-13 13:21:16 {'loss': 1.1274, 'grad_norm': 20.214048385620117, 'learning_rate': 2.22e-05, 'epoch': 0.56}
2025-05-13 13:21:25 {'loss': 1.0233, 'grad_norm': 13.432079315185547, 'learning_rate': 2.0200000000000003e-05, 'epoch': 0.6}
2025-05-13 13:21:35 {'loss': 1.2718, 'grad_norm': 14.509381294250488, 'learning_rate': 1.8200000000000002e-05, 'epoch': 0.64}
2025-05-13 13:21:44 {'loss': 1.1081, 'grad_norm': 11.4351167678833, 'learning_rate': 1.62e-05, 'epoch': 0.68}
2025-05-13 13:21:54 {'loss': 1.3089, 'grad_norm': 9.157770156860352, 'learning_rate': 1.42e-05, 'epoch': 0.72}
2025-05-13 13:22:07 {'loss': 1.0992, 'grad_norm': 11.927536964416504, 'learning_rate': 1.22e-05, 'epoch': 0.76}
2025-05-13 13:22:17 {'loss': 1.2785, 'grad_norm': 14.36587905883789, 'learning_rate': 1.02e-05, 'epoch': 0.8}
2025-05-13 13:22:27 {'loss': 1.2617, 'grad_norm': 12.387429237365723, 'learning_rate': 8.200000000000001e-06, 'epoch': 0.84}
2025-05-13 13:22:36 {'loss': 1.1382, 'grad_norm': 12.514969825744629, 'learning_rate': 6.2e-06, 'epoch': 0.88}
2025-05-13 13:22:46 {'loss': 1.1889, 'grad_norm': 12.561593055725098, 'learning_rate': 4.2000000000000004e-06, 'epoch': 0.92}
2025-05-13 13:22:55 {'loss': 1.035, 'grad_norm': 9.857131004333496, 'learning_rate': 2.2e-06, 'epoch': 0.96}
2025-05-13 13:23:08 {'loss': 0.821, 'grad_norm': 7.920310020446777, 'learning_rate': 2.0000000000000002e-07, 'epoch': 1.0}
2025-05-13 13:23:08 {'train_runtime': 259.7584, 'train_samples_per_second': 1.925, 'train_steps_per_second': 0.962, 'train_loss': 1.0896451568603516, 'epoch': 1.0}
2025-05-13 13:23:42 {'loss': 0.5481, 'grad_norm': 8.514246940612793, 'learning_rate': 4.82e-05, 'epoch': 0.04}
2025-05-13 13:23:56 {'loss': 0.6274, 'grad_norm': 8.850375175476074, 'learning_rate': 4.6200000000000005e-05, 'epoch': 0.08}
2025-05-13 13:24:05 {'loss': 0.6694, 'grad_norm': 9.719736099243164, 'learning_rate': 4.4200000000000004e-05, 'epoch': 0.12}
2025-05-13 13:24:15 {'loss': 0.7579, 'grad_norm': 12.910091400146484, 'learning_rate': 4.22e-05, 'epoch': 0.16}
2025-05-13 13:24:24 {'loss': 0.7745, 'grad_norm': 18.07182502746582, 'learning_rate': 4.02e-05, 'epoch': 0.2}
2025-05-13 13:24:34 {'loss': 0.7969, 'grad_norm': 14.431644439697266, 'learning_rate': 3.82e-05, 'epoch': 0.24}
2025-05-13 13:24:44 {'loss': 0.8113, 'grad_norm': 13.639501571655273, 'learning_rate': 3.62e-05, 'epoch': 0.28}
2025-05-13 13:24:53 {'loss': 0.7902, 'grad_norm': 11.110507011413574, 'learning_rate': 3.4200000000000005e-05, 'epoch': 0.32}
2025-05-13 13:25:06 {'loss': 0.8227, 'grad_norm': 11.421666145324707, 'learning_rate': 3.2200000000000003e-05, 'epoch': 0.36}
2025-05-13 13:25:16 {'loss': 0.7907, 'grad_norm': 11.948728561401367, 'learning_rate': 3.02e-05, 'epoch': 0.4}
2025-05-13 13:25:26 {'loss': 0.8285, 'grad_norm': 12.971101760864258, 'learning_rate': 2.8199999999999998e-05, 'epoch': 0.44}
2025-05-13 13:25:36 {'loss': 0.9238, 'grad_norm': 11.14509105682373, 'learning_rate': 2.6200000000000003e-05, 'epoch': 0.48}
2025-05-13 13:25:45 {'loss': 0.833, 'grad_norm': 13.371801376342773, 'learning_rate': 2.4200000000000002e-05, 'epoch': 0.52}
2025-05-13 13:25:54 {'loss': 0.9501, 'grad_norm': 19.082460403442383, 'learning_rate': 2.22e-05, 'epoch': 0.56}
2025-05-13 13:26:08 {'loss': 0.8483, 'grad_norm': 11.831804275512695, 'learning_rate': 2.0200000000000003e-05, 'epoch': 0.6}
2025-05-13 13:26:17 {'loss': 1.0977, 'grad_norm': 9.936162948608398, 'learning_rate': 1.8200000000000002e-05, 'epoch': 0.64}
2025-05-13 13:26:27 {'loss': 0.9563, 'grad_norm': 9.876827239990234, 'learning_rate': 1.62e-05, 'epoch': 0.68}
2025-05-13 13:26:36 {'loss': 1.1526, 'grad_norm': 8.943462371826172, 'learning_rate': 1.42e-05, 'epoch': 0.72}
2025-05-13 13:26:46 {'loss': 0.9754, 'grad_norm': 10.429888725280762, 'learning_rate': 1.22e-05, 'epoch': 0.76}
2025-05-13 13:26:56 {'loss': 1.1675, 'grad_norm': 12.965577125549316, 'learning_rate': 1.02e-05, 'epoch': 0.8}
2025-05-13 13:27:09 {'loss': 1.1691, 'grad_norm': 14.329486846923828, 'learning_rate': 8.200000000000001e-06, 'epoch': 0.84}
2025-05-13 13:27:18 {'loss': 1.0651, 'grad_norm': 13.711791038513184, 'learning_rate': 6.2e-06, 'epoch': 0.88}
2025-05-13 13:27:28 {'loss': 1.1102, 'grad_norm': 12.893461227416992, 'learning_rate': 4.2000000000000004e-06, 'epoch': 0.92}
2025-05-13 13:27:38 {'loss': 0.9098, 'grad_norm': 10.396915435791016, 'learning_rate': 2.2e-06, 'epoch': 0.96}
2025-05-13 13:27:47 {'loss': 0.604, 'grad_norm': 8.632681846618652, 'learning_rate': 2.0000000000000002e-07, 'epoch': 1.0}
2025-05-13 13:27:47 {'train_runtime': 255.5026, 'train_samples_per_second': 1.957, 'train_steps_per_second': 0.978, 'train_loss': 0.8792306175231933, 'epoch': 1.0}
2025-05-13 13:28:21 {'loss': 0.3317, 'grad_norm': 6.615481853485107, 'learning_rate': 4.82e-05, 'epoch': 0.04}
2025-05-13 13:28:31 {'loss': 0.4402, 'grad_norm': 8.103501319885254, 'learning_rate': 4.6200000000000005e-05, 'epoch': 0.08}
2025-05-13 13:28:44 {'loss': 0.4495, 'grad_norm': 7.17033052444458, 'learning_rate': 4.4200000000000004e-05, 'epoch': 0.12}
2025-05-13 13:28:54 {'loss': 0.5514, 'grad_norm': 11.884796142578125, 'learning_rate': 4.22e-05, 'epoch': 0.16}
2025-05-13 13:29:03 {'loss': 0.5589, 'grad_norm': 13.953606605529785, 'learning_rate': 4.02e-05, 'epoch': 0.2}
2025-05-13 13:29:13 {'loss': 0.6184, 'grad_norm': 12.690035820007324, 'learning_rate': 3.82e-05, 'epoch': 0.24}
2025-05-13 13:29:23 {'loss': 0.6123, 'grad_norm': 12.158244132995605, 'learning_rate': 3.62e-05, 'epoch': 0.28}
2025-05-13 13:29:32 {'loss': 0.6317, 'grad_norm': 8.473913192749023, 'learning_rate': 3.4200000000000005e-05, 'epoch': 0.32}
2025-05-13 13:29:42 {'loss': 0.6265, 'grad_norm': 11.031872749328613, 'learning_rate': 3.2200000000000003e-05, 'epoch': 0.36}
2025-05-13 13:29:55 {'loss': 0.6305, 'grad_norm': 8.859527587890625, 'learning_rate': 3.02e-05, 'epoch': 0.4}
2025-05-13 13:30:05 {'loss': 0.6569, 'grad_norm': 12.478569030761719, 'learning_rate': 2.8199999999999998e-05, 'epoch': 0.44}
2025-05-13 13:30:15 {'loss': 0.7581, 'grad_norm': 10.978435516357422, 'learning_rate': 2.6200000000000003e-05, 'epoch': 0.48}
2025-05-13 13:30:24 {'loss': 0.6578, 'grad_norm': 11.56550407409668, 'learning_rate': 2.4200000000000002e-05, 'epoch': 0.52}
2025-05-13 13:30:34 {'loss': 0.7682, 'grad_norm': 17.419750213623047, 'learning_rate': 2.22e-05, 'epoch': 0.56}
2025-05-13 13:30:43 {'loss': 0.7115, 'grad_norm': 11.181468963623047, 'learning_rate': 2.0200000000000003e-05, 'epoch': 0.6}
2025-05-13 13:30:57 {'loss': 0.9553, 'grad_norm': 9.350264549255371, 'learning_rate': 1.8200000000000002e-05, 'epoch': 0.64}
2025-05-13 13:31:06 {'loss': 0.8168, 'grad_norm': 9.742260932922363, 'learning_rate': 1.62e-05, 'epoch': 0.68}
2025-05-13 13:31:16 {'loss': 1.014, 'grad_norm': 9.283716201782227, 'learning_rate': 1.42e-05, 'epoch': 0.72}
2025-05-13 13:31:26 {'loss': 0.878, 'grad_norm': 11.888051986694336, 'learning_rate': 1.22e-05, 'epoch': 0.76}
2025-05-13 13:31:35 {'loss': 1.0681, 'grad_norm': 13.993132591247559, 'learning_rate': 1.02e-05, 'epoch': 0.8}
2025-05-13 13:31:45 {'loss': 1.0864, 'grad_norm': 12.921503067016602, 'learning_rate': 8.200000000000001e-06, 'epoch': 0.84}
2025-05-13 13:31:55 {'loss': 1.0076, 'grad_norm': 12.179827690124512, 'learning_rate': 6.2e-06, 'epoch': 0.88}
2025-05-13 13:32:08 {'loss': 1.0369, 'grad_norm': 12.208800315856934, 'learning_rate': 4.2000000000000004e-06, 'epoch': 0.92}
2025-05-13 13:32:18 {'loss': 0.8088, 'grad_norm': 9.612824440002441, 'learning_rate': 2.2e-06, 'epoch': 0.96}
2025-05-13 13:32:27 {'loss': 0.5038, 'grad_norm': 8.004541397094727, 'learning_rate': 2.0000000000000002e-07, 'epoch': 1.0}
2025-05-13 13:32:27 {'train_runtime': 255.9676, 'train_samples_per_second': 1.953, 'train_steps_per_second': 0.977, 'train_loss': 0.7271781673431397, 'epoch': 1.0}
2025-05-13 13:32:59 {'loss': 0.2011, 'grad_norm': 5.781416416168213, 'learning_rate': 4.82e-05, 'epoch': 0.04}
2025-05-13 13:33:13 {'loss': 0.296, 'grad_norm': 7.91112756729126, 'learning_rate': 4.6200000000000005e-05, 'epoch': 0.08}
2025-05-13 13:33:22 {'loss': 0.2834, 'grad_norm': 6.6639275550842285, 'learning_rate': 4.4200000000000004e-05, 'epoch': 0.12}
2025-05-13 13:33:32 {'loss': 0.3792, 'grad_norm': 9.294739723205566, 'learning_rate': 4.22e-05, 'epoch': 0.16}
2025-05-13 13:33:42 {'loss': 0.3908, 'grad_norm': 11.810991287231445, 'learning_rate': 4.02e-05, 'epoch': 0.2}
2025-05-13 13:33:51 {'loss': 0.4587, 'grad_norm': 10.526397705078125, 'learning_rate': 3.82e-05, 'epoch': 0.24}
2025-05-13 13:34:01 {'loss': 0.4643, 'grad_norm': 11.7634916305542, 'learning_rate': 3.62e-05, 'epoch': 0.28}
2025-05-13 13:34:14 {'loss': 0.4845, 'grad_norm': 12.56328296661377, 'learning_rate': 3.4200000000000005e-05, 'epoch': 0.32}
2025-05-13 13:34:24 {'loss': 0.493, 'grad_norm': 10.391369819641113, 'learning_rate': 3.2200000000000003e-05, 'epoch': 0.36}
2025-05-13 13:34:33 {'loss': 0.4848, 'grad_norm': 8.816697120666504, 'learning_rate': 3.02e-05, 'epoch': 0.4}
2025-05-13 13:34:43 {'loss': 0.4937, 'grad_norm': 10.637781143188477, 'learning_rate': 2.8199999999999998e-05, 'epoch': 0.44}
2025-05-13 13:34:52 {'loss': 0.5789, 'grad_norm': 10.874310493469238, 'learning_rate': 2.6200000000000003e-05, 'epoch': 0.48}
2025-05-13 13:35:02 {'loss': 0.5214, 'grad_norm': 12.385919570922852, 'learning_rate': 2.4200000000000002e-05, 'epoch': 0.52}
2025-05-13 13:35:11 {'loss': 0.6199, 'grad_norm': 16.59426498413086, 'learning_rate': 2.22e-05, 'epoch': 0.56}
2025-05-13 13:35:25 {'loss': 0.5762, 'grad_norm': 11.066338539123535, 'learning_rate': 2.0200000000000003e-05, 'epoch': 0.6}
2025-05-13 13:35:34 {'loss': 0.8295, 'grad_norm': 8.975237846374512, 'learning_rate': 1.8200000000000002e-05, 'epoch': 0.64}
2025-05-13 13:35:44 {'loss': 0.7101, 'grad_norm': 11.187599182128906, 'learning_rate': 1.62e-05, 'epoch': 0.68}
2025-05-13 13:35:54 {'loss': 0.878, 'grad_norm': 9.42457389831543, 'learning_rate': 1.42e-05, 'epoch': 0.72}
2025-05-13 13:36:03 {'loss': 0.794, 'grad_norm': 11.450490951538086, 'learning_rate': 1.22e-05, 'epoch': 0.76}
2025-05-13 13:36:13 {'loss': 0.992, 'grad_norm': 12.459307670593262, 'learning_rate': 1.02e-05, 'epoch': 0.8}
2025-05-13 13:36:26 {'loss': 1.029, 'grad_norm': 13.677318572998047, 'learning_rate': 8.200000000000001e-06, 'epoch': 0.84}
2025-05-13 13:36:35 {'loss': 0.9709, 'grad_norm': 12.215568542480469, 'learning_rate': 6.2e-06, 'epoch': 0.88}
2025-05-13 13:36:45 {'loss': 0.9889, 'grad_norm': 13.147656440734863, 'learning_rate': 4.2000000000000004e-06, 'epoch': 0.92}
2025-05-13 13:36:55 {'loss': 0.728, 'grad_norm': 8.921481132507324, 'learning_rate': 2.2e-06, 'epoch': 0.96}
2025-05-13 13:37:04 {'loss': 0.3945, 'grad_norm': 7.533817768096924, 'learning_rate': 2.0000000000000002e-07, 'epoch': 1.0}
2025-05-13 13:37:04 {'train_runtime': 254.5016, 'train_samples_per_second': 1.965, 'train_steps_per_second': 0.982, 'train_loss': 0.6016265573501587, 'epoch': 1.0}
2025-05-13 13:23:16 INFO :      Sent reply
2025-05-13 13:23:30 INFO :      
2025-05-13 13:23:30 INFO :      Received: train message 1f504c42-1eb1-4bcd-8914-431e11304127
2025-05-13 13:27:54 INFO :      Sent reply
2025-05-13 13:28:10 INFO :      
2025-05-13 13:28:10 INFO :      Received: train message 2dc1a64b-aecb-4e25-b825-93182a79b6de
2025-05-13 13:32:33 INFO :      Sent reply
2025-05-13 13:32:48 INFO :      
2025-05-13 13:32:48 INFO :      Received: train message bdf96b18-a45a-49b2-9d45-f46c0a7d911c
2025-05-13 13:37:11 INFO :      Sent reply
2025-05-13 13:37:19 INFO :      
2025-05-13 13:37:19 INFO :      Received: reconnect message 7ffedbd5-0c59-43f1-a411-4d48235995ad
2025-05-13 13:37:20 INFO :      Disconnect and shut down
