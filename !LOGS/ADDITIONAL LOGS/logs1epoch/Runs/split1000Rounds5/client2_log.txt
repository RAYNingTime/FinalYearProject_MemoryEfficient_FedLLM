2025-05-13 14:41:29 
Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]
Generating train split:  78%|███████▊  | 94000/120000 [00:00<00:00, 933033.68 examples/s]
Generating train split: 100%|██████████| 120000/120000 [00:00<00:00, 1010947.68 examples/s]
2025-05-13 14:41:29 
Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]
Generating test split: 100%|██████████| 7600/7600 [00:00<00:00, 915392.42 examples/s]
2025-05-13 14:41:31 
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 1323.89 examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 1318.31 examples/s]
2025-05-13 14:41:31 
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]
Map:  27%|██▋       | 270/1000 [00:00<00:00, 2661.15 examples/s]
Map:  67%|██████▋   | 667/1000 [00:00<00:00, 2636.85 examples/s]
Map:  96%|█████████▋| 964/1000 [00:00<00:00, 2307.40 examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 2227.19 examples/s]
2025-05-13 14:41:31 /app/client.py:45: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
2025-05-13 14:41:31   trainer = Trainer(
2025-05-13 14:41:32 WARNING :   DEPRECATED FEATURE: flwr.client.start_numpy_client() is deprecated. 
2025-05-13 14:41:32 Instead, use `flwr.client.start_client()` by ensuring you first call the `.to_client()` method as shown below: 
2025-05-13 14:41:32 flwr.client.start_client(
2025-05-13 14:41:32 server_address='<IP>:<PORT>',
2025-05-13 14:41:32 client=FlowerClient().to_client(), # <-- where FlowerClient is of type flwr.client.NumPyClient object
2025-05-13 14:41:32 )
2025-05-13 14:41:32 Using `start_numpy_client()` is deprecated.
2025-05-13 14:41:32 
2025-05-13 14:41:32             This is a deprecated feature. It will be removed
2025-05-13 14:41:32             entirely in future versions of Flower.
2025-05-13 14:41:32         
2025-05-13 14:41:32 WARNING :   DEPRECATED FEATURE: flwr.client.start_client() is deprecated.
2025-05-13 14:41:32 Instead, use the `flower-supernode` CLI command to start a SuperNode as shown below:
2025-05-13 14:41:32 
2025-05-13 14:41:32 $ flower-supernode --insecure --superlink='<IP>:<PORT>'
2025-05-13 14:41:32 
2025-05-13 14:41:32 To view all available options, run:
2025-05-13 14:41:32 
2025-05-13 14:41:32 $ flower-supernode --help
2025-05-13 14:41:32 
2025-05-13 14:41:32 Using `start_client()` is deprecated.
2025-05-13 14:41:32 
2025-05-13 14:41:32             This is a deprecated feature. It will be removed
2025-05-13 14:41:32             entirely in future versions of Flower.
2025-05-13 14:41:32         
2025-05-13 14:41:32 INFO :      
2025-05-13 14:41:32 INFO :      Received: get_parameters message 0b82e741-b569-4269-bca3-eb838cae4a03
2025-05-13 14:41:35 INFO :      Sent reply
2025-05-13 14:41:45 INFO :      
2025-05-13 14:41:45 INFO :      Received: train message be70acaf-57fc-47bd-bffc-4dee06749a8d
2025-05-13 14:41:59 {'loss': 2.6476, 'grad_norm': 18.667423248291016, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 14:42:09 {'loss': 1.6533, 'grad_norm': 14.597430229187012, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 14:42:22 {'loss': 1.6346, 'grad_norm': 19.806570053100586, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 14:42:32 {'loss': 1.5147, 'grad_norm': 11.533321380615234, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 14:42:41 {'loss': 1.5607, 'grad_norm': 16.239545822143555, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 14:42:51 {'loss': 1.4235, 'grad_norm': 14.735560417175293, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 14:43:01 {'loss': 1.3801, 'grad_norm': 11.47992992401123, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 14:43:14 {'loss': 1.4928, 'grad_norm': 12.755566596984863, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 14:43:24 {'loss': 1.2775, 'grad_norm': 13.867303848266602, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 14:43:33 {'loss': 1.413, 'grad_norm': 13.377055168151855, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 14:43:43 {'loss': 1.4638, 'grad_norm': 11.14098834991455, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 14:43:53 {'loss': 1.5317, 'grad_norm': 16.719707489013672, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 14:44:02 {'loss': 1.5412, 'grad_norm': 14.699211120605469, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 14:44:16 {'loss': 1.34, 'grad_norm': 11.303004264831543, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 14:44:26 {'loss': 1.4992, 'grad_norm': 11.791444778442383, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 14:44:35 {'loss': 1.4179, 'grad_norm': 12.727545738220215, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 14:44:45 {'loss': 1.6515, 'grad_norm': 13.811737060546875, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 14:44:55 {'loss': 1.4676, 'grad_norm': 11.954639434814453, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 14:45:05 {'loss': 1.4366, 'grad_norm': 14.865253448486328, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 14:45:18 {'loss': 1.3679, 'grad_norm': 12.745451927185059, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 14:45:28 {'loss': 1.583, 'grad_norm': 13.52482795715332, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 14:45:38 {'loss': 1.5815, 'grad_norm': 20.672588348388672, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 14:45:47 {'loss': 1.448, 'grad_norm': 16.254541397094727, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 14:45:57 {'loss': 1.5469, 'grad_norm': 10.708218574523926, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 14:46:06 {'loss': 1.4196, 'grad_norm': 10.960878372192383, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 14:46:20 {'loss': 1.3361, 'grad_norm': 11.715882301330566, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 14:46:29 {'loss': 1.4753, 'grad_norm': 14.1637544631958, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 14:46:39 {'loss': 1.5778, 'grad_norm': 10.979082107543945, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 14:46:49 {'loss': 1.4089, 'grad_norm': 11.89306926727295, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 14:46:59 {'loss': 1.4044, 'grad_norm': 10.345044136047363, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 14:47:08 {'loss': 1.4182, 'grad_norm': 12.881041526794434, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 14:47:18 {'loss': 1.381, 'grad_norm': 12.148990631103516, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 14:47:31 {'loss': 1.3378, 'grad_norm': 11.297174453735352, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 14:47:41 {'loss': 1.4548, 'grad_norm': 10.685917854309082, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 14:47:50 {'loss': 1.4016, 'grad_norm': 11.256150245666504, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 14:48:00 {'loss': 1.2591, 'grad_norm': 13.242323875427246, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 14:48:09 {'loss': 1.5684, 'grad_norm': 12.140288352966309, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 14:48:19 {'loss': 1.2984, 'grad_norm': 13.39657974243164, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 14:48:29 {'loss': 1.4199, 'grad_norm': 10.896543502807617, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 14:48:42 {'loss': 1.4087, 'grad_norm': 12.410995483398438, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 14:48:52 {'loss': 1.4482, 'grad_norm': 10.810933113098145, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 14:49:02 {'loss': 1.3652, 'grad_norm': 12.634051322937012, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 14:49:11 {'loss': 1.2963, 'grad_norm': 12.784635543823242, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 14:49:21 {'loss': 1.3526, 'grad_norm': 12.187051773071289, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 14:49:31 {'loss': 1.2595, 'grad_norm': 11.523948669433594, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 14:49:44 {'loss': 1.3866, 'grad_norm': 11.84868335723877, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 14:49:54 {'loss': 1.5414, 'grad_norm': 16.074962615966797, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 14:50:03 {'loss': 1.1848, 'grad_norm': 8.580780982971191, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 14:50:13 {'loss': 1.4945, 'grad_norm': 13.382322311401367, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 14:50:22 {'loss': 1.3534, 'grad_norm': 10.782709121704102, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 14:50:22 {'train_runtime': 516.4295, 'train_samples_per_second': 1.936, 'train_steps_per_second': 0.968, 'train_loss': 1.462536033630371, 'epoch': 1.0}
2025-05-13 14:50:58 {'loss': 0.8742, 'grad_norm': 11.3317232131958, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 14:51:08 {'loss': 1.0113, 'grad_norm': 9.381179809570312, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 14:51:22 {'loss': 1.0529, 'grad_norm': 13.850459098815918, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 14:51:31 {'loss': 1.0427, 'grad_norm': 9.224855422973633, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 14:51:41 {'loss': 1.0427, 'grad_norm': 12.175333976745605, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 14:51:51 {'loss': 1.0267, 'grad_norm': 12.929387092590332, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 14:52:00 {'loss': 0.9578, 'grad_norm': 9.577899932861328, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 14:52:10 {'loss': 1.076, 'grad_norm': 12.489730834960938, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 14:52:23 {'loss': 0.8949, 'grad_norm': 10.682544708251953, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 14:52:33 {'loss': 0.9827, 'grad_norm': 10.887852668762207, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 14:52:42 {'loss': 1.0428, 'grad_norm': 9.321113586425781, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 14:52:52 {'loss': 1.1082, 'grad_norm': 10.88374137878418, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 14:53:01 {'loss': 1.1416, 'grad_norm': 10.62307357788086, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 14:53:11 {'loss': 0.9729, 'grad_norm': 10.5408353805542, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 14:53:24 {'loss': 1.1096, 'grad_norm': 10.407919883728027, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 14:53:34 {'loss': 1.0154, 'grad_norm': 11.872344017028809, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 14:53:44 {'loss': 1.2282, 'grad_norm': 12.938863754272461, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 14:53:53 {'loss': 1.0998, 'grad_norm': 10.847574234008789, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 14:54:03 {'loss': 1.0919, 'grad_norm': 13.29143238067627, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 14:54:13 {'loss': 1.0281, 'grad_norm': 10.967524528503418, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 14:54:22 {'loss': 1.1927, 'grad_norm': 12.349968910217285, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 14:54:32 {'loss': 1.2337, 'grad_norm': 15.08395767211914, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 14:54:45 {'loss': 1.1219, 'grad_norm': 14.97036361694336, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 14:54:55 {'loss': 1.1968, 'grad_norm': 9.149460792541504, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 14:55:04 {'loss': 1.1042, 'grad_norm': 8.809840202331543, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 14:55:14 {'loss': 1.0674, 'grad_norm': 11.411962509155273, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 14:55:23 {'loss': 1.1927, 'grad_norm': 12.24872875213623, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 14:55:37 {'loss': 1.2982, 'grad_norm': 10.290772438049316, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 14:55:46 {'loss': 1.101, 'grad_norm': 10.987435340881348, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 14:55:56 {'loss': 1.1522, 'grad_norm': 10.681610107421875, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 14:56:05 {'loss': 1.1567, 'grad_norm': 11.911856651306152, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 14:56:15 {'loss': 1.1706, 'grad_norm': 10.330103874206543, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 14:56:24 {'loss': 1.1386, 'grad_norm': 9.63045883178711, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 14:56:34 {'loss': 1.2364, 'grad_norm': 11.72547435760498, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 14:56:47 {'loss': 1.1871, 'grad_norm': 11.511741638183594, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 14:56:57 {'loss': 1.0856, 'grad_norm': 11.242377281188965, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 14:57:06 {'loss': 1.3564, 'grad_norm': 12.119418144226074, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 14:57:16 {'loss': 1.1045, 'grad_norm': 13.655289649963379, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 14:57:26 {'loss': 1.2356, 'grad_norm': 9.89541244506836, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 14:57:35 {'loss': 1.2516, 'grad_norm': 15.69943618774414, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 14:57:45 {'loss': 1.3098, 'grad_norm': 10.97724723815918, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 14:57:58 {'loss': 1.2172, 'grad_norm': 12.12585163116455, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 14:58:08 {'loss': 1.1774, 'grad_norm': 11.36531925201416, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 14:58:18 {'loss': 1.1996, 'grad_norm': 9.978365898132324, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 14:58:27 {'loss': 1.1426, 'grad_norm': 10.005638122558594, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 14:58:37 {'loss': 1.2972, 'grad_norm': 12.113786697387695, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 14:58:50 {'loss': 1.444, 'grad_norm': 13.27875804901123, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 14:59:00 {'loss': 1.0508, 'grad_norm': 8.080606460571289, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 14:59:09 {'loss': 1.1804, 'grad_norm': 10.484134674072266, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 14:59:19 {'loss': 0.8138, 'grad_norm': 7.912569522857666, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 14:59:19 {'train_runtime': 510.7382, 'train_samples_per_second': 1.958, 'train_steps_per_second': 0.979, 'train_loss': 1.1243437843322754, 'epoch': 1.0}
2025-05-13 14:59:52 {'loss': 0.53, 'grad_norm': 9.097503662109375, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 15:00:06 {'loss': 0.6904, 'grad_norm': 8.027504920959473, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 15:00:15 {'loss': 0.7441, 'grad_norm': 11.770605087280273, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 15:00:25 {'loss': 0.7461, 'grad_norm': 8.778541564941406, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 15:00:34 {'loss': 0.7329, 'grad_norm': 11.187856674194336, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 15:00:44 {'loss': 0.7683, 'grad_norm': 11.738333702087402, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 15:00:53 {'loss': 0.6961, 'grad_norm': 9.956257820129395, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 15:01:07 {'loss': 0.8037, 'grad_norm': 10.423371315002441, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 15:01:16 {'loss': 0.6401, 'grad_norm': 7.673334121704102, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 15:01:26 {'loss': 0.7547, 'grad_norm': 14.741395950317383, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 15:01:36 {'loss': 0.7934, 'grad_norm': 8.612288475036621, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 15:01:45 {'loss': 0.8385, 'grad_norm': 9.301312446594238, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 15:01:59 {'loss': 0.8868, 'grad_norm': 9.156448364257812, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 15:02:08 {'loss': 0.7636, 'grad_norm': 10.177605628967285, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 15:02:18 {'loss': 0.8648, 'grad_norm': 10.652417182922363, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 15:02:27 {'loss': 0.7713, 'grad_norm': 11.304559707641602, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 15:02:37 {'loss': 0.9728, 'grad_norm': 12.036602973937988, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 15:02:46 {'loss': 0.8566, 'grad_norm': 9.662018775939941, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 15:02:56 {'loss': 0.867, 'grad_norm': 10.802114486694336, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 15:03:09 {'loss': 0.8276, 'grad_norm': 13.49249267578125, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 15:03:19 {'loss': 0.9567, 'grad_norm': 11.540982246398926, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 15:03:28 {'loss': 0.9817, 'grad_norm': 13.910213470458984, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 15:03:38 {'loss': 0.9156, 'grad_norm': 12.610824584960938, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 15:03:47 {'loss': 0.986, 'grad_norm': 8.191637992858887, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 15:03:57 {'loss': 0.9067, 'grad_norm': 9.047628402709961, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 15:04:10 {'loss': 0.8841, 'grad_norm': 12.158385276794434, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 15:04:20 {'loss': 0.9872, 'grad_norm': 13.888538360595703, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 15:04:30 {'loss': 1.0761, 'grad_norm': 9.779940605163574, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 15:04:39 {'loss': 0.9426, 'grad_norm': 11.82186508178711, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 15:04:49 {'loss': 0.9708, 'grad_norm': 9.583664894104004, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 15:04:59 {'loss': 1.0151, 'grad_norm': 12.271120071411133, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 15:05:09 {'loss': 1.0079, 'grad_norm': 11.578092575073242, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 15:05:22 {'loss': 0.9889, 'grad_norm': 9.459115028381348, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 15:05:31 {'loss': 1.07, 'grad_norm': 9.748970985412598, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 15:05:41 {'loss': 1.0383, 'grad_norm': 9.9427490234375, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 15:05:50 {'loss': 0.9512, 'grad_norm': 13.724872589111328, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 15:06:00 {'loss': 1.2395, 'grad_norm': 11.809796333312988, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 15:06:09 {'loss': 0.9797, 'grad_norm': 11.716219902038574, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 15:06:23 {'loss': 1.1468, 'grad_norm': 11.012289047241211, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 15:06:32 {'loss': 1.1556, 'grad_norm': 12.667304039001465, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 15:06:42 {'loss': 1.2101, 'grad_norm': 11.363221168518066, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 15:06:52 {'loss': 1.1467, 'grad_norm': 12.154339790344238, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 15:07:01 {'loss': 1.1083, 'grad_norm': 11.50361156463623, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 15:07:15 {'loss': 1.1616, 'grad_norm': 10.832356452941895, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 15:07:24 {'loss': 1.1117, 'grad_norm': 10.712337493896484, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 15:07:34 {'loss': 1.2613, 'grad_norm': 11.876995086669922, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 15:07:43 {'loss': 1.3975, 'grad_norm': 11.991922378540039, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 15:07:53 {'loss': 0.9916, 'grad_norm': 7.573071479797363, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 15:08:02 {'loss': 1.0763, 'grad_norm': 13.290763854980469, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 15:08:16 {'loss': 0.6208, 'grad_norm': 8.965666770935059, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 15:08:16 {'train_runtime': 513.0844, 'train_samples_per_second': 1.949, 'train_steps_per_second': 0.974, 'train_loss': 0.9367050762176514, 'epoch': 1.0}
2025-05-13 15:08:44 {'loss': 0.3242, 'grad_norm': 6.674596786499023, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 15:08:58 {'loss': 0.4586, 'grad_norm': 9.3467435836792, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 15:09:07 {'loss': 0.5301, 'grad_norm': 11.544791221618652, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 15:09:17 {'loss': 0.5087, 'grad_norm': 7.344874858856201, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 15:09:26 {'loss': 0.5471, 'grad_norm': 11.218941688537598, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 15:09:36 {'loss': 0.5658, 'grad_norm': 11.749950408935547, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 15:09:46 {'loss': 0.5062, 'grad_norm': 9.175406455993652, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 15:09:59 {'loss': 0.5779, 'grad_norm': 9.94432544708252, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 15:10:08 {'loss': 0.4455, 'grad_norm': 7.814126014709473, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 15:10:18 {'loss': 0.5484, 'grad_norm': 10.757131576538086, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 15:10:27 {'loss': 0.5884, 'grad_norm': 8.553467750549316, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 15:10:37 {'loss': 0.6251, 'grad_norm': 8.121828079223633, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 15:10:50 {'loss': 0.6949, 'grad_norm': 9.261263847351074, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 15:11:00 {'loss': 0.6155, 'grad_norm': 10.955328941345215, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 15:11:09 {'loss': 0.6787, 'grad_norm': 9.928589820861816, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 15:11:19 {'loss': 0.6019, 'grad_norm': 9.4383544921875, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 15:11:28 {'loss': 0.7433, 'grad_norm': 10.213298797607422, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 15:11:38 {'loss': 0.6659, 'grad_norm': 12.08181095123291, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 15:11:51 {'loss': 0.7028, 'grad_norm': 12.111372947692871, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 15:12:01 {'loss': 0.6676, 'grad_norm': 8.872746467590332, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 15:12:10 {'loss': 0.7902, 'grad_norm': 14.021708488464355, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 15:12:20 {'loss': 0.8044, 'grad_norm': 15.257088661193848, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 15:12:30 {'loss': 0.7347, 'grad_norm': 11.80394172668457, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 15:12:43 {'loss': 0.8201, 'grad_norm': 7.587857723236084, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 15:12:53 {'loss': 0.7558, 'grad_norm': 9.266284942626953, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 15:13:02 {'loss': 0.7137, 'grad_norm': 10.374454498291016, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 15:13:12 {'loss': 0.8013, 'grad_norm': 12.047850608825684, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 15:13:22 {'loss': 0.9051, 'grad_norm': 10.930977821350098, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 15:13:31 {'loss': 0.7967, 'grad_norm': 9.961091041564941, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 15:13:41 {'loss': 0.83, 'grad_norm': 10.323195457458496, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 15:13:54 {'loss': 0.8617, 'grad_norm': 12.818504333496094, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 15:14:04 {'loss': 0.8497, 'grad_norm': 10.81076717376709, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 15:14:13 {'loss': 0.8542, 'grad_norm': 8.478463172912598, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 15:14:23 {'loss': 0.9485, 'grad_norm': 9.730812072753906, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 15:14:32 {'loss': 0.9304, 'grad_norm': 9.98270034790039, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 15:14:41 {'loss': 0.8313, 'grad_norm': 12.459123611450195, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 15:14:55 {'loss': 1.121, 'grad_norm': 12.453289031982422, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 15:15:04 {'loss': 0.8943, 'grad_norm': 10.732454299926758, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 15:15:14 {'loss': 1.0623, 'grad_norm': 9.629734992980957, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 15:15:23 {'loss': 1.0588, 'grad_norm': 12.420263290405273, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 15:15:33 {'loss': 1.158, 'grad_norm': 11.811697006225586, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 15:15:42 {'loss': 1.0757, 'grad_norm': 10.7979097366333, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 15:15:52 {'loss': 1.0666, 'grad_norm': 11.482047080993652, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 15:16:05 {'loss': 1.1086, 'grad_norm': 13.099592208862305, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 15:16:14 {'loss': 1.0871, 'grad_norm': 12.784932136535645, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 15:16:24 {'loss': 1.2368, 'grad_norm': 14.400555610656738, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 15:16:34 {'loss': 1.373, 'grad_norm': 13.560651779174805, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 15:16:43 {'loss': 0.9547, 'grad_norm': 7.233937740325928, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 15:16:53 {'loss': 0.9899, 'grad_norm': 8.700345039367676, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 15:17:06 {'loss': 0.5289, 'grad_norm': 7.019270896911621, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 15:17:06 {'train_runtime': 511.0077, 'train_samples_per_second': 1.957, 'train_steps_per_second': 0.978, 'train_loss': 0.7908034362792968, 'epoch': 1.0}
2025-05-13 15:17:37 {'loss': 0.1996, 'grad_norm': 6.195380687713623, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 15:17:50 {'loss': 0.301, 'grad_norm': 6.177771091461182, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 15:18:00 {'loss': 0.3936, 'grad_norm': 10.351057052612305, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 15:18:09 {'loss': 0.3702, 'grad_norm': 6.851762294769287, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 15:18:19 {'loss': 0.4207, 'grad_norm': 9.566206932067871, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 15:18:29 {'loss': 0.3837, 'grad_norm': 8.38704776763916, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 15:18:38 {'loss': 0.3535, 'grad_norm': 7.648932456970215, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 15:18:52 {'loss': 0.4228, 'grad_norm': 9.732954025268555, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 15:19:01 {'loss': 0.3106, 'grad_norm': 7.5930609703063965, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 15:19:11 {'loss': 0.4092, 'grad_norm': 10.66043472290039, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 15:19:21 {'loss': 0.4518, 'grad_norm': 7.371739864349365, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 15:19:30 {'loss': 0.4511, 'grad_norm': 8.116569519042969, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 15:19:40 {'loss': 0.5126, 'grad_norm': 10.311573028564453, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 15:19:50 {'loss': 0.4569, 'grad_norm': 9.621406555175781, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 15:19:59 {'loss': 0.5278, 'grad_norm': 8.400845527648926, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 15:20:13 {'loss': 0.4601, 'grad_norm': 9.81387710571289, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 15:20:22 {'loss': 0.5624, 'grad_norm': 12.709278106689453, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 15:20:32 {'loss': 0.508, 'grad_norm': 7.731442451477051, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 15:20:42 {'loss': 0.5656, 'grad_norm': 10.09536361694336, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 15:20:51 {'loss': 0.5534, 'grad_norm': 10.315255165100098, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 15:21:01 {'loss': 0.6279, 'grad_norm': 11.277783393859863, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 15:21:14 {'loss': 0.6594, 'grad_norm': 15.122061729431152, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 15:21:24 {'loss': 0.5972, 'grad_norm': 11.948506355285645, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 15:21:33 {'loss': 0.664, 'grad_norm': 7.407657146453857, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 15:21:43 {'loss': 0.6124, 'grad_norm': 10.519112586975098, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 14:50:30 INFO :      Sent reply
2025-05-13 14:50:47 INFO :      
2025-05-13 14:50:47 INFO :      Received: train message 8bdfc86f-5448-48c8-b470-21519f8ec6ee
2025-05-13 14:59:27 INFO :      Sent reply
2025-05-13 14:59:41 INFO :      
2025-05-13 14:59:41 INFO :      Received: train message b64202ec-2c62-4b8b-8f19-6a39ee781334
2025-05-13 15:08:19 INFO :      Sent reply
2025-05-13 15:08:33 INFO :      
2025-05-13 15:08:33 INFO :      Received: train message a847a6f3-de19-4312-b0ed-4015bc5b48e8
2025-05-13 15:17:10 INFO :      Sent reply
2025-05-13 15:17:24 INFO :      
2025-05-13 15:17:24 INFO :      Received: train message e041b7ce-1696-4267-b902-408fe885dc62
2025-05-13 15:26:06 INFO :      Sent reply
2025-05-13 15:26:15 INFO :      
2025-05-13 15:26:15 INFO :      Received: reconnect message 7ca90cfe-3a98-45b6-8a27-ac2db5be4f3f
2025-05-13 15:26:15 INFO :      Disconnect and shut down
2025-05-13 15:21:52 {'loss': 0.5826, 'grad_norm': 10.039960861206055, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 15:22:02 {'loss': 0.644, 'grad_norm': 10.996194839477539, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 15:22:12 {'loss': 0.7783, 'grad_norm': 10.473001480102539, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 15:22:25 {'loss': 0.6676, 'grad_norm': 15.270186424255371, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 15:22:35 {'loss': 0.7046, 'grad_norm': 10.505117416381836, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 15:22:44 {'loss': 0.7569, 'grad_norm': 12.23161792755127, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 15:22:54 {'loss': 0.7504, 'grad_norm': 11.219152450561523, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 15:23:04 {'loss': 0.7453, 'grad_norm': 10.19951057434082, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 15:23:14 {'loss': 0.8342, 'grad_norm': 9.631760597229004, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 15:23:27 {'loss': 0.8246, 'grad_norm': 9.751161575317383, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 15:23:36 {'loss': 0.7653, 'grad_norm': 12.526820182800293, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 15:23:46 {'loss': 1.0206, 'grad_norm': 12.196809768676758, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 15:23:56 {'loss': 0.8298, 'grad_norm': 10.830063819885254, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 15:24:05 {'loss': 0.9905, 'grad_norm': 9.454440116882324, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 15:24:15 {'loss': 0.9916, 'grad_norm': 12.934950828552246, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 15:24:28 {'loss': 1.0576, 'grad_norm': 11.006021499633789, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 15:24:38 {'loss': 1.0281, 'grad_norm': 11.721087455749512, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 15:24:48 {'loss': 1.0296, 'grad_norm': 12.474541664123535, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 15:24:57 {'loss': 1.0932, 'grad_norm': 11.077741622924805, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 15:25:07 {'loss': 1.0453, 'grad_norm': 11.022262573242188, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 15:25:20 {'loss': 1.2335, 'grad_norm': 14.411815643310547, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 15:25:30 {'loss': 1.3286, 'grad_norm': 12.51902961730957, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 15:25:39 {'loss': 0.9148, 'grad_norm': 8.202847480773926, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 15:25:49 {'loss': 0.9134, 'grad_norm': 10.18970012664795, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 15:25:59 {'loss': 0.403, 'grad_norm': 6.846887588500977, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 15:25:59 {'train_runtime': 513.1046, 'train_samples_per_second': 1.949, 'train_steps_per_second': 0.974, 'train_loss': 0.6741768877506256, 'epoch': 1.0}
