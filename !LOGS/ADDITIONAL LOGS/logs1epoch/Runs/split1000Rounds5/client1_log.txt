2025-05-13 14:41:34 
Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]
Generating train split: 100%|██████████| 120000/120000 [00:00<00:00, 1194090.90 examples/s]
Generating train split: 100%|██████████| 120000/120000 [00:00<00:00, 1190402.54 examples/s]
2025-05-13 14:41:34 
Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]
Generating test split: 100%|██████████| 7600/7600 [00:00<00:00, 989867.73 examples/s]
2025-05-13 14:41:36 
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 1118.12 examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 1112.46 examples/s]
2025-05-13 14:41:36 
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]
Map:  19%|█▉        | 193/1000 [00:00<00:00, 1890.59 examples/s]
Map:  47%|████▋     | 466/1000 [00:00<00:00, 1835.60 examples/s]
Map:  72%|███████▏  | 722/1000 [00:00<00:00, 1767.30 examples/s]
Map:  91%|█████████ | 907/1000 [00:00<00:00, 1788.34 examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 1655.55 examples/s]
2025-05-13 14:41:37 /app/client.py:45: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
2025-05-13 14:41:37   trainer = Trainer(
2025-05-13 14:41:37 WARNING :   DEPRECATED FEATURE: flwr.client.start_numpy_client() is deprecated. 
2025-05-13 14:41:37 Instead, use `flwr.client.start_client()` by ensuring you first call the `.to_client()` method as shown below: 
2025-05-13 14:41:37 flwr.client.start_client(
2025-05-13 14:41:37 server_address='<IP>:<PORT>',
2025-05-13 14:41:37 client=FlowerClient().to_client(), # <-- where FlowerClient is of type flwr.client.NumPyClient object
2025-05-13 14:41:37 )
2025-05-13 14:41:37 Using `start_numpy_client()` is deprecated.
2025-05-13 14:41:37 
2025-05-13 14:41:37             This is a deprecated feature. It will be removed
2025-05-13 14:41:37             entirely in future versions of Flower.
2025-05-13 14:41:37         
2025-05-13 14:41:37 WARNING :   DEPRECATED FEATURE: flwr.client.start_client() is deprecated.
2025-05-13 14:41:37 Instead, use the `flower-supernode` CLI command to start a SuperNode as shown below:
2025-05-13 14:41:37 
2025-05-13 14:41:37 $ flower-supernode --insecure --superlink='<IP>:<PORT>'
2025-05-13 14:41:37 
2025-05-13 14:41:37 To view all available options, run:
2025-05-13 14:41:37 
2025-05-13 14:41:37 $ flower-supernode --help
2025-05-13 14:41:37 
2025-05-13 14:41:37 Using `start_client()` is deprecated.
2025-05-13 14:41:37 
2025-05-13 14:41:37             This is a deprecated feature. It will be removed
2025-05-13 14:41:37             entirely in future versions of Flower.
2025-05-13 14:41:37         
2025-05-13 14:41:45 INFO :      
2025-05-13 14:41:45 INFO :      Received: train message e93be3b8-3a14-47a6-868f-1d147cad52a0
2025-05-13 14:50:35 INFO :      Sent reply
2025-05-13 14:50:47 INFO :      
2025-05-13 14:50:47 INFO :      Received: train message e9375265-6500-44fa-bc5b-30b186d9f45f
2025-05-13 14:59:29 INFO :      Sent reply
2025-05-13 14:59:41 INFO :      
2025-05-13 14:59:41 INFO :      Received: train message cc6f313f-196d-4706-b9d9-b9916f6fdf5f
2025-05-13 14:42:01 {'loss': 2.6406, 'grad_norm': 15.826362609863281, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 14:42:11 {'loss': 1.4202, 'grad_norm': 11.653894424438477, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 14:42:20 {'loss': 1.6113, 'grad_norm': 10.886569023132324, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 14:42:30 {'loss': 1.6039, 'grad_norm': 12.441505432128906, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 14:42:43 {'loss': 1.5624, 'grad_norm': 14.330039024353027, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 14:42:53 {'loss': 1.517, 'grad_norm': 14.742879867553711, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 14:43:03 {'loss': 1.6006, 'grad_norm': 13.867490768432617, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 14:43:12 {'loss': 1.3823, 'grad_norm': 11.059622764587402, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 14:43:22 {'loss': 1.3799, 'grad_norm': 13.0650634765625, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 14:43:36 {'loss': 1.513, 'grad_norm': 11.348529815673828, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 14:43:45 {'loss': 1.4763, 'grad_norm': 21.940719604492188, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 14:43:55 {'loss': 1.5061, 'grad_norm': 16.843303680419922, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 14:44:05 {'loss': 1.3799, 'grad_norm': 12.786792755126953, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 14:44:19 {'loss': 1.4791, 'grad_norm': 11.11572551727295, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 14:44:28 {'loss': 1.7824, 'grad_norm': 11.26890754699707, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 14:44:38 {'loss': 1.5202, 'grad_norm': 13.413585662841797, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 14:44:48 {'loss': 1.3751, 'grad_norm': 12.63166618347168, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 14:45:01 {'loss': 1.4001, 'grad_norm': 12.540765762329102, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 14:45:11 {'loss': 1.4564, 'grad_norm': 12.428918838500977, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 14:45:21 {'loss': 1.5456, 'grad_norm': 15.021730422973633, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 14:45:31 {'loss': 1.3542, 'grad_norm': 10.385589599609375, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 14:45:44 {'loss': 1.5346, 'grad_norm': 13.854893684387207, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 14:45:54 {'loss': 1.5689, 'grad_norm': 13.029830932617188, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 14:46:04 {'loss': 1.3063, 'grad_norm': 10.114571571350098, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 14:46:13 {'loss': 1.3659, 'grad_norm': 11.102926254272461, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 14:46:23 {'loss': 1.6042, 'grad_norm': 16.72222900390625, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 14:46:36 {'loss': 1.4371, 'grad_norm': 10.516678810119629, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 14:46:46 {'loss': 1.4524, 'grad_norm': 14.894941329956055, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 14:46:56 {'loss': 1.4196, 'grad_norm': 11.252875328063965, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 14:47:06 {'loss': 1.4238, 'grad_norm': 13.343634605407715, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 14:47:19 {'loss': 1.4611, 'grad_norm': 7.930050849914551, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 14:47:29 {'loss': 1.4366, 'grad_norm': 10.62153434753418, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 14:47:39 {'loss': 1.2099, 'grad_norm': 10.083813667297363, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 14:47:52 {'loss': 1.4072, 'grad_norm': 11.125349998474121, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 14:48:02 {'loss': 1.4537, 'grad_norm': 13.970118522644043, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 14:48:12 {'loss': 1.4776, 'grad_norm': 11.906455039978027, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 14:48:22 {'loss': 1.432, 'grad_norm': 11.724740982055664, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 14:48:35 {'loss': 1.3671, 'grad_norm': 15.211524963378906, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 14:48:45 {'loss': 1.3938, 'grad_norm': 12.805434226989746, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 14:48:55 {'loss': 1.2829, 'grad_norm': 12.038111686706543, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 14:49:08 {'loss': 1.4113, 'grad_norm': 10.657891273498535, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 14:49:18 {'loss': 1.4648, 'grad_norm': 13.49866771697998, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 14:49:28 {'loss': 1.3139, 'grad_norm': 11.978492736816406, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 14:49:38 {'loss': 1.5032, 'grad_norm': 13.495165824890137, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 14:49:51 {'loss': 1.3157, 'grad_norm': 12.83072566986084, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 14:50:01 {'loss': 1.4435, 'grad_norm': 12.629426002502441, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 14:50:10 {'loss': 1.4472, 'grad_norm': 13.414178848266602, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 14:50:20 {'loss': 1.4615, 'grad_norm': 10.407258033752441, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 14:50:30 {'loss': 1.5338, 'grad_norm': 12.072859764099121, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 14:50:32 {'loss': 1.3275, 'grad_norm': 11.417728424072266, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 14:50:32 {'train_runtime': 525.9977, 'train_samples_per_second': 1.901, 'train_steps_per_second': 0.951, 'train_loss': 1.4752679920196534, 'epoch': 1.0}
2025-05-13 14:51:01 {'loss': 0.9327, 'grad_norm': 8.662952423095703, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 14:51:10 {'loss': 0.8633, 'grad_norm': 8.900148391723633, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 14:51:24 {'loss': 1.0781, 'grad_norm': 9.275063514709473, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 14:51:34 {'loss': 1.0318, 'grad_norm': 9.791115760803223, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 14:51:43 {'loss': 1.0413, 'grad_norm': 9.268254280090332, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 14:51:53 {'loss': 1.04, 'grad_norm': 9.955005645751953, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 14:52:03 {'loss': 1.1196, 'grad_norm': 10.099813461303711, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 14:52:16 {'loss': 0.9711, 'grad_norm': 9.101719856262207, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 14:52:26 {'loss': 0.9895, 'grad_norm': 10.537009239196777, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 14:52:35 {'loss': 1.0496, 'grad_norm': 13.073025703430176, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 14:52:45 {'loss': 1.0868, 'grad_norm': 12.178668022155762, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 14:52:54 {'loss': 1.1016, 'grad_norm': 14.6608304977417, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 14:53:08 {'loss': 0.9823, 'grad_norm': 10.320528030395508, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 14:53:17 {'loss': 1.0855, 'grad_norm': 8.149213790893555, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 14:53:27 {'loss': 1.3103, 'grad_norm': 9.85066032409668, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 14:53:37 {'loss': 1.1226, 'grad_norm': 11.52584171295166, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 14:53:47 {'loss': 1.029, 'grad_norm': 10.501608848571777, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 14:54:00 {'loss': 1.0817, 'grad_norm': 10.987720489501953, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 14:54:10 {'loss': 1.0992, 'grad_norm': 10.347333908081055, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 14:54:20 {'loss': 1.1612, 'grad_norm': 11.219408988952637, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 14:54:29 {'loss': 1.0429, 'grad_norm': 9.96310043334961, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 14:54:39 {'loss': 1.2231, 'grad_norm': 17.167922973632812, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 14:54:52 {'loss': 1.2346, 'grad_norm': 11.5424165725708, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 14:55:02 {'loss': 1.0044, 'grad_norm': 8.765742301940918, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 14:55:11 {'loss': 1.0544, 'grad_norm': 10.353690147399902, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 14:55:21 {'loss': 1.2645, 'grad_norm': 15.165907859802246, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 14:55:30 {'loss': 1.1276, 'grad_norm': 13.726204872131348, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 14:55:40 {'loss': 1.1574, 'grad_norm': 11.595292091369629, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 14:55:53 {'loss': 1.1412, 'grad_norm': 11.438504219055176, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 14:56:03 {'loss': 1.1707, 'grad_norm': 10.635263442993164, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 14:56:12 {'loss': 1.217, 'grad_norm': 7.344043254852295, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 14:56:22 {'loss': 1.1799, 'grad_norm': 7.549124717712402, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 14:56:32 {'loss': 0.9964, 'grad_norm': 9.266679763793945, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 14:56:45 {'loss': 1.1514, 'grad_norm': 9.797876358032227, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 14:56:55 {'loss': 1.2238, 'grad_norm': 11.389808654785156, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 14:57:04 {'loss': 1.2509, 'grad_norm': 9.468233108520508, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 14:57:14 {'loss': 1.249, 'grad_norm': 12.348791122436523, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 14:57:23 {'loss': 1.202, 'grad_norm': 12.41085147857666, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 14:57:37 {'loss': 1.2208, 'grad_norm': 12.653020858764648, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 14:57:46 {'loss': 1.1306, 'grad_norm': 11.542426109313965, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 14:57:56 {'loss': 1.2723, 'grad_norm': 11.522480964660645, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 14:58:06 {'loss': 1.3097, 'grad_norm': 12.206839561462402, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 14:58:16 {'loss': 1.2164, 'grad_norm': 10.725909233093262, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 14:58:29 {'loss': 1.4003, 'grad_norm': 13.494538307189941, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 14:58:38 {'loss': 1.1897, 'grad_norm': 11.64902114868164, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 14:58:48 {'loss': 1.3378, 'grad_norm': 11.476890563964844, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 14:58:58 {'loss': 1.3618, 'grad_norm': 12.115304946899414, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 14:59:07 {'loss': 1.2969, 'grad_norm': 9.975580215454102, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 14:59:21 {'loss': 1.2088, 'grad_norm': 9.650527954101562, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 14:59:27 {'loss': 0.8194, 'grad_norm': 7.641706466674805, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 14:59:27 {'train_runtime': 518.7742, 'train_samples_per_second': 1.928, 'train_steps_per_second': 0.964, 'train_loss': 1.1366525821685791, 'epoch': 1.0}
2025-05-13 14:59:54 {'loss': 0.5892, 'grad_norm': 8.040486335754395, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 15:00:04 {'loss': 0.5967, 'grad_norm': 12.291534423828125, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 15:00:13 {'loss': 0.7596, 'grad_norm': 7.883657932281494, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 15:00:27 {'loss': 0.7155, 'grad_norm': 8.015192985534668, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 15:00:36 {'loss': 0.7522, 'grad_norm': 12.02138614654541, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 15:00:46 {'loss': 0.7318, 'grad_norm': 10.371251106262207, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 15:00:56 {'loss': 0.8307, 'grad_norm': 10.611231803894043, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 15:01:05 {'loss': 0.7187, 'grad_norm': 7.2831621170043945, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 15:01:19 {'loss': 0.7576, 'grad_norm': 15.245073318481445, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 15:01:28 {'loss': 0.7814, 'grad_norm': 8.571016311645508, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 15:01:38 {'loss': 0.8127, 'grad_norm': 12.352163314819336, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 15:01:48 {'loss': 0.8238, 'grad_norm': 13.927971839904785, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 15:02:01 {'loss': 0.7532, 'grad_norm': 11.428662300109863, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 15:02:11 {'loss': 0.8121, 'grad_norm': 7.339838981628418, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 15:02:20 {'loss': 1.0437, 'grad_norm': 9.289861679077148, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 15:02:30 {'loss': 0.8575, 'grad_norm': 10.620539665222168, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 15:02:39 {'loss': 0.768, 'grad_norm': 9.388891220092773, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 15:02:52 {'loss': 0.841, 'grad_norm': 154.8362579345703, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 15:03:02 {'loss': 0.875, 'grad_norm': 12.789351463317871, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 15:03:12 {'loss': 0.934, 'grad_norm': 11.221269607543945, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 15:03:21 {'loss': 0.8193, 'grad_norm': 8.990822792053223, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 15:03:31 {'loss': 0.9822, 'grad_norm': 13.453435897827148, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 15:03:44 {'loss': 1.0036, 'grad_norm': 11.216054916381836, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 15:03:54 {'loss': 0.8182, 'grad_norm': 7.842051982879639, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 15:04:04 {'loss': 0.8711, 'grad_norm': 11.390817642211914, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 15:04:13 {'loss': 1.0517, 'grad_norm': 15.359107971191406, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 15:04:23 {'loss': 0.9315, 'grad_norm': 9.79954719543457, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 15:04:37 {'loss': 0.9624, 'grad_norm': 10.225931167602539, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 15:04:46 {'loss': 0.9879, 'grad_norm': 9.731544494628906, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 15:04:56 {'loss': 0.9891, 'grad_norm': 9.416932106018066, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 15:05:06 {'loss': 1.0315, 'grad_norm': 7.522188663482666, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 15:05:16 {'loss': 1.0211, 'grad_norm': 7.636017799377441, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 15:05:29 {'loss': 0.8526, 'grad_norm': 9.745434761047363, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 15:05:38 {'loss': 0.9951, 'grad_norm': 9.612683296203613, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 15:05:48 {'loss': 1.0886, 'grad_norm': 12.17374038696289, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 15:05:57 {'loss': 1.1205, 'grad_norm': 10.05066204071045, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 15:06:11 {'loss': 1.1266, 'grad_norm': 12.560768127441406, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 15:06:20 {'loss': 1.074, 'grad_norm': 14.275960922241211, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 15:06:30 {'loss': 1.1138, 'grad_norm': 13.743253707885742, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 15:06:39 {'loss': 1.041, 'grad_norm': 17.003190994262695, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 15:06:49 {'loss': 1.171, 'grad_norm': 10.75892448425293, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 15:07:03 {'loss': 1.2341, 'grad_norm': 12.242895126342773, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 15:07:12 {'loss': 1.1366, 'grad_norm': 10.934354782104492, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 15:07:22 {'loss': 1.3285, 'grad_norm': 14.567307472229004, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 15:07:32 {'loss': 1.1518, 'grad_norm': 12.851701736450195, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 15:07:41 {'loss': 1.2778, 'grad_norm': 10.71049690246582, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 15:07:54 {'loss': 1.2968, 'grad_norm': 12.804234504699707, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 15:08:04 {'loss': 1.2304, 'grad_norm': 8.859628677368164, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 15:08:14 {'loss': 1.0893, 'grad_norm': 10.866535186767578, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 15:08:19 {'loss': 0.64, 'grad_norm': 8.580607414245605, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 15:08:19 {'train_runtime': 517.6837, 'train_samples_per_second': 1.932, 'train_steps_per_second': 0.966, 'train_loss': 0.9438495073318481, 'epoch': 1.0}
2025-05-13 15:08:46 {'loss': 0.3874, 'grad_norm': 5.961780548095703, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 15:08:56 {'loss': 0.3927, 'grad_norm': 8.9060697555542, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 15:09:06 {'loss': 0.5348, 'grad_norm': 9.058476448059082, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 15:09:19 {'loss': 0.4989, 'grad_norm': 7.162652015686035, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 15:09:28 {'loss': 0.5314, 'grad_norm': 13.734942436218262, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 15:09:38 {'loss': 0.5381, 'grad_norm': 9.254154205322266, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 15:09:48 {'loss': 0.5984, 'grad_norm': 8.027482986450195, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 15:09:57 {'loss': 0.5287, 'grad_norm': 7.441176414489746, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 15:10:10 {'loss': 0.5746, 'grad_norm': 9.467260360717773, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 15:10:20 {'loss': 0.571, 'grad_norm': 8.43510627746582, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 15:10:30 {'loss': 0.6154, 'grad_norm': 9.425620079040527, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 15:10:39 {'loss': 0.6465, 'grad_norm': 13.758532524108887, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 15:10:49 {'loss': 0.5771, 'grad_norm': 10.09559440612793, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 15:11:02 {'loss': 0.6362, 'grad_norm': 7.695659637451172, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 15:11:11 {'loss': 0.7958, 'grad_norm': 10.639273643493652, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 15:11:21 {'loss': 0.6346, 'grad_norm': 10.86381721496582, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 15:11:31 {'loss': 0.6014, 'grad_norm': 9.63518238067627, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 15:11:40 {'loss': 0.6815, 'grad_norm': 10.626628875732422, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 15:11:54 {'loss': 0.6781, 'grad_norm': 9.595121383666992, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 15:12:03 {'loss': 0.7419, 'grad_norm': 12.194238662719727, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 15:12:13 {'loss': 0.6492, 'grad_norm': 7.40604305267334, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 15:12:23 {'loss': 0.7782, 'grad_norm': 12.860681533813477, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 15:12:36 {'loss': 0.7975, 'grad_norm': 11.010946273803711, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 15:12:46 {'loss': 0.663, 'grad_norm': 8.390827178955078, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 15:12:55 {'loss': 0.7102, 'grad_norm': 9.787955284118652, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 15:13:05 {'loss': 0.8362, 'grad_norm': 13.97655963897705, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 15:13:15 {'loss': 0.7587, 'grad_norm': 8.47465991973877, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 15:13:24 {'loss': 0.7962, 'grad_norm': 10.879525184631348, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 15:13:38 {'loss': 0.8377, 'grad_norm': 12.519659042358398, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 15:13:47 {'loss': 0.8614, 'grad_norm': 11.278242111206055, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 15:13:57 {'loss': 0.8855, 'grad_norm': 7.185699462890625, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 15:14:06 {'loss': 0.8837, 'grad_norm': 7.601963996887207, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 15:14:20 {'loss': 0.7349, 'grad_norm': 9.225814819335938, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 15:14:29 {'loss': 0.8666, 'grad_norm': 9.116022109985352, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 15:14:39 {'loss': 0.961, 'grad_norm': 12.189339637756348, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 15:14:48 {'loss': 1.0031, 'grad_norm': 9.917431831359863, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 15:15:02 {'loss': 1.0079, 'grad_norm': 10.769201278686523, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 15:15:11 {'loss': 0.995, 'grad_norm': 14.05954360961914, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 15:15:21 {'loss': 1.0399, 'grad_norm': 13.097947120666504, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 15:15:30 {'loss': 0.968, 'grad_norm': 12.868370056152344, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 15:15:40 {'loss': 1.097, 'grad_norm': 11.314685821533203, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 15:15:53 {'loss': 1.1898, 'grad_norm': 10.65353775024414, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 15:16:03 {'loss': 1.0928, 'grad_norm': 10.265036582946777, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 15:16:12 {'loss': 1.2903, 'grad_norm': 13.871970176696777, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 15:16:22 {'loss': 1.1011, 'grad_norm': 12.432778358459473, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 15:16:32 {'loss': 1.2478, 'grad_norm': 12.187335968017578, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 15:16:45 {'loss': 1.2389, 'grad_norm': 12.196396827697754, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 15:16:55 {'loss': 1.1901, 'grad_norm': 10.803757667541504, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 15:17:04 {'loss': 1.0053, 'grad_norm': 10.800199508666992, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 15:17:10 {'loss': 0.5032, 'grad_norm': 8.289101600646973, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 15:17:10 {'train_runtime': 515.6373, 'train_samples_per_second': 1.939, 'train_steps_per_second': 0.97, 'train_loss': 0.7950914545059204, 'epoch': 1.0}
2025-05-13 15:17:39 {'loss': 0.2499, 'grad_norm': 4.811699867248535, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 15:17:48 {'loss': 0.2719, 'grad_norm': 11.259329795837402, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 15:18:02 {'loss': 0.377, 'grad_norm': 8.454113960266113, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 15:18:12 {'loss': 0.369, 'grad_norm': 6.147116661071777, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 15:18:21 {'loss': 0.399, 'grad_norm': 9.333168029785156, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 15:18:31 {'loss': 0.4244, 'grad_norm': 8.63477611541748, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 15:18:45 {'loss': 0.4401, 'grad_norm': 7.668605804443359, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 15:18:54 {'loss': 0.3833, 'grad_norm': 5.556540489196777, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 15:19:04 {'loss': 0.4163, 'grad_norm': 8.595341682434082, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 15:19:14 {'loss': 0.4116, 'grad_norm': 7.513125896453857, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 15:19:24 {'loss': 0.4663, 'grad_norm': 10.367003440856934, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 15:19:37 {'loss': 0.4585, 'grad_norm': 13.433953285217285, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 15:19:47 {'loss': 0.4249, 'grad_norm': 10.126751899719238, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 15:19:56 {'loss': 0.4645, 'grad_norm': 8.60244369506836, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 15:20:06 {'loss': 0.6338, 'grad_norm': 9.131573677062988, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 15:20:16 {'loss': 0.4775, 'grad_norm': 8.720995903015137, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 15:20:29 {'loss': 0.4645, 'grad_norm': 8.199263572692871, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 15:20:39 {'loss': 0.5187, 'grad_norm': 10.718117713928223, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 15:20:48 {'loss': 0.5633, 'grad_norm': 11.205399513244629, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 15:20:58 {'loss': 0.6044, 'grad_norm': 9.41037368774414, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 15:21:08 {'loss': 0.4929, 'grad_norm': 6.964356422424316, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 15:21:17 {'loss': 0.6147, 'grad_norm': 10.731378555297852, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 15:21:31 {'loss': 0.6542, 'grad_norm': 11.335256576538086, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 15:21:40 {'loss': 0.5218, 'grad_norm': 6.530417442321777, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 15:21:50 {'loss': 0.572, 'grad_norm': 8.596269607543945, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 15:22:00 {'loss': 0.6875, 'grad_norm': 14.900081634521484, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 15:22:09 {'loss': 0.6297, 'grad_norm': 8.697381973266602, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 15:22:23 {'loss': 0.6793, 'grad_norm': 10.221420288085938, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 15:22:32 {'loss': 0.724, 'grad_norm': 11.195914268493652, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 15:22:42 {'loss': 0.7291, 'grad_norm': 9.129889488220215, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 15:22:52 {'loss': 0.782, 'grad_norm': 7.375555038452148, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 15:23:02 {'loss': 0.7647, 'grad_norm': 7.804066181182861, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 15:23:15 {'loss': 0.624, 'grad_norm': 9.306962013244629, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 15:23:25 {'loss': 0.7624, 'grad_norm': 9.349764823913574, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 15:23:34 {'loss': 0.8568, 'grad_norm': 12.458467483520508, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 15:23:44 {'loss': 0.8963, 'grad_norm': 9.1736421585083, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 15:23:54 {'loss': 0.926, 'grad_norm': 11.421844482421875, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 15:24:03 {'loss': 0.9048, 'grad_norm': 13.958579063415527, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 15:24:17 {'loss': 0.9647, 'grad_norm': 13.983939170837402, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 15:24:26 {'loss': 0.8823, 'grad_norm': 12.207420349121094, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 15:24:36 {'loss': 1.0399, 'grad_norm': 11.408543586730957, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 15:24:46 {'loss': 1.1092, 'grad_norm': 9.958736419677734, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 15:24:55 {'loss': 1.0297, 'grad_norm': 10.53394603729248, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 15:25:09 {'loss': 1.2284, 'grad_norm': 15.171849250793457, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 15:08:22 INFO :      Sent reply
2025-05-13 15:08:33 INFO :      
2025-05-13 15:08:33 INFO :      Received: train message a4b42bc0-699d-407f-b920-931b51bfca47
2025-05-13 15:17:13 INFO :      Sent reply
2025-05-13 15:17:24 INFO :      
2025-05-13 15:17:24 INFO :      Received: train message 1602b75c-a90b-42af-95fd-53f2dda76693
2025-05-13 15:26:09 INFO :      Sent reply
2025-05-13 15:26:15 INFO :      
2025-05-13 15:26:15 INFO :      Received: reconnect message 4fe845bb-6a2d-49d0-acfb-c58e3573299d
2025-05-13 15:26:15 INFO :      Disconnect and shut down
2025-05-13 15:25:18 {'loss': 1.085, 'grad_norm': 13.280379295349121, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 15:25:28 {'loss': 1.2454, 'grad_norm': 12.089530944824219, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 15:25:37 {'loss': 1.2305, 'grad_norm': 13.275344848632812, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 15:25:47 {'loss': 1.1474, 'grad_norm': 10.407732963562012, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 15:26:00 {'loss': 0.8934, 'grad_norm': 9.367257118225098, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 15:26:06 {'loss': 0.4034, 'grad_norm': 6.96378231048584, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 15:26:06 {'train_runtime': 521.5252, 'train_samples_per_second': 1.917, 'train_steps_per_second': 0.959, 'train_loss': 0.6780060896873474, 'epoch': 1.0}
