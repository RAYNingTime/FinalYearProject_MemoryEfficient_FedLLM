2025-05-13 18:18:57 client1-1  | 
Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]
Generating train split: 100%|██████████| 120000/120000 [00:00<00:00, 1199106.31 examples/s]
2025-05-13 18:18:36 client2-1  | 
Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]
Generating train split:  73%|███████▎  | 88000/120000 [00:00<00:00, 873873.33 examples/s]
Generating train split: 100%|██████████| 120000/120000 [00:00<00:00, 986019.24 examples/s]
2025-05-13 18:18:57 client1-1  | 
Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]
Generating test split: 100%|██████████| 7600/7600 [00:00<00:00, 1014341.96 examples/s]
2025-05-13 18:18:59 client1-1  | 
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 1273.63 examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 1268.29 examples/s]
2025-05-13 18:19:00 client1-1  | 
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]
Map:  18%|█▊        | 178/1000 [00:00<00:00, 1753.36 examples/s]
Map:  37%|███▋      | 367/1000 [00:00<00:00, 1822.20 examples/s]
Map:  59%|█████▊    | 586/1000 [00:00<00:00, 1984.11 examples/s]
Map:  89%|████████▉ | 889/1000 [00:00<00:00, 1994.04 examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 1878.07 examples/s]
2025-05-13 18:19:00 client1-1  | /app/client.py:45: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
2025-05-13 18:19:00 client1-1  |   trainer = Trainer(
2025-05-13 18:19:00 client1-1  | WARNING :   DEPRECATED FEATURE: flwr.client.start_numpy_client() is deprecated. 
2025-05-13 18:19:00 client1-1  | Instead, use `flwr.client.start_client()` by ensuring you first call the `.to_client()` method as shown below: 
2025-05-13 18:18:36 client2-1  | 
Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]
Generating test split: 100%|██████████| 7600/7600 [00:00<00:00, 922171.74 examples/s]
2025-05-13 18:17:13 server-1   | WARNING :   DEPRECATED FEATURE: flwr.server.start_server() is deprecated.
2025-05-13 18:17:13 server-1   | Instead, use the `flower-superlink` CLI command to start a SuperLink as shown below:
2025-05-13 18:17:13 server-1   | 
2025-05-13 18:18:38 client2-1  | 
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 1225.80 examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 1220.19 examples/s]
2025-05-13 18:18:39 client2-1  | 
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]
Map:  18%|█▊        | 184/1000 [00:00<00:00, 1815.07 examples/s]
Map:  53%|█████▎    | 532/1000 [00:00<00:00, 2779.93 examples/s]
Map:  87%|████████▋ | 874/1000 [00:00<00:00, 2104.32 examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 1956.56 examples/s]
2025-05-13 18:18:39 client2-1  | /app/client.py:45: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
2025-05-13 18:18:39 client2-1  |   trainer = Trainer(
2025-05-13 18:18:40 client2-1  | WARNING :   DEPRECATED FEATURE: flwr.client.start_numpy_client() is deprecated. 
2025-05-13 18:19:00 client1-1  | flwr.client.start_client(
2025-05-13 18:18:40 client2-1  | Instead, use `flwr.client.start_client()` by ensuring you first call the `.to_client()` method as shown below: 
2025-05-13 18:19:00 client1-1  | server_address='<IP>:<PORT>',
2025-05-13 18:18:40 client2-1  | flwr.client.start_client(
2025-05-13 18:17:13 server-1   | $ flower-superlink --insecure
2025-05-13 18:17:13 server-1   | 
2025-05-13 18:17:13 server-1   | To view usage and all available options, run:
2025-05-13 18:17:13 server-1   | 
2025-05-13 18:18:40 client2-1  | server_address='<IP>:<PORT>',
2025-05-13 18:18:40 client2-1  | client=FlowerClient().to_client(), # <-- where FlowerClient is of type flwr.client.NumPyClient object
2025-05-13 18:18:40 client2-1  | )
2025-05-13 18:18:40 client2-1  | Using `start_numpy_client()` is deprecated.
2025-05-13 18:17:13 server-1   | $ flower-superlink --help
2025-05-13 18:17:13 server-1   | 
2025-05-13 18:18:40 client2-1  | 
2025-05-13 18:17:13 server-1   | Using `start_server()` is deprecated.
2025-05-13 18:18:40 client2-1  |             This is a deprecated feature. It will be removed
2025-05-13 18:17:13 server-1   | 
2025-05-13 18:19:00 client1-1  | client=FlowerClient().to_client(), # <-- where FlowerClient is of type flwr.client.NumPyClient object
2025-05-13 18:17:13 server-1   |             This is a deprecated feature. It will be removed
2025-05-13 18:18:40 client2-1  |             entirely in future versions of Flower.
2025-05-13 18:17:13 server-1   |             entirely in future versions of Flower.
2025-05-13 18:18:40 client2-1  |         
2025-05-13 18:18:40 client2-1  | WARNING :   DEPRECATED FEATURE: flwr.client.start_client() is deprecated.
2025-05-13 18:17:13 server-1   |         
2025-05-13 18:17:13 server-1   | INFO :      Starting Flower server, config: num_rounds=25, no round_timeout
2025-05-13 18:17:13 server-1   | INFO :      Flower ECE: gRPC server running (25 rounds), SSL is disabled
2025-05-13 18:19:00 client1-1  | )
2025-05-13 18:17:13 server-1   | INFO :      [INIT]
2025-05-13 18:18:40 client2-1  | Instead, use the `flower-supernode` CLI command to start a SuperNode as shown below:
2025-05-13 18:18:40 client2-1  | 
2025-05-13 18:17:13 server-1   | INFO :      Requesting initial parameters from one random client
2025-05-13 18:18:40 client2-1  | $ flower-supernode --insecure --superlink='<IP>:<PORT>'
2025-05-13 18:18:40 client2-1  | 
2025-05-13 18:18:44 server-1   | INFO :      Received initial parameters from one random client
2025-05-13 18:18:44 server-1   | INFO :      Starting evaluation of initial global parameters
2025-05-13 18:18:40 client2-1  | To view all available options, run:
2025-05-13 18:18:40 client2-1  | 
2025-05-13 18:19:00 client1-1  | Using `start_numpy_client()` is deprecated.
2025-05-13 18:19:00 client1-1  | 
2025-05-13 18:19:00 client1-1  |             This is a deprecated feature. It will be removed
2025-05-13 18:19:00 client1-1  |             entirely in future versions of Flower.
2025-05-13 18:19:00 client1-1  |         
2025-05-13 18:19:00 client1-1  | WARNING :   DEPRECATED FEATURE: flwr.client.start_client() is deprecated.
2025-05-13 18:18:44 server-1   | INFO :      Evaluation returned no results (`None`)
2025-05-13 18:19:00 client1-1  | Instead, use the `flower-supernode` CLI command to start a SuperNode as shown below:
2025-05-13 18:18:40 client2-1  | $ flower-supernode --help
2025-05-13 18:18:40 client2-1  | 
2025-05-13 18:18:40 client2-1  | Using `start_client()` is deprecated.
2025-05-13 18:18:40 client2-1  | 
2025-05-13 18:18:44 server-1   | INFO :      
2025-05-13 18:18:44 server-1   | INFO :      [ROUND 1]
2025-05-13 18:19:00 server-1   | INFO :      configure_fit: strategy sampled 2 clients (out of 2)
2025-05-13 18:27:55 server-1   | INFO :      aggregate_fit: received 2 results and 0 failures
2025-05-13 18:18:40 client2-1  |             This is a deprecated feature. It will be removed
2025-05-13 18:28:01 server-1   | WARNING :   No fit_metrics_aggregation_fn provided
2025-05-13 18:18:40 client2-1  |             entirely in future versions of Flower.
2025-05-13 18:28:01 server-1   | INFO :      configure_evaluate: no clients selected, skipping evaluation
2025-05-13 18:19:00 client1-1  | 
2025-05-13 18:28:01 server-1   | INFO :      
2025-05-13 18:28:01 server-1   | INFO :      [ROUND 2]
2025-05-13 18:28:01 server-1   | INFO :      configure_fit: strategy sampled 2 clients (out of 2)
2025-05-13 18:36:39 server-1   | INFO :      aggregate_fit: received 2 results and 0 failures
2025-05-13 18:36:45 server-1   | INFO :      configure_evaluate: no clients selected, skipping evaluation
2025-05-13 18:19:00 client1-1  | $ flower-supernode --insecure --superlink='<IP>:<PORT>'
2025-05-13 18:19:00 client1-1  | 
2025-05-13 18:19:00 client1-1  | To view all available options, run:
2025-05-13 18:19:00 client1-1  | 
2025-05-13 18:19:00 client1-1  | $ flower-supernode --help
2025-05-13 18:19:00 client1-1  | 
2025-05-13 18:19:00 client1-1  | Using `start_client()` is deprecated.
2025-05-13 18:19:00 client1-1  | 
2025-05-13 18:19:00 client1-1  |             This is a deprecated feature. It will be removed
2025-05-13 18:19:00 client1-1  |             entirely in future versions of Flower.
2025-05-13 18:36:45 server-1   | INFO :      
2025-05-13 18:19:00 client1-1  |         
2025-05-13 18:19:08 client1-1  | INFO :      
2025-05-13 18:19:08 client1-1  | INFO :      Received: train message 426aa205-cc0e-4279-b317-66a8726d0db1
2025-05-13 18:19:23 client1-1  | {'loss': 2.6406, 'grad_norm': 15.826362609863281, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 18:19:32 client1-1  | {'loss': 1.4202, 'grad_norm': 11.653894424438477, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 18:19:46 client1-1  | {'loss': 1.6113, 'grad_norm': 10.886569023132324, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 18:19:56 client1-1  | {'loss': 1.6039, 'grad_norm': 12.441505432128906, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 18:20:05 client1-1  | {'loss': 1.5624, 'grad_norm': 14.330039024353027, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 18:20:15 client1-1  | {'loss': 1.517, 'grad_norm': 14.742879867553711, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 18:20:25 client1-1  | {'loss': 1.6006, 'grad_norm': 13.867490768432617, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 18:20:34 client1-1  | {'loss': 1.3823, 'grad_norm': 11.059622764587402, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 18:20:44 client1-1  | {'loss': 1.3799, 'grad_norm': 13.0650634765625, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 18:36:45 server-1   | INFO :      [ROUND 3]
2025-05-13 18:36:45 server-1   | INFO :      configure_fit: strategy sampled 2 clients (out of 2)
2025-05-13 18:20:44 client1-1  | Error while downloading from https://cdn-lfs.hf.co/repos/2d/51/2d51352256ba577724ec53b247175bd0928dfc5387e98f45a2f3eab954c26eaf/9da6494f3af047e5e96ad93912f347aafed1bf6be00e03751b2db0d6e927eca1?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1747152433&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NzE1MjQzM319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy8yZC81MS8yZDUxMzUyMjU2YmE1Nzc3MjRlYzUzYjI0NzE3NWJkMDkyOGRmYzUzODdlOThmNDVhMmYzZWFiOTU0YzI2ZWFmLzlkYTY0OTRmM2FmMDQ3ZTVlOTZhZDkzOTEyZjM0N2FhZmVkMWJmNmJlMDBlMDM3NTFiMmRiMGQ2ZTkyN2VjYTE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=gSnEACNdgvfizczrcuBBItE%7E%7Eo75AcbTUvEB9jkAfG%7Ew5%7E1Xci88ONmBFmkoVRb8H7dnlYQ4NbTIJAF9fCcGpEkCtggP2g8e9K5sMM7Shsn%7E%7EfWbbiQRplV9V5m4rMHF0EsZo9RSUj%7EYX9QsmWwgKVdP6ilGdAJzeBnE0UJB1HHIpzbPBioZBuITUFqbj75-C-Ssn5CrEIVM2IaYJRpbySW9Vg1jo7JKMCDfseyO3kTqGxdCbOqpFZZQvEXwi5NBF9hdXyigs50D0WUAMmZdu0-QMN8hr5l1mnMnMslCMRXhNeY2EGrumCN8uuPtQOSQ-UaLLasdu53iix9meSvBSw__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.
2025-05-13 18:20:44 client1-1  | Trying to resume download...
2025-05-13 18:45:24 server-1   | INFO :      aggregate_fit: received 2 results and 0 failures
2025-05-13 18:20:57 client1-1  | {'loss': 1.513, 'grad_norm': 11.348529815673828, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 18:18:40 client2-1  |         
2025-05-13 18:18:40 client2-1  | INFO :      
2025-05-13 18:21:07 client1-1  | {'loss': 1.4763, 'grad_norm': 21.940719604492188, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 18:21:16 client1-1  | {'loss': 1.5061, 'grad_norm': 16.843303680419922, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 18:18:40 client2-1  | INFO :      Received: get_parameters message 69a7c8d0-7f92-4e5e-aa54-fdf924226c28
2025-05-13 18:18:43 client2-1  | INFO :      Sent reply
2025-05-13 18:19:08 client2-1  | INFO :      
2025-05-13 18:19:08 client2-1  | INFO :      Received: train message 5450a893-fd7b-49d1-a021-42767e335e93
2025-05-13 18:45:29 server-1   | INFO :      configure_evaluate: no clients selected, skipping evaluation
2025-05-13 18:21:26 client1-1  | {'loss': 1.3799, 'grad_norm': 12.786792755126953, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 18:21:35 client1-1  | {'loss': 1.4791, 'grad_norm': 11.11572551727295, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 18:45:29 server-1   | INFO :      
2025-05-13 18:19:24 client2-1  | {'loss': 2.6476, 'grad_norm': 18.667423248291016, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 18:19:34 client2-1  | {'loss': 1.6533, 'grad_norm': 14.597430229187012, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 18:19:44 client2-1  | {'loss': 1.6346, 'grad_norm': 19.806570053100586, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 18:19:53 client2-1  | {'loss': 1.5147, 'grad_norm': 11.533321380615234, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 18:45:29 server-1   | INFO :      [ROUND 4]
2025-05-13 18:20:03 client2-1  | {'loss': 1.5607, 'grad_norm': 16.239545822143555, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 18:45:29 server-1   | INFO :      configure_fit: strategy sampled 2 clients (out of 2)
2025-05-13 18:54:11 server-1   | INFO :      aggregate_fit: received 2 results and 0 failures
2025-05-13 18:54:16 server-1   | INFO :      configure_evaluate: no clients selected, skipping evaluation
2025-05-13 18:54:16 server-1   | INFO :      
2025-05-13 18:54:16 server-1   | INFO :      [ROUND 5]
2025-05-13 18:20:13 client2-1  | {'loss': 1.4235, 'grad_norm': 14.735560417175293, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 18:54:16 server-1   | INFO :      configure_fit: strategy sampled 2 clients (out of 2)
2025-05-13 19:03:01 server-1   | INFO :      aggregate_fit: received 2 results and 0 failures
2025-05-13 19:03:06 server-1   | INFO :      configure_evaluate: no clients selected, skipping evaluation
2025-05-13 18:21:45 client1-1  | {'loss': 1.7824, 'grad_norm': 11.26890754699707, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 19:03:06 server-1   | INFO :      
2025-05-13 18:20:26 client2-1  | {'loss': 1.3801, 'grad_norm': 11.47992992401123, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 18:20:36 client2-1  | {'loss': 1.4928, 'grad_norm': 12.755566596984863, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 18:20:46 client2-1  | {'loss': 1.2775, 'grad_norm': 13.867303848266602, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 19:03:06 server-1   | INFO :      [ROUND 6]
2025-05-13 18:21:58 client1-1  | {'loss': 1.5202, 'grad_norm': 13.413585662841797, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 18:22:08 client1-1  | {'loss': 1.3751, 'grad_norm': 12.63166618347168, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 18:22:17 client1-1  | {'loss': 1.4001, 'grad_norm': 12.540765762329102, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 18:22:27 client1-1  | {'loss': 1.4564, 'grad_norm': 12.428918838500977, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 18:22:37 client1-1  | {'loss': 1.5456, 'grad_norm': 15.021730422973633, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 19:03:06 server-1   | INFO :      configure_fit: strategy sampled 2 clients (out of 2)
2025-05-13 18:20:55 client2-1  | {'loss': 1.413, 'grad_norm': 13.377055168151855, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 19:11:47 server-1   | INFO :      aggregate_fit: received 2 results and 0 failures
2025-05-13 18:21:05 client2-1  | {'loss': 1.4638, 'grad_norm': 11.14098834991455, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 19:11:52 server-1   | INFO :      configure_evaluate: no clients selected, skipping evaluation
2025-05-13 18:22:46 client1-1  | {'loss': 1.3542, 'grad_norm': 10.385589599609375, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 18:21:14 client2-1  | {'loss': 1.5317, 'grad_norm': 16.719707489013672, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 18:23:00 client1-1  | {'loss': 1.5346, 'grad_norm': 13.854893684387207, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 18:21:24 client2-1  | {'loss': 1.5412, 'grad_norm': 14.699211120605469, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 19:11:52 server-1   | INFO :      
2025-05-13 19:11:52 server-1   | INFO :      [ROUND 7]
2025-05-13 19:11:52 server-1   | INFO :      configure_fit: strategy sampled 2 clients (out of 2)
2025-05-13 18:23:09 client1-1  | {'loss': 1.5689, 'grad_norm': 13.029830932617188, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 19:20:35 server-1   | INFO :      aggregate_fit: received 2 results and 0 failures
2025-05-13 19:20:40 server-1   | INFO :      configure_evaluate: no clients selected, skipping evaluation
2025-05-13 19:20:40 server-1   | INFO :      
2025-05-13 18:21:37 client2-1  | {'loss': 1.34, 'grad_norm': 11.303004264831543, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 18:21:46 client2-1  | {'loss': 1.4992, 'grad_norm': 11.791444778442383, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 18:21:56 client2-1  | {'loss': 1.4179, 'grad_norm': 12.727545738220215, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 18:22:06 client2-1  | {'loss': 1.6515, 'grad_norm': 13.811737060546875, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 19:20:40 server-1   | INFO :      [ROUND 8]
2025-05-13 18:22:15 client2-1  | {'loss': 1.4676, 'grad_norm': 11.954639434814453, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 18:22:25 client2-1  | {'loss': 1.4366, 'grad_norm': 14.865253448486328, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 18:23:19 client1-1  | {'loss': 1.3063, 'grad_norm': 10.114571571350098, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 18:22:39 client2-1  | {'loss': 1.3679, 'grad_norm': 12.745451927185059, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 18:22:49 client2-1  | {'loss': 1.583, 'grad_norm': 13.52482795715332, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 19:20:40 server-1   | INFO :      configure_fit: strategy sampled 2 clients (out of 2)
2025-05-13 18:23:29 client1-1  | {'loss': 1.3659, 'grad_norm': 11.102926254272461, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 19:29:21 server-1   | INFO :      aggregate_fit: received 2 results and 0 failures
2025-05-13 18:23:38 client1-1  | {'loss': 1.6042, 'grad_norm': 16.72222900390625, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 18:23:52 client1-1  | {'loss': 1.4371, 'grad_norm': 10.516678810119629, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 18:24:01 client1-1  | {'loss': 1.4524, 'grad_norm': 14.894941329956055, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 19:29:27 server-1   | INFO :      configure_evaluate: no clients selected, skipping evaluation
2025-05-13 18:24:11 client1-1  | {'loss': 1.4196, 'grad_norm': 11.252875328063965, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 18:24:20 client1-1  | {'loss': 1.4238, 'grad_norm': 13.343634605407715, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 18:24:30 client1-1  | {'loss': 1.4611, 'grad_norm': 7.930050849914551, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 18:24:40 client1-1  | {'loss': 1.4366, 'grad_norm': 10.62153434753418, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 19:29:27 server-1   | INFO :      
2025-05-13 19:29:27 server-1   | INFO :      [ROUND 9]
2025-05-13 19:29:27 server-1   | INFO :      configure_fit: strategy sampled 2 clients (out of 2)
2025-05-13 18:24:53 client1-1  | {'loss': 1.2099, 'grad_norm': 10.083813667297363, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 18:25:03 client1-1  | {'loss': 1.4072, 'grad_norm': 11.125349998474121, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 18:22:58 client2-1  | {'loss': 1.5815, 'grad_norm': 20.672588348388672, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 18:25:12 client1-1  | {'loss': 1.4537, 'grad_norm': 13.970118522644043, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 18:25:22 client1-1  | {'loss': 1.4776, 'grad_norm': 11.906455039978027, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 19:38:04 server-1   | INFO :      aggregate_fit: received 2 results and 0 failures
2025-05-13 19:38:10 server-1   | INFO :      configure_evaluate: no clients selected, skipping evaluation
2025-05-13 18:23:11 client2-1  | {'loss': 1.448, 'grad_norm': 16.254541397094727, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 18:23:21 client2-1  | {'loss': 1.5469, 'grad_norm': 10.708218574523926, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 18:23:31 client2-1  | {'loss': 1.4196, 'grad_norm': 10.960878372192383, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 19:38:10 server-1   | INFO :      
2025-05-13 18:25:32 client1-1  | {'loss': 1.432, 'grad_norm': 11.724740982055664, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 18:25:41 client1-1  | {'loss': 1.3671, 'grad_norm': 15.211524963378906, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 19:38:10 server-1   | INFO :      [ROUND 10]
2025-05-13 19:38:10 server-1   | INFO :      configure_fit: strategy sampled 2 clients (out of 2)
2025-05-13 19:46:50 server-1   | INFO :      aggregate_fit: received 2 results and 0 failures
2025-05-13 19:46:56 server-1   | INFO :      configure_evaluate: no clients selected, skipping evaluation
2025-05-13 18:23:40 client2-1  | {'loss': 1.3361, 'grad_norm': 11.715882301330566, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 18:23:50 client2-1  | {'loss': 1.4753, 'grad_norm': 14.1637544631958, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 18:24:03 client2-1  | {'loss': 1.5778, 'grad_norm': 10.979082107543945, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 18:24:13 client2-1  | {'loss': 1.4089, 'grad_norm': 11.89306926727295, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 18:24:23 client2-1  | {'loss': 1.4044, 'grad_norm': 10.345044136047363, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 19:46:56 server-1   | INFO :      
2025-05-13 19:46:56 server-1   | INFO :      [ROUND 11]
2025-05-13 19:46:56 server-1   | INFO :      configure_fit: strategy sampled 2 clients (out of 2)
2025-05-13 19:55:33 server-1   | INFO :      aggregate_fit: received 2 results and 0 failures
2025-05-13 19:55:38 server-1   | INFO :      configure_evaluate: no clients selected, skipping evaluation
2025-05-13 18:24:32 client2-1  | {'loss': 1.4182, 'grad_norm': 12.881041526794434, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 18:24:42 client2-1  | {'loss': 1.381, 'grad_norm': 12.148990631103516, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 18:25:55 client1-1  | {'loss': 1.3938, 'grad_norm': 12.805434226989746, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 18:26:05 client1-1  | {'loss': 1.2829, 'grad_norm': 12.038111686706543, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 18:26:14 client1-1  | {'loss': 1.4113, 'grad_norm': 10.657891273498535, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 18:26:24 client1-1  | {'loss': 1.4648, 'grad_norm': 13.49866771697998, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 19:55:38 server-1   | INFO :      
2025-05-13 19:55:38 server-1   | INFO :      [ROUND 12]
2025-05-13 19:55:38 server-1   | INFO :      configure_fit: strategy sampled 2 clients (out of 2)
2025-05-13 20:04:16 server-1   | INFO :      aggregate_fit: received 2 results and 0 failures
2025-05-13 20:04:21 server-1   | INFO :      configure_evaluate: no clients selected, skipping evaluation
2025-05-13 20:04:21 server-1   | INFO :      
2025-05-13 20:04:21 server-1   | INFO :      [ROUND 13]
2025-05-13 20:04:21 server-1   | INFO :      configure_fit: strategy sampled 2 clients (out of 2)
2025-05-13 20:13:01 server-1   | INFO :      aggregate_fit: received 2 results and 0 failures
2025-05-13 18:26:33 client1-1  | {'loss': 1.3139, 'grad_norm': 11.978492736816406, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 18:26:43 client1-1  | {'loss': 1.5032, 'grad_norm': 13.495165824890137, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 18:26:52 client1-1  | {'loss': 1.3157, 'grad_norm': 12.83072566986084, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 18:27:06 client1-1  | {'loss': 1.4435, 'grad_norm': 12.629426002502441, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 18:27:15 client1-1  | {'loss': 1.4472, 'grad_norm': 13.414178848266602, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 18:27:25 client1-1  | {'loss': 1.4615, 'grad_norm': 10.407258033752441, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 18:24:56 client2-1  | {'loss': 1.3378, 'grad_norm': 11.297174453735352, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 18:25:05 client2-1  | {'loss': 1.4548, 'grad_norm': 10.685917854309082, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 20:13:06 server-1   | INFO :      configure_evaluate: no clients selected, skipping evaluation
2025-05-13 20:13:06 server-1   | INFO :      
2025-05-13 20:13:06 server-1   | INFO :      [ROUND 14]
2025-05-13 18:25:15 client2-1  | {'loss': 1.4016, 'grad_norm': 11.256150245666504, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 18:25:24 client2-1  | {'loss': 1.2591, 'grad_norm': 13.242323875427246, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 20:13:06 server-1   | INFO :      configure_fit: strategy sampled 2 clients (out of 2)
2025-05-13 18:25:34 client2-1  | {'loss': 1.5684, 'grad_norm': 12.140288352966309, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 18:25:44 client2-1  | {'loss': 1.2984, 'grad_norm': 13.39657974243164, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 20:21:52 server-1   | INFO :      aggregate_fit: received 2 results and 0 failures
2025-05-13 20:21:57 server-1   | INFO :      configure_evaluate: no clients selected, skipping evaluation
2025-05-13 20:21:57 server-1   | INFO :      
2025-05-13 20:21:57 server-1   | INFO :      [ROUND 15]
2025-05-13 18:25:53 client2-1  | {'loss': 1.4199, 'grad_norm': 10.896543502807617, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 20:21:57 server-1   | INFO :      configure_fit: strategy sampled 2 clients (out of 2)
2025-05-13 18:27:35 client1-1  | {'loss': 1.5338, 'grad_norm': 12.072859764099121, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 18:27:45 client1-1  | {'loss': 1.3275, 'grad_norm': 11.417728424072266, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 18:26:07 client2-1  | {'loss': 1.4087, 'grad_norm': 12.410995483398438, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 18:27:45 client1-1  | {'train_runtime': 515.0444, 'train_samples_per_second': 1.942, 'train_steps_per_second': 0.971, 'train_loss': 1.4752679920196534, 'epoch': 1.0}
2025-05-13 18:27:53 client1-1  | INFO :      Sent reply
2025-05-13 20:30:37 server-1   | INFO :      aggregate_fit: received 2 results and 0 failures
2025-05-13 20:30:42 server-1   | INFO :      configure_evaluate: no clients selected, skipping evaluation
2025-05-13 20:30:42 server-1   | INFO :      
2025-05-13 18:28:06 client1-1  | INFO :      
2025-05-13 18:28:06 client1-1  | INFO :      Received: train message 7002bb70-e7a7-4639-89cc-ac0b13fc8520
2025-05-13 20:30:42 server-1   | INFO :      [ROUND 16]
2025-05-13 20:30:42 server-1   | INFO :      configure_fit: strategy sampled 2 clients (out of 2)
2025-05-13 20:39:21 server-1   | INFO :      aggregate_fit: received 2 results and 0 failures
2025-05-13 20:39:26 server-1   | INFO :      configure_evaluate: no clients selected, skipping evaluation
2025-05-13 18:26:16 client2-1  | {'loss': 1.4482, 'grad_norm': 10.810933113098145, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 18:26:26 client2-1  | {'loss': 1.3652, 'grad_norm': 12.634051322937012, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 18:26:35 client2-1  | {'loss': 1.2963, 'grad_norm': 12.784635543823242, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 18:26:45 client2-1  | {'loss': 1.3526, 'grad_norm': 12.187051773071289, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 18:28:21 client1-1  | {'loss': 0.9327, 'grad_norm': 8.662952423095703, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 18:28:30 client1-1  | {'loss': 0.8633, 'grad_norm': 8.900148391723633, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 18:28:40 client1-1  | {'loss': 1.0781, 'grad_norm': 9.275063514709473, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 18:28:50 client1-1  | {'loss': 1.0318, 'grad_norm': 9.791115760803223, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 18:28:59 client1-1  | {'loss': 1.0413, 'grad_norm': 9.268254280090332, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 18:29:12 client1-1  | {'loss': 1.04, 'grad_norm': 9.955005645751953, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 18:29:22 client1-1  | {'loss': 1.1196, 'grad_norm': 10.099813461303711, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 18:29:31 client1-1  | {'loss': 0.9711, 'grad_norm': 9.101719856262207, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 18:29:41 client1-1  | {'loss': 0.9895, 'grad_norm': 10.537009239196777, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 18:29:50 client1-1  | {'loss': 1.0496, 'grad_norm': 13.073025703430176, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 18:30:00 client1-1  | {'loss': 1.0868, 'grad_norm': 12.178668022155762, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 18:30:09 client1-1  | {'loss': 1.1016, 'grad_norm': 14.6608304977417, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 18:30:22 client1-1  | {'loss': 0.9823, 'grad_norm': 10.320528030395508, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 18:30:32 client1-1  | {'loss': 1.0855, 'grad_norm': 8.149213790893555, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 18:30:42 client1-1  | {'loss': 1.3103, 'grad_norm': 9.85066032409668, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 18:30:51 client1-1  | {'loss': 1.1226, 'grad_norm': 11.52584171295166, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 18:26:55 client2-1  | {'loss': 1.2595, 'grad_norm': 11.523948669433594, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 18:27:08 client2-1  | {'loss': 1.3866, 'grad_norm': 11.84868335723877, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 18:27:18 client2-1  | {'loss': 1.5414, 'grad_norm': 16.074962615966797, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 20:39:26 server-1   | INFO :      
2025-05-13 18:31:00 client1-1  | {'loss': 1.029, 'grad_norm': 10.501608848571777, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 20:39:26 server-1   | INFO :      [ROUND 17]
2025-05-13 18:27:27 client2-1  | {'loss': 1.1848, 'grad_norm': 8.580780982971191, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 18:27:37 client2-1  | {'loss': 1.4945, 'grad_norm': 13.382322311401367, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 18:27:47 client2-1  | {'loss': 1.3534, 'grad_norm': 10.782709121704102, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 18:31:10 client1-1  | {'loss': 1.0817, 'grad_norm': 10.987720489501953, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 18:27:47 client2-1  | {'train_runtime': 517.4495, 'train_samples_per_second': 1.933, 'train_steps_per_second': 0.966, 'train_loss': 1.462536033630371, 'epoch': 1.0}
2025-05-13 18:27:54 client2-1  | INFO :      Sent reply
2025-05-13 18:28:05 client2-1  | INFO :      
2025-05-13 18:28:05 client2-1  | INFO :      Received: train message 7104fbc4-d957-4969-a99c-22257f74e3c0
2025-05-13 18:28:19 client2-1  | {'loss': 0.8742, 'grad_norm': 11.3317232131958, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 18:28:28 client2-1  | {'loss': 1.0113, 'grad_norm': 9.381179809570312, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 18:28:38 client2-1  | {'loss': 1.0529, 'grad_norm': 13.850459098815918, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 18:28:51 client2-1  | {'loss': 1.0427, 'grad_norm': 9.224855422973633, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 18:29:01 client2-1  | {'loss': 1.0427, 'grad_norm': 12.175333976745605, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 20:39:26 server-1   | INFO :      configure_fit: strategy sampled 2 clients (out of 2)
2025-05-13 20:48:07 server-1   | INFO :      aggregate_fit: received 2 results and 0 failures
2025-05-13 20:48:13 server-1   | INFO :      configure_evaluate: no clients selected, skipping evaluation
2025-05-13 20:48:13 server-1   | INFO :      
2025-05-13 20:48:13 server-1   | INFO :      [ROUND 18]
2025-05-13 20:48:13 server-1   | INFO :      configure_fit: strategy sampled 2 clients (out of 2)
2025-05-13 20:56:55 server-1   | INFO :      aggregate_fit: received 2 results and 0 failures
2025-05-13 20:57:00 server-1   | INFO :      configure_evaluate: no clients selected, skipping evaluation
2025-05-13 20:57:00 server-1   | INFO :      
2025-05-13 18:31:23 client1-1  | {'loss': 1.0992, 'grad_norm': 10.347333908081055, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 20:57:00 server-1   | INFO :      [ROUND 19]
2025-05-13 20:57:00 server-1   | INFO :      configure_fit: strategy sampled 2 clients (out of 2)
2025-05-13 21:05:42 server-1   | INFO :      aggregate_fit: received 2 results and 0 failures
2025-05-13 21:05:47 server-1   | INFO :      configure_evaluate: no clients selected, skipping evaluation
2025-05-13 18:31:33 client1-1  | {'loss': 1.1612, 'grad_norm': 11.219408988952637, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 18:29:10 client2-1  | {'loss': 1.0267, 'grad_norm': 12.929387092590332, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 18:31:42 client1-1  | {'loss': 1.0429, 'grad_norm': 9.96310043334961, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 18:31:51 client1-1  | {'loss': 1.2231, 'grad_norm': 17.167922973632812, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 18:32:01 client1-1  | {'loss': 1.2346, 'grad_norm': 11.5424165725708, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 18:32:10 client1-1  | {'loss': 1.0044, 'grad_norm': 8.765742301940918, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 18:32:20 client1-1  | {'loss': 1.0544, 'grad_norm': 10.353690147399902, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 21:05:47 server-1   | INFO :      
2025-05-13 18:32:33 client1-1  | {'loss': 1.2645, 'grad_norm': 15.165907859802246, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 21:05:47 server-1   | INFO :      [ROUND 20]
2025-05-13 21:05:47 server-1   | INFO :      configure_fit: strategy sampled 2 clients (out of 2)
2025-05-13 21:14:31 server-1   | INFO :      aggregate_fit: received 2 results and 0 failures
2025-05-13 18:29:20 client2-1  | {'loss': 0.9578, 'grad_norm': 9.577899932861328, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 21:14:36 server-1   | INFO :      configure_evaluate: no clients selected, skipping evaluation
2025-05-13 18:32:42 client1-1  | {'loss': 1.1276, 'grad_norm': 13.726204872131348, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 18:29:29 client2-1  | {'loss': 1.076, 'grad_norm': 12.489730834960938, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 21:14:36 server-1   | INFO :      
2025-05-13 18:32:52 client1-1  | {'loss': 1.1574, 'grad_norm': 11.595292091369629, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 18:33:01 client1-1  | {'loss': 1.1412, 'grad_norm': 11.438504219055176, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 18:33:11 client1-1  | {'loss': 1.1707, 'grad_norm': 10.635263442993164, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 18:33:21 client1-1  | {'loss': 1.217, 'grad_norm': 7.344043254852295, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 18:29:39 client2-1  | {'loss': 0.8949, 'grad_norm': 10.682544708251953, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 18:29:48 client2-1  | {'loss': 0.9827, 'grad_norm': 10.887852668762207, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 21:14:36 server-1   | INFO :      [ROUND 21]
2025-05-13 18:33:30 client1-1  | {'loss': 1.1799, 'grad_norm': 7.549124717712402, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 18:33:39 client1-1  | {'loss': 0.9964, 'grad_norm': 9.266679763793945, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 18:33:52 client1-1  | {'loss': 1.1514, 'grad_norm': 9.797876358032227, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 21:14:36 server-1   | INFO :      configure_fit: strategy sampled 2 clients (out of 2)
2025-05-13 21:23:14 server-1   | INFO :      aggregate_fit: received 2 results and 0 failures
2025-05-13 21:23:19 server-1   | INFO :      configure_evaluate: no clients selected, skipping evaluation
2025-05-13 18:34:02 client1-1  | {'loss': 1.2238, 'grad_norm': 11.389808654785156, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 18:34:11 client1-1  | {'loss': 1.2509, 'grad_norm': 9.468233108520508, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 18:34:21 client1-1  | {'loss': 1.249, 'grad_norm': 12.348791122436523, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 18:34:30 client1-1  | {'loss': 1.202, 'grad_norm': 12.41085147857666, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 18:34:40 client1-1  | {'loss': 1.2208, 'grad_norm': 12.653020858764648, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 18:34:53 client1-1  | {'loss': 1.1306, 'grad_norm': 11.542426109313965, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 18:35:02 client1-1  | {'loss': 1.2723, 'grad_norm': 11.522480964660645, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 18:35:12 client1-1  | {'loss': 1.3097, 'grad_norm': 12.206839561462402, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 18:35:21 client1-1  | {'loss': 1.2164, 'grad_norm': 10.725909233093262, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 18:35:31 client1-1  | {'loss': 1.4003, 'grad_norm': 13.494538307189941, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 18:29:58 client2-1  | {'loss': 1.0428, 'grad_norm': 9.321113586425781, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 18:30:11 client2-1  | {'loss': 1.1082, 'grad_norm': 10.88374137878418, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 18:30:20 client2-1  | {'loss': 1.1416, 'grad_norm': 10.62307357788086, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 21:23:19 server-1   | INFO :      
2025-05-13 21:23:19 server-1   | INFO :      [ROUND 22]
2025-05-13 21:23:19 server-1   | INFO :      configure_fit: strategy sampled 2 clients (out of 2)
2025-05-13 21:32:00 server-1   | INFO :      aggregate_fit: received 2 results and 0 failures
2025-05-13 18:30:30 client2-1  | {'loss': 0.9729, 'grad_norm': 10.5408353805542, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 18:30:39 client2-1  | {'loss': 1.1096, 'grad_norm': 10.407919883728027, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 18:30:49 client2-1  | {'loss': 1.0154, 'grad_norm': 11.872344017028809, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 21:32:05 server-1   | INFO :      configure_evaluate: no clients selected, skipping evaluation
2025-05-13 18:30:58 client2-1  | {'loss': 1.2282, 'grad_norm': 12.938863754272461, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 18:31:12 client2-1  | {'loss': 1.0998, 'grad_norm': 10.847574234008789, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 18:35:40 client1-1  | {'loss': 1.1897, 'grad_norm': 11.64902114868164, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 18:35:54 client1-1  | {'loss': 1.3378, 'grad_norm': 11.476890563964844, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 18:36:03 client1-1  | {'loss': 1.3618, 'grad_norm': 12.115304946899414, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 18:31:21 client2-1  | {'loss': 1.0919, 'grad_norm': 13.29143238067627, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 18:31:31 client2-1  | {'loss': 1.0281, 'grad_norm': 10.967524528503418, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 18:31:40 client2-1  | {'loss': 1.1927, 'grad_norm': 12.349968910217285, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 18:36:13 client1-1  | {'loss': 1.2969, 'grad_norm': 9.975580215454102, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 18:31:49 client2-1  | {'loss': 1.2337, 'grad_norm': 15.08395767211914, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 18:36:22 client1-1  | {'loss': 1.2088, 'grad_norm': 9.650527954101562, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 18:36:32 client1-1  | {'loss': 0.8194, 'grad_norm': 7.641706466674805, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 18:36:32 client1-1  | {'train_runtime': 504.0597, 'train_samples_per_second': 1.984, 'train_steps_per_second': 0.992, 'train_loss': 1.1366525821685791, 'epoch': 1.0}
2025-05-13 18:31:59 client2-1  | {'loss': 1.1219, 'grad_norm': 14.97036361694336, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 18:32:08 client2-1  | {'loss': 1.1968, 'grad_norm': 9.149460792541504, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 18:32:21 client2-1  | {'loss': 1.1042, 'grad_norm': 8.809840202331543, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 21:32:05 server-1   | INFO :      
2025-05-13 21:32:05 server-1   | INFO :      [ROUND 23]
2025-05-13 18:36:39 client1-1  | INFO :      Sent reply
2025-05-13 18:32:31 client2-1  | {'loss': 1.0674, 'grad_norm': 11.411962509155273, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 18:32:40 client2-1  | {'loss': 1.1927, 'grad_norm': 12.24872875213623, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 18:32:50 client2-1  | {'loss': 1.2982, 'grad_norm': 10.290772438049316, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 21:32:05 server-1   | INFO :      configure_fit: strategy sampled 2 clients (out of 2)
2025-05-13 21:40:48 server-1   | INFO :      aggregate_fit: received 2 results and 0 failures
2025-05-13 21:40:53 server-1   | INFO :      configure_evaluate: no clients selected, skipping evaluation
2025-05-13 18:36:50 client1-1  | INFO :      
2025-05-13 18:36:50 client1-1  | INFO :      Received: train message b35412fa-cbec-41f8-a956-e51c8ef3709c
2025-05-13 21:40:53 server-1   | INFO :      
2025-05-13 18:37:02 client1-1  | {'loss': 0.5892, 'grad_norm': 8.040486335754395, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 21:40:53 server-1   | INFO :      [ROUND 24]
2025-05-13 21:40:53 server-1   | INFO :      configure_fit: strategy sampled 2 clients (out of 2)
2025-05-13 18:32:59 client2-1  | {'loss': 1.101, 'grad_norm': 10.987435340881348, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 18:33:09 client2-1  | {'loss': 1.1522, 'grad_norm': 10.681610107421875, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 18:33:18 client2-1  | {'loss': 1.1567, 'grad_norm': 11.911856651306152, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 18:37:12 client1-1  | {'loss': 0.5967, 'grad_norm': 12.291534423828125, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 18:37:21 client1-1  | {'loss': 0.7596, 'grad_norm': 7.883657932281494, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 21:49:37 server-1   | INFO :      aggregate_fit: received 2 results and 0 failures
2025-05-13 21:49:42 server-1   | INFO :      configure_evaluate: no clients selected, skipping evaluation
2025-05-13 21:49:42 server-1   | INFO :      
2025-05-13 21:49:42 server-1   | INFO :      [ROUND 25]
2025-05-13 21:49:42 server-1   | INFO :      configure_fit: strategy sampled 2 clients (out of 2)
2025-05-13 21:58:18 server-1   | INFO :      aggregate_fit: received 2 results and 0 failures
2025-05-13 21:58:23 server-1   | INFO :      configure_evaluate: no clients selected, skipping evaluation
2025-05-13 18:37:31 client1-1  | {'loss': 0.7155, 'grad_norm': 8.015192985534668, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 21:58:23 server-1   | INFO :      
2025-05-13 21:58:23 server-1   | INFO :      [SUMMARY]
2025-05-13 18:33:28 client2-1  | {'loss': 1.1706, 'grad_norm': 10.330103874206543, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 18:33:41 client2-1  | {'loss': 1.1386, 'grad_norm': 9.63045883178711, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 18:33:50 client2-1  | {'loss': 1.2364, 'grad_norm': 11.72547435760498, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 18:34:00 client2-1  | {'loss': 1.1871, 'grad_norm': 11.511741638183594, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 18:34:09 client2-1  | {'loss': 1.0856, 'grad_norm': 11.242377281188965, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 21:58:23 server-1   | INFO :      Run finished 25 round(s) in 13179.47s
2025-05-13 21:58:23 server-1   | INFO :      
2025-05-13 18:34:19 client2-1  | {'loss': 1.3564, 'grad_norm': 12.119418144226074, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 18:34:28 client2-1  | {'loss': 1.1045, 'grad_norm': 13.655289649963379, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 18:34:38 client2-1  | {'loss': 1.2356, 'grad_norm': 9.89541244506836, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 18:34:51 client2-1  | {'loss': 1.2516, 'grad_norm': 15.69943618774414, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 18:35:00 client2-1  | {'loss': 1.3098, 'grad_norm': 10.97724723815918, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 18:35:10 client2-1  | {'loss': 1.2172, 'grad_norm': 12.12585163116455, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 18:35:19 client2-1  | {'loss': 1.1774, 'grad_norm': 11.36531925201416, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 18:35:29 client2-1  | {'loss': 1.1996, 'grad_norm': 9.978365898132324, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 18:35:38 client2-1  | {'loss': 1.1426, 'grad_norm': 10.005638122558594, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 18:37:44 client1-1  | {'loss': 0.7522, 'grad_norm': 12.02138614654541, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 18:37:54 client1-1  | {'loss': 0.7318, 'grad_norm': 10.371251106262207, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 18:38:04 client1-1  | {'loss': 0.8307, 'grad_norm': 10.611231803894043, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 18:35:48 client2-1  | {'loss': 1.2972, 'grad_norm': 12.113786697387695, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 18:36:01 client2-1  | {'loss': 1.444, 'grad_norm': 13.27875804901123, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 18:36:10 client2-1  | {'loss': 1.0508, 'grad_norm': 8.080606460571289, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 18:36:20 client2-1  | {'loss': 1.1804, 'grad_norm': 10.484134674072266, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 18:36:29 client2-1  | {'loss': 0.8138, 'grad_norm': 7.912569522857666, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 18:36:29 client2-1  | {'train_runtime': 502.9636, 'train_samples_per_second': 1.988, 'train_steps_per_second': 0.994, 'train_loss': 1.1243437843322754, 'epoch': 1.0}
2025-05-13 18:38:13 client1-1  | {'loss': 0.7187, 'grad_norm': 7.2831621170043945, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 18:38:23 client1-1  | {'loss': 0.7576, 'grad_norm': 15.245073318481445, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 18:36:37 client2-1  | INFO :      Sent reply
2025-05-13 18:36:50 client2-1  | INFO :      
2025-05-13 18:38:32 client1-1  | {'loss': 0.7814, 'grad_norm': 8.571016311645508, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 18:38:45 client1-1  | {'loss': 0.8127, 'grad_norm': 12.352163314819336, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 18:38:55 client1-1  | {'loss': 0.8238, 'grad_norm': 13.927971839904785, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 18:39:04 client1-1  | {'loss': 0.7532, 'grad_norm': 11.428662300109863, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 18:39:14 client1-1  | {'loss': 0.8121, 'grad_norm': 7.339838981628418, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 18:39:23 client1-1  | {'loss': 1.0437, 'grad_norm': 9.289861679077148, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 18:39:33 client1-1  | {'loss': 0.8575, 'grad_norm': 10.620539665222168, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 18:39:42 client1-1  | {'loss': 0.768, 'grad_norm': 9.388891220092773, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 18:36:50 client2-1  | INFO :      Received: train message 445b7fb1-3b2d-481b-bc0c-39ea0ab8b2e4
2025-05-13 18:37:01 client2-1  | {'loss': 0.53, 'grad_norm': 9.097503662109375, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 18:37:10 client2-1  | {'loss': 0.6904, 'grad_norm': 8.027504920959473, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 18:37:23 client2-1  | {'loss': 0.7441, 'grad_norm': 11.770605087280273, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 18:37:33 client2-1  | {'loss': 0.7461, 'grad_norm': 8.778541564941406, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 18:37:42 client2-1  | {'loss': 0.7329, 'grad_norm': 11.187856674194336, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 18:39:55 client1-1  | {'loss': 0.841, 'grad_norm': 154.8362579345703, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 18:40:05 client1-1  | {'loss': 0.875, 'grad_norm': 12.789351463317871, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 18:40:14 client1-1  | {'loss': 0.934, 'grad_norm': 11.221269607543945, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 18:37:52 client2-1  | {'loss': 0.7683, 'grad_norm': 11.738333702087402, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 18:38:01 client2-1  | {'loss': 0.6961, 'grad_norm': 9.956257820129395, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 18:38:11 client2-1  | {'loss': 0.8037, 'grad_norm': 10.423371315002441, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 18:38:24 client2-1  | {'loss': 0.6401, 'grad_norm': 7.673334121704102, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 18:38:34 client2-1  | {'loss': 0.7547, 'grad_norm': 14.741395950317383, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 18:38:43 client2-1  | {'loss': 0.7934, 'grad_norm': 8.612288475036621, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 18:38:53 client2-1  | {'loss': 0.8385, 'grad_norm': 9.301312446594238, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 18:39:02 client2-1  | {'loss': 0.8868, 'grad_norm': 9.156448364257812, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 18:40:24 client1-1  | {'loss': 0.8193, 'grad_norm': 8.990822792053223, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 18:39:12 client2-1  | {'loss': 0.7636, 'grad_norm': 10.177605628967285, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 18:39:21 client2-1  | {'loss': 0.8648, 'grad_norm': 10.652417182922363, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 18:39:35 client2-1  | {'loss': 0.7713, 'grad_norm': 11.304559707641602, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 18:39:44 client2-1  | {'loss': 0.9728, 'grad_norm': 12.036602973937988, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 18:40:33 client1-1  | {'loss': 0.9822, 'grad_norm': 13.453435897827148, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 18:40:43 client1-1  | {'loss': 1.0036, 'grad_norm': 11.216054916381836, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 18:40:56 client1-1  | {'loss': 0.8182, 'grad_norm': 7.842051982879639, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 18:41:06 client1-1  | {'loss': 0.8711, 'grad_norm': 11.390817642211914, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 18:41:16 client1-1  | {'loss': 1.0517, 'grad_norm': 15.359107971191406, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 18:41:25 client1-1  | {'loss': 0.9315, 'grad_norm': 9.79954719543457, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 18:41:34 client1-1  | {'loss': 0.9624, 'grad_norm': 10.225931167602539, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 18:41:44 client1-1  | {'loss': 0.9879, 'grad_norm': 9.731544494628906, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 18:41:53 client1-1  | {'loss': 0.9891, 'grad_norm': 9.416932106018066, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 18:42:06 client1-1  | {'loss': 1.0315, 'grad_norm': 7.522188663482666, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 18:42:15 client1-1  | {'loss': 1.0211, 'grad_norm': 7.636017799377441, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 18:42:25 client1-1  | {'loss': 0.8526, 'grad_norm': 9.745434761047363, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 18:42:35 client1-1  | {'loss': 0.9951, 'grad_norm': 9.612683296203613, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 18:42:44 client1-1  | {'loss': 1.0886, 'grad_norm': 12.17374038696289, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 18:42:54 client1-1  | {'loss': 1.1205, 'grad_norm': 10.05066204071045, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 18:43:03 client1-1  | {'loss': 1.1266, 'grad_norm': 12.560768127441406, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 18:43:13 client1-1  | {'loss': 1.074, 'grad_norm': 14.275960922241211, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 18:43:26 client1-1  | {'loss': 1.1138, 'grad_norm': 13.743253707885742, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 18:43:36 client1-1  | {'loss': 1.041, 'grad_norm': 17.003190994262695, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 18:43:45 client1-1  | {'loss': 1.171, 'grad_norm': 10.75892448425293, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 18:43:55 client1-1  | {'loss': 1.2341, 'grad_norm': 12.242895126342773, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 18:44:04 client1-1  | {'loss': 1.1366, 'grad_norm': 10.934354782104492, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 18:44:13 client1-1  | {'loss': 1.3285, 'grad_norm': 14.567307472229004, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 18:44:27 client1-1  | {'loss': 1.1518, 'grad_norm': 12.851701736450195, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 18:44:36 client1-1  | {'loss': 1.2778, 'grad_norm': 10.71049690246582, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 18:39:53 client2-1  | {'loss': 0.8566, 'grad_norm': 9.662018775939941, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 18:40:03 client2-1  | {'loss': 0.867, 'grad_norm': 10.802114486694336, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 18:40:12 client2-1  | {'loss': 0.8276, 'grad_norm': 13.49249267578125, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 18:40:22 client2-1  | {'loss': 0.9567, 'grad_norm': 11.540982246398926, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 18:44:45 client1-1  | {'loss': 1.2968, 'grad_norm': 12.804234504699707, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 18:44:55 client1-1  | {'loss': 1.2304, 'grad_norm': 8.859628677368164, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 18:45:04 client1-1  | {'loss': 1.0893, 'grad_norm': 10.866535186767578, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 18:45:14 client1-1  | {'loss': 0.64, 'grad_norm': 8.580607414245605, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 18:40:35 client2-1  | {'loss': 0.9817, 'grad_norm': 13.910213470458984, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 18:40:45 client2-1  | {'loss': 0.9156, 'grad_norm': 12.610824584960938, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 18:40:54 client2-1  | {'loss': 0.986, 'grad_norm': 8.191637992858887, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 18:41:04 client2-1  | {'loss': 0.9067, 'grad_norm': 9.047628402709961, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 18:45:14 client1-1  | {'train_runtime': 503.077, 'train_samples_per_second': 1.988, 'train_steps_per_second': 0.994, 'train_loss': 0.9438495073318481, 'epoch': 1.0}
2025-05-13 18:45:22 client1-1  | INFO :      Sent reply
2025-05-13 18:45:34 client1-1  | INFO :      
2025-05-13 18:45:34 client1-1  | INFO :      Received: train message 750166d3-5111-454c-9ca7-412a91c558d1
2025-05-13 18:45:45 client1-1  | {'loss': 0.3874, 'grad_norm': 5.961780548095703, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 18:45:58 client1-1  | {'loss': 0.3927, 'grad_norm': 8.9060697555542, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 18:46:08 client1-1  | {'loss': 0.5348, 'grad_norm': 9.058476448059082, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 18:41:13 client2-1  | {'loss': 0.8841, 'grad_norm': 12.158385276794434, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 18:41:23 client2-1  | {'loss': 0.9872, 'grad_norm': 13.888538360595703, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 18:46:18 client1-1  | {'loss': 0.4989, 'grad_norm': 7.162652015686035, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 18:46:27 client1-1  | {'loss': 0.5314, 'grad_norm': 13.734942436218262, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 18:46:36 client1-1  | {'loss': 0.5381, 'grad_norm': 9.254154205322266, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 18:46:46 client1-1  | {'loss': 0.5984, 'grad_norm': 8.027482986450195, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 18:46:55 client1-1  | {'loss': 0.5287, 'grad_norm': 7.441176414489746, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 18:47:08 client1-1  | {'loss': 0.5746, 'grad_norm': 9.467260360717773, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 18:41:36 client2-1  | {'loss': 1.0761, 'grad_norm': 9.779940605163574, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 18:47:18 client1-1  | {'loss': 0.571, 'grad_norm': 8.43510627746582, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 18:47:27 client1-1  | {'loss': 0.6154, 'grad_norm': 9.425620079040527, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 18:41:45 client2-1  | {'loss': 0.9426, 'grad_norm': 11.82186508178711, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 18:41:55 client2-1  | {'loss': 0.9708, 'grad_norm': 9.583664894104004, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 18:42:04 client2-1  | {'loss': 1.0151, 'grad_norm': 12.271120071411133, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 18:42:14 client2-1  | {'loss': 1.0079, 'grad_norm': 11.578092575073242, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 18:47:37 client1-1  | {'loss': 0.6465, 'grad_norm': 13.758532524108887, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 18:42:23 client2-1  | {'loss': 0.9889, 'grad_norm': 9.459115028381348, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 18:42:36 client2-1  | {'loss': 1.07, 'grad_norm': 9.748970985412598, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 18:42:46 client2-1  | {'loss': 1.0383, 'grad_norm': 9.9427490234375, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 18:47:47 client1-1  | {'loss': 0.5771, 'grad_norm': 10.09559440612793, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 18:48:00 client1-1  | {'loss': 0.6362, 'grad_norm': 7.695659637451172, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 18:48:10 client1-1  | {'loss': 0.7958, 'grad_norm': 10.639273643493652, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 18:48:19 client1-1  | {'loss': 0.6346, 'grad_norm': 10.86381721496582, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 18:48:29 client1-1  | {'loss': 0.6014, 'grad_norm': 9.63518238067627, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 18:48:42 client1-1  | {'loss': 0.6815, 'grad_norm': 10.626628875732422, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 18:48:52 client1-1  | {'loss': 0.6781, 'grad_norm': 9.595121383666992, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 18:49:01 client1-1  | {'loss': 0.7419, 'grad_norm': 12.194238662719727, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 18:49:11 client1-1  | {'loss': 0.6492, 'grad_norm': 7.40604305267334, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 18:49:20 client1-1  | {'loss': 0.7782, 'grad_norm': 12.860681533813477, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 18:49:30 client1-1  | {'loss': 0.7975, 'grad_norm': 11.010946273803711, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 18:49:39 client1-1  | {'loss': 0.663, 'grad_norm': 8.390827178955078, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 18:42:56 client2-1  | {'loss': 0.9512, 'grad_norm': 13.724872589111328, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 18:43:05 client2-1  | {'loss': 1.2395, 'grad_norm': 11.809796333312988, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 18:43:15 client2-1  | {'loss': 0.9797, 'grad_norm': 11.716219902038574, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 18:43:24 client2-1  | {'loss': 1.1468, 'grad_norm': 11.012289047241211, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 18:43:34 client2-1  | {'loss': 1.1556, 'grad_norm': 12.667304039001465, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 18:43:47 client2-1  | {'loss': 1.2101, 'grad_norm': 11.363221168518066, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 18:43:57 client2-1  | {'loss': 1.1467, 'grad_norm': 12.154339790344238, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 18:44:06 client2-1  | {'loss': 1.1083, 'grad_norm': 11.50361156463623, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 18:44:16 client2-1  | {'loss': 1.1616, 'grad_norm': 10.832356452941895, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 18:44:25 client2-1  | {'loss': 1.1117, 'grad_norm': 10.712337493896484, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 18:44:38 client2-1  | {'loss': 1.2613, 'grad_norm': 11.876995086669922, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 18:44:48 client2-1  | {'loss': 1.3975, 'grad_norm': 11.991922378540039, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 18:44:57 client2-1  | {'loss': 0.9916, 'grad_norm': 7.573071479797363, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 18:45:07 client2-1  | {'loss': 1.0763, 'grad_norm': 13.290763854980469, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 18:45:16 client2-1  | {'loss': 0.6208, 'grad_norm': 8.965666770935059, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 18:45:16 client2-1  | {'train_runtime': 504.8613, 'train_samples_per_second': 1.981, 'train_steps_per_second': 0.99, 'train_loss': 0.9367050762176514, 'epoch': 1.0}
2025-05-13 18:45:23 client2-1  | INFO :      Sent reply
2025-05-13 18:45:33 client2-1  | INFO :      
2025-05-13 18:45:33 client2-1  | INFO :      Received: train message 44a53a72-8394-4d4b-a59c-0d0de119ac55
2025-05-13 18:45:47 client2-1  | {'loss': 0.3242, 'grad_norm': 6.674596786499023, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 18:45:56 client2-1  | {'loss': 0.4586, 'grad_norm': 9.3467435836792, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 18:46:06 client2-1  | {'loss': 0.5301, 'grad_norm': 11.544791221618652, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 18:46:16 client2-1  | {'loss': 0.5087, 'grad_norm': 7.344874858856201, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 18:46:25 client2-1  | {'loss': 0.5471, 'grad_norm': 11.218941688537598, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 18:46:38 client2-1  | {'loss': 0.5658, 'grad_norm': 11.749950408935547, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 18:49:52 client1-1  | {'loss': 0.7102, 'grad_norm': 9.787955284118652, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 18:46:48 client2-1  | {'loss': 0.5062, 'grad_norm': 9.175406455993652, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 18:46:57 client2-1  | {'loss': 0.5779, 'grad_norm': 9.94432544708252, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 18:47:06 client2-1  | {'loss': 0.4455, 'grad_norm': 7.814126014709473, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 18:47:16 client2-1  | {'loss': 0.5484, 'grad_norm': 10.757131576538086, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 18:47:25 client2-1  | {'loss': 0.5884, 'grad_norm': 8.553467750549316, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 18:47:35 client2-1  | {'loss': 0.6251, 'grad_norm': 8.121828079223633, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 18:47:48 client2-1  | {'loss': 0.6949, 'grad_norm': 9.261263847351074, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 18:47:58 client2-1  | {'loss': 0.6155, 'grad_norm': 10.955328941345215, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 18:48:07 client2-1  | {'loss': 0.6787, 'grad_norm': 9.928589820861816, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 18:48:17 client2-1  | {'loss': 0.6019, 'grad_norm': 9.4383544921875, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 18:48:26 client2-1  | {'loss': 0.7433, 'grad_norm': 10.213298797607422, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 18:50:02 client1-1  | {'loss': 0.8362, 'grad_norm': 13.97655963897705, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 18:50:11 client1-1  | {'loss': 0.7587, 'grad_norm': 8.47465991973877, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 18:50:21 client1-1  | {'loss': 0.7962, 'grad_norm': 10.879525184631348, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 18:50:30 client1-1  | {'loss': 0.8377, 'grad_norm': 12.519659042358398, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 18:48:36 client2-1  | {'loss': 0.6659, 'grad_norm': 12.08181095123291, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 18:48:49 client2-1  | {'loss': 0.7028, 'grad_norm': 12.111372947692871, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 18:48:59 client2-1  | {'loss': 0.6676, 'grad_norm': 8.872746467590332, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 18:50:40 client1-1  | {'loss': 0.8614, 'grad_norm': 11.278242111206055, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 18:49:08 client2-1  | {'loss': 0.7902, 'grad_norm': 14.021708488464355, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 18:49:18 client2-1  | {'loss': 0.8044, 'grad_norm': 15.257088661193848, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 18:49:31 client2-1  | {'loss': 0.7347, 'grad_norm': 11.80394172668457, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 18:50:49 client1-1  | {'loss': 0.8855, 'grad_norm': 7.185699462890625, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 18:51:02 client1-1  | {'loss': 0.8837, 'grad_norm': 7.601963996887207, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 18:51:12 client1-1  | {'loss': 0.7349, 'grad_norm': 9.225814819335938, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 18:49:41 client2-1  | {'loss': 0.8201, 'grad_norm': 7.587857723236084, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 18:51:21 client1-1  | {'loss': 0.8666, 'grad_norm': 9.116022109985352, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 18:49:50 client2-1  | {'loss': 0.7558, 'grad_norm': 9.266284942626953, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 18:51:31 client1-1  | {'loss': 0.961, 'grad_norm': 12.189339637756348, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 18:51:40 client1-1  | {'loss': 1.0031, 'grad_norm': 9.917431831359863, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 18:51:50 client1-1  | {'loss': 1.0079, 'grad_norm': 10.769201278686523, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 18:50:00 client2-1  | {'loss': 0.7137, 'grad_norm': 10.374454498291016, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 18:51:59 client1-1  | {'loss': 0.995, 'grad_norm': 14.05954360961914, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 18:52:09 client1-1  | {'loss': 1.0399, 'grad_norm': 13.097947120666504, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 18:52:22 client1-1  | {'loss': 0.968, 'grad_norm': 12.868370056152344, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 18:52:31 client1-1  | {'loss': 1.097, 'grad_norm': 11.314685821533203, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 18:52:41 client1-1  | {'loss': 1.1898, 'grad_norm': 10.65353775024414, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 18:52:51 client1-1  | {'loss': 1.0928, 'grad_norm': 10.265036582946777, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 18:53:00 client1-1  | {'loss': 1.2903, 'grad_norm': 13.871970176696777, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 18:53:09 client1-1  | {'loss': 1.1011, 'grad_norm': 12.432778358459473, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 18:53:23 client1-1  | {'loss': 1.2478, 'grad_norm': 12.187335968017578, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 18:50:13 client2-1  | {'loss': 0.8013, 'grad_norm': 12.047850608825684, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 18:53:32 client1-1  | {'loss': 1.2389, 'grad_norm': 12.196396827697754, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 18:50:23 client2-1  | {'loss': 0.9051, 'grad_norm': 10.930977821350098, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 18:53:42 client1-1  | {'loss': 1.1901, 'grad_norm': 10.803757667541504, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 18:50:33 client2-1  | {'loss': 0.7967, 'grad_norm': 9.961091041564941, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 18:53:51 client1-1  | {'loss': 1.0053, 'grad_norm': 10.800199508666992, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 18:50:42 client2-1  | {'loss': 0.83, 'grad_norm': 10.323195457458496, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 18:50:55 client2-1  | {'loss': 0.8617, 'grad_norm': 12.818504333496094, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 18:51:05 client2-1  | {'loss': 0.8497, 'grad_norm': 10.81076717376709, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 18:51:15 client2-1  | {'loss': 0.8542, 'grad_norm': 8.478463172912598, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 18:54:01 client1-1  | {'loss': 0.5032, 'grad_norm': 8.289101600646973, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 18:51:24 client2-1  | {'loss': 0.9485, 'grad_norm': 9.730812072753906, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 18:51:38 client2-1  | {'loss': 0.9304, 'grad_norm': 9.98270034790039, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 18:51:47 client2-1  | {'loss': 0.8313, 'grad_norm': 12.459123611450195, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 18:51:57 client2-1  | {'loss': 1.121, 'grad_norm': 12.453289031982422, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 18:52:06 client2-1  | {'loss': 0.8943, 'grad_norm': 10.732454299926758, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 18:52:19 client2-1  | {'loss': 1.0623, 'grad_norm': 9.629734992980957, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 18:52:29 client2-1  | {'loss': 1.0588, 'grad_norm': 12.420263290405273, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 18:52:39 client2-1  | {'loss': 1.158, 'grad_norm': 11.811697006225586, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 18:52:49 client2-1  | {'loss': 1.0757, 'grad_norm': 10.7979097366333, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 18:53:02 client2-1  | {'loss': 1.0666, 'grad_norm': 11.482047080993652, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 18:53:11 client2-1  | {'loss': 1.1086, 'grad_norm': 13.099592208862305, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 18:53:21 client2-1  | {'loss': 1.0871, 'grad_norm': 12.784932136535645, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 18:53:30 client2-1  | {'loss': 1.2368, 'grad_norm': 14.400555610656738, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 18:53:44 client2-1  | {'loss': 1.373, 'grad_norm': 13.560651779174805, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 18:53:53 client2-1  | {'loss': 0.9547, 'grad_norm': 7.233937740325928, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 18:54:03 client2-1  | {'loss': 0.9899, 'grad_norm': 8.700345039367676, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 18:54:08 client2-1  | {'loss': 0.5289, 'grad_norm': 7.019270896911621, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 18:54:01 client1-1  | {'train_runtime': 505.726, 'train_samples_per_second': 1.977, 'train_steps_per_second': 0.989, 'train_loss': 0.7950914545059204, 'epoch': 1.0}
2025-05-13 18:54:06 client1-1  | INFO :      Sent reply
2025-05-13 18:54:21 client1-1  | INFO :      
2025-05-13 18:54:08 client2-1  | {'train_runtime': 513.9237, 'train_samples_per_second': 1.946, 'train_steps_per_second': 0.973, 'train_loss': 0.7908034362792968, 'epoch': 1.0}
2025-05-13 18:54:10 client2-1  | INFO :      Sent reply
2025-05-13 18:54:23 client2-1  | INFO :      
2025-05-13 18:54:23 client2-1  | INFO :      Received: train message d19b3065-6c58-4e34-b3cc-e4e29ec50043
2025-05-13 18:54:35 client2-1  | {'loss': 0.1996, 'grad_norm': 6.195380687713623, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 18:54:48 client2-1  | {'loss': 0.301, 'grad_norm': 6.177771091461182, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 18:54:58 client2-1  | {'loss': 0.3936, 'grad_norm': 10.351057052612305, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 18:54:21 client1-1  | INFO :      Received: train message 4666196e-4535-4193-b58e-5870dbd26767
2025-05-13 18:54:32 client1-1  | {'loss': 0.2499, 'grad_norm': 4.811699867248535, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 18:54:46 client1-1  | {'loss': 0.2719, 'grad_norm': 11.259329795837402, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 18:55:07 client2-1  | {'loss': 0.3702, 'grad_norm': 6.851762294769287, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 18:54:55 client1-1  | {'loss': 0.377, 'grad_norm': 8.454113960266113, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 18:55:17 client2-1  | {'loss': 0.4207, 'grad_norm': 9.566206932067871, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 18:55:26 client2-1  | {'loss': 0.3837, 'grad_norm': 8.38704776763916, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 18:55:05 client1-1  | {'loss': 0.369, 'grad_norm': 6.147116661071777, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 18:55:14 client1-1  | {'loss': 0.399, 'grad_norm': 9.333168029785156, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 18:55:24 client1-1  | {'loss': 0.4244, 'grad_norm': 8.63477611541748, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 18:55:33 client1-1  | {'loss': 0.4401, 'grad_norm': 7.668605804443359, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 18:55:47 client1-1  | {'loss': 0.3833, 'grad_norm': 5.556540489196777, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 18:55:56 client1-1  | {'loss': 0.4163, 'grad_norm': 8.595341682434082, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 18:56:06 client1-1  | {'loss': 0.4116, 'grad_norm': 7.513125896453857, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 18:56:15 client1-1  | {'loss': 0.4663, 'grad_norm': 10.367003440856934, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 18:55:40 client2-1  | {'loss': 0.3535, 'grad_norm': 7.648932456970215, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 18:55:50 client2-1  | {'loss': 0.4228, 'grad_norm': 9.732954025268555, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 18:55:59 client2-1  | {'loss': 0.3106, 'grad_norm': 7.5930609703063965, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 18:56:08 client2-1  | {'loss': 0.4092, 'grad_norm': 10.66043472290039, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 18:56:25 client1-1  | {'loss': 0.4585, 'grad_norm': 13.433953285217285, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 18:56:18 client2-1  | {'loss': 0.4518, 'grad_norm': 7.371739864349365, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 18:56:34 client1-1  | {'loss': 0.4249, 'grad_norm': 10.126751899719238, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 18:56:47 client1-1  | {'loss': 0.4645, 'grad_norm': 8.60244369506836, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 18:56:57 client1-1  | {'loss': 0.6338, 'grad_norm': 9.131573677062988, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 18:57:06 client1-1  | {'loss': 0.4775, 'grad_norm': 8.720995903015137, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 18:57:16 client1-1  | {'loss': 0.4645, 'grad_norm': 8.199263572692871, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 18:57:25 client1-1  | {'loss': 0.5187, 'grad_norm': 10.718117713928223, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 18:57:35 client1-1  | {'loss': 0.5633, 'grad_norm': 11.205399513244629, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 18:56:27 client2-1  | {'loss': 0.4511, 'grad_norm': 8.116569519042969, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 18:56:40 client2-1  | {'loss': 0.5126, 'grad_norm': 10.311573028564453, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 18:56:50 client2-1  | {'loss': 0.4569, 'grad_norm': 9.621406555175781, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 18:56:59 client2-1  | {'loss': 0.5278, 'grad_norm': 8.400845527648926, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 18:57:48 client1-1  | {'loss': 0.6044, 'grad_norm': 9.41037368774414, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 18:57:57 client1-1  | {'loss': 0.4929, 'grad_norm': 6.964356422424316, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 18:57:09 client2-1  | {'loss': 0.4601, 'grad_norm': 9.81387710571289, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 18:57:18 client2-1  | {'loss': 0.5624, 'grad_norm': 12.709278106689453, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 18:57:28 client2-1  | {'loss': 0.508, 'grad_norm': 7.731442451477051, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 18:57:37 client2-1  | {'loss': 0.5656, 'grad_norm': 10.09536361694336, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 18:58:07 client1-1  | {'loss': 0.6147, 'grad_norm': 10.731378555297852, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 18:57:50 client2-1  | {'loss': 0.5534, 'grad_norm': 10.315255165100098, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 18:58:00 client2-1  | {'loss': 0.6279, 'grad_norm': 11.277783393859863, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 18:58:09 client2-1  | {'loss': 0.6594, 'grad_norm': 15.122061729431152, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 18:58:17 client1-1  | {'loss': 0.6542, 'grad_norm': 11.335256576538086, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 18:58:26 client1-1  | {'loss': 0.5218, 'grad_norm': 6.530417442321777, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 18:58:36 client1-1  | {'loss': 0.572, 'grad_norm': 8.596269607543945, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 18:58:19 client2-1  | {'loss': 0.5972, 'grad_norm': 11.948506355285645, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 18:58:49 client1-1  | {'loss': 0.6875, 'grad_norm': 14.900081634521484, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 18:58:59 client1-1  | {'loss': 0.6297, 'grad_norm': 8.697381973266602, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 18:59:08 client1-1  | {'loss': 0.6793, 'grad_norm': 10.221420288085938, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 18:59:17 client1-1  | {'loss': 0.724, 'grad_norm': 11.195914268493652, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 18:59:27 client1-1  | {'loss': 0.7291, 'grad_norm': 9.129889488220215, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 18:58:28 client2-1  | {'loss': 0.664, 'grad_norm': 7.407657146453857, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 18:59:36 client1-1  | {'loss': 0.782, 'grad_norm': 7.375555038452148, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 18:58:38 client2-1  | {'loss': 0.6124, 'grad_norm': 10.519112586975098, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 18:59:49 client1-1  | {'loss': 0.7647, 'grad_norm': 7.804066181182861, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 18:59:59 client1-1  | {'loss': 0.624, 'grad_norm': 9.306962013244629, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 19:00:08 client1-1  | {'loss': 0.7624, 'grad_norm': 9.349764823913574, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 19:00:18 client1-1  | {'loss': 0.8568, 'grad_norm': 12.458467483520508, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 18:58:51 client2-1  | {'loss': 0.5826, 'grad_norm': 10.039960861206055, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 19:00:27 client1-1  | {'loss': 0.8963, 'grad_norm': 9.1736421585083, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 19:00:37 client1-1  | {'loss': 0.926, 'grad_norm': 11.421844482421875, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 19:00:50 client1-1  | {'loss': 0.9048, 'grad_norm': 13.958579063415527, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 19:00:59 client1-1  | {'loss': 0.9647, 'grad_norm': 13.983939170837402, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 19:01:09 client1-1  | {'loss': 0.8823, 'grad_norm': 12.207420349121094, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 19:01:18 client1-1  | {'loss': 1.0399, 'grad_norm': 11.408543586730957, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 18:59:01 client2-1  | {'loss': 0.644, 'grad_norm': 10.996194839477539, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 18:59:10 client2-1  | {'loss': 0.7783, 'grad_norm': 10.473001480102539, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 18:59:20 client2-1  | {'loss': 0.6676, 'grad_norm': 15.270186424255371, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 19:01:28 client1-1  | {'loss': 1.1092, 'grad_norm': 9.958736419677734, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 19:01:38 client1-1  | {'loss': 1.0297, 'grad_norm': 10.53394603729248, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 18:59:29 client2-1  | {'loss': 0.7046, 'grad_norm': 10.505117416381836, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 18:59:38 client2-1  | {'loss': 0.7569, 'grad_norm': 12.23161792755127, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 18:59:48 client2-1  | {'loss': 0.7504, 'grad_norm': 11.219152450561523, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 19:00:01 client2-1  | {'loss': 0.7453, 'grad_norm': 10.19951057434082, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 19:01:47 client1-1  | {'loss': 1.2284, 'grad_norm': 15.171849250793457, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 19:02:00 client1-1  | {'loss': 1.085, 'grad_norm': 13.280379295349121, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 19:02:10 client1-1  | {'loss': 1.2454, 'grad_norm': 12.089530944824219, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 19:02:19 client1-1  | {'loss': 1.2305, 'grad_norm': 13.275344848632812, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 19:00:10 client2-1  | {'loss': 0.8342, 'grad_norm': 9.631760597229004, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 19:00:20 client2-1  | {'loss': 0.8246, 'grad_norm': 9.751161575317383, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 19:00:29 client2-1  | {'loss': 0.7653, 'grad_norm': 12.526820182800293, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 19:00:39 client2-1  | {'loss': 1.0206, 'grad_norm': 12.196809768676758, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 19:00:48 client2-1  | {'loss': 0.8298, 'grad_norm': 10.830063819885254, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 19:01:01 client2-1  | {'loss': 0.9905, 'grad_norm': 9.454440116882324, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 19:02:29 client1-1  | {'loss': 1.1474, 'grad_norm': 10.407732963562012, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 19:01:11 client2-1  | {'loss': 0.9916, 'grad_norm': 12.934950828552246, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 19:02:38 client1-1  | {'loss': 0.8934, 'grad_norm': 9.367257118225098, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 19:01:21 client2-1  | {'loss': 1.0576, 'grad_norm': 11.006021499633789, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 19:01:30 client2-1  | {'loss': 1.0281, 'grad_norm': 11.721087455749512, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 19:01:40 client2-1  | {'loss': 1.0296, 'grad_norm': 12.474541664123535, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 19:02:51 client1-1  | {'loss': 0.4034, 'grad_norm': 6.96378231048584, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 19:01:49 client2-1  | {'loss': 1.0932, 'grad_norm': 11.077741622924805, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 19:02:51 client1-1  | {'train_runtime': 508.4822, 'train_samples_per_second': 1.967, 'train_steps_per_second': 0.983, 'train_loss': 0.6780060896873474, 'epoch': 1.0}
2025-05-13 19:01:59 client2-1  | {'loss': 1.0453, 'grad_norm': 11.022262573242188, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 19:02:12 client2-1  | {'loss': 1.2335, 'grad_norm': 14.411815643310547, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 19:02:21 client2-1  | {'loss': 1.3286, 'grad_norm': 12.51902961730957, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 19:02:31 client2-1  | {'loss': 0.9148, 'grad_norm': 8.202847480773926, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 19:02:40 client2-1  | {'loss': 0.9134, 'grad_norm': 10.18970012664795, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 19:03:00 client1-1  | INFO :      Sent reply
2025-05-13 19:02:50 client2-1  | {'loss': 0.403, 'grad_norm': 6.846887588500977, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 19:03:11 client1-1  | INFO :      
2025-05-13 19:02:50 client2-1  | {'train_runtime': 505.0688, 'train_samples_per_second': 1.98, 'train_steps_per_second': 0.99, 'train_loss': 0.6741768877506256, 'epoch': 1.0}
2025-05-13 19:03:00 client2-1  | INFO :      Sent reply
2025-05-13 19:03:11 client2-1  | INFO :      
2025-05-13 19:03:11 client2-1  | INFO :      Received: train message 98be7439-2870-4a97-96b3-53bcc892282f
2025-05-13 19:03:24 client2-1  | {'loss': 0.1462, 'grad_norm': 6.605175971984863, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 19:03:38 client2-1  | {'loss': 0.2155, 'grad_norm': 5.021816730499268, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 19:03:48 client2-1  | {'loss': 0.2985, 'grad_norm': 9.754419326782227, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 19:03:11 client1-1  | INFO :      Received: train message 36d4eba6-ece9-4d7b-bb81-289e74297795
2025-05-13 19:03:22 client1-1  | {'loss': 0.1647, 'grad_norm': 3.2090437412261963, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 19:03:32 client1-1  | {'loss': 0.1827, 'grad_norm': 5.563619613647461, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 19:03:45 client1-1  | {'loss': 0.278, 'grad_norm': 7.505514621734619, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 19:03:55 client1-1  | {'loss': 0.2632, 'grad_norm': 5.91767692565918, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 19:04:05 client1-1  | {'loss': 0.2954, 'grad_norm': 8.552359580993652, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 19:04:14 client1-1  | {'loss': 0.2983, 'grad_norm': 11.08202075958252, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 19:04:24 client1-1  | {'loss': 0.3595, 'grad_norm': 7.230288505554199, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 19:03:57 client2-1  | {'loss': 0.2784, 'grad_norm': 5.30289363861084, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 19:04:07 client2-1  | {'loss': 0.277, 'grad_norm': 10.031195640563965, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 19:04:37 client1-1  | {'loss': 0.2767, 'grad_norm': 4.166879177093506, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 19:04:17 client2-1  | {'loss': 0.295, 'grad_norm': 9.864551544189453, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 19:04:30 client2-1  | {'loss': 0.273, 'grad_norm': 7.262415885925293, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 19:04:40 client2-1  | {'loss': 0.3123, 'grad_norm': 8.799627304077148, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 19:04:49 client2-1  | {'loss': 0.2355, 'grad_norm': 6.857981204986572, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 19:04:59 client2-1  | {'loss': 0.2966, 'grad_norm': 8.793478965759277, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 19:05:09 client2-1  | {'loss': 0.327, 'grad_norm': 6.4412031173706055, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 19:04:47 client1-1  | {'loss': 0.2939, 'grad_norm': 6.782358169555664, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 19:05:18 client2-1  | {'loss': 0.3382, 'grad_norm': 6.954945087432861, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 19:05:31 client2-1  | {'loss': 0.3869, 'grad_norm': 8.612149238586426, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 19:05:41 client2-1  | {'loss': 0.3542, 'grad_norm': 8.475859642028809, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 19:04:57 client1-1  | {'loss': 0.3111, 'grad_norm': 7.032033920288086, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 19:05:50 client2-1  | {'loss': 0.406, 'grad_norm': 7.884446144104004, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 19:05:06 client1-1  | {'loss': 0.336, 'grad_norm': 7.736778259277344, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 19:06:00 client2-1  | {'loss': 0.343, 'grad_norm': 8.473267555236816, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 19:06:10 client2-1  | {'loss': 0.4321, 'grad_norm': 9.86916732788086, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 19:06:19 client2-1  | {'loss': 0.3887, 'grad_norm': 8.533645629882812, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 19:05:16 client1-1  | {'loss': 0.3613, 'grad_norm': 12.44975757598877, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 19:06:29 client2-1  | {'loss': 0.4324, 'grad_norm': 7.821770191192627, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 19:05:25 client1-1  | {'loss': 0.3249, 'grad_norm': 8.042229652404785, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 19:05:35 client1-1  | {'loss': 0.3784, 'grad_norm': 6.610067367553711, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 19:05:48 client1-1  | {'loss': 0.4888, 'grad_norm': 7.855478763580322, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 19:06:42 client2-1  | {'loss': 0.4262, 'grad_norm': 9.161747932434082, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 19:06:52 client2-1  | {'loss': 0.5267, 'grad_norm': 10.91909122467041, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 19:07:01 client2-1  | {'loss': 0.5191, 'grad_norm': 14.980859756469727, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 19:05:57 client1-1  | {'loss': 0.3459, 'grad_norm': 11.00524616241455, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 19:07:11 client2-1  | {'loss': 0.4807, 'grad_norm': 11.22014331817627, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 19:07:21 client2-1  | {'loss': 0.5365, 'grad_norm': 6.291222095489502, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 19:07:30 client2-1  | {'loss': 0.4929, 'grad_norm': 7.644124984741211, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 19:06:07 client1-1  | {'loss': 0.3378, 'grad_norm': 6.428599834442139, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 19:06:17 client1-1  | {'loss': 0.4039, 'grad_norm': 10.337538719177246, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 19:06:26 client1-1  | {'loss': 0.4229, 'grad_norm': 9.882704734802246, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 19:06:36 client1-1  | {'loss': 0.4897, 'grad_norm': 11.036611557006836, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 19:07:43 client2-1  | {'loss': 0.4893, 'grad_norm': 10.28736686706543, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 19:07:53 client2-1  | {'loss': 0.533, 'grad_norm': 10.653844833374023, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 19:08:02 client2-1  | {'loss': 0.6415, 'grad_norm': 8.943870544433594, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 19:08:12 client2-1  | {'loss': 0.5635, 'grad_norm': 11.741676330566406, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 19:06:49 client1-1  | {'loss': 0.3979, 'grad_norm': 6.3697662353515625, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 19:06:59 client1-1  | {'loss': 0.4832, 'grad_norm': 13.505504608154297, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 19:07:08 client1-1  | {'loss': 0.5278, 'grad_norm': 10.663359642028809, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 19:07:18 client1-1  | {'loss': 0.42, 'grad_norm': 7.240450859069824, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 19:07:28 client1-1  | {'loss': 0.4554, 'grad_norm': 7.72315788269043, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 19:07:41 client1-1  | {'loss': 0.563, 'grad_norm': 14.249054908752441, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 19:08:21 client2-1  | {'loss': 0.6026, 'grad_norm': 9.975278854370117, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 19:08:31 client2-1  | {'loss': 0.6272, 'grad_norm': 11.311760902404785, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 19:08:40 client2-1  | {'loss': 0.6367, 'grad_norm': 10.846290588378906, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 19:08:53 client2-1  | {'loss': 0.6565, 'grad_norm': 8.720216751098633, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 19:07:50 client1-1  | {'loss': 0.5202, 'grad_norm': 7.435588836669922, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 19:08:00 client1-1  | {'loss': 0.5513, 'grad_norm': 9.144377708435059, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 19:09:03 client2-1  | {'loss': 0.729, 'grad_norm': 9.656622886657715, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 19:08:09 client1-1  | {'loss': 0.6038, 'grad_norm': 10.950055122375488, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 19:09:13 client2-1  | {'loss': 0.7314, 'grad_norm': 9.83601188659668, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 19:08:19 client1-1  | {'loss': 0.6077, 'grad_norm': 9.783116340637207, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 19:08:29 client1-1  | {'loss': 0.6787, 'grad_norm': 6.546167373657227, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 19:08:42 client1-1  | {'loss': 0.6476, 'grad_norm': 6.8945841789245605, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 19:08:51 client1-1  | {'loss': 0.5363, 'grad_norm': 9.092752456665039, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 19:09:01 client1-1  | {'loss': 0.6441, 'grad_norm': 9.49316120147705, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 19:09:11 client1-1  | {'loss': 0.7361, 'grad_norm': 12.78886890411377, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 19:09:22 client2-1  | {'loss': 0.6793, 'grad_norm': 11.004752159118652, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 19:09:20 client1-1  | {'loss': 0.7815, 'grad_norm': 9.200518608093262, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 19:09:29 client1-1  | {'loss': 0.8286, 'grad_norm': 14.08676815032959, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 19:09:39 client1-1  | {'loss': 0.8163, 'grad_norm': 14.80832290649414, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 19:09:49 client1-1  | {'loss': 0.8855, 'grad_norm': 15.035484313964844, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 19:09:58 client1-1  | {'loss': 0.8289, 'grad_norm': 11.949737548828125, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 19:10:11 client1-1  | {'loss': 0.9818, 'grad_norm': 12.198678016662598, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 19:10:21 client1-1  | {'loss': 1.062, 'grad_norm': 11.594566345214844, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 19:10:30 client1-1  | {'loss': 0.9978, 'grad_norm': 11.748754501342773, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 19:09:32 client2-1  | {'loss': 0.9292, 'grad_norm': 12.569966316223145, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 19:10:40 client1-1  | {'loss': 1.1906, 'grad_norm': 16.428037643432617, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 19:10:49 client1-1  | {'loss': 1.0612, 'grad_norm': 17.78350830078125, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 19:09:42 client2-1  | {'loss': 0.752, 'grad_norm': 9.159887313842773, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 19:10:59 client1-1  | {'loss': 1.2164, 'grad_norm': 13.00387191772461, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 19:09:55 client2-1  | {'loss': 0.8926, 'grad_norm': 9.551911354064941, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 19:10:04 client2-1  | {'loss': 0.9129, 'grad_norm': 19.432676315307617, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 19:10:14 client2-1  | {'loss': 1.0141, 'grad_norm': 13.55911636352539, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 19:10:23 client2-1  | {'loss': 0.989, 'grad_norm': 11.731874465942383, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 19:10:33 client2-1  | {'loss': 0.9795, 'grad_norm': 11.70742416381836, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 19:10:42 client2-1  | {'loss': 1.0523, 'grad_norm': 10.946362495422363, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 19:10:56 client2-1  | {'loss': 0.9986, 'grad_norm': 10.85875129699707, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 19:11:05 client2-1  | {'loss': 1.1808, 'grad_norm': 13.33210563659668, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 19:11:15 client2-1  | {'loss': 1.298, 'grad_norm': 13.78814697265625, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 19:11:24 client2-1  | {'loss': 0.8679, 'grad_norm': 8.69635009765625, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 19:11:34 client2-1  | {'loss': 0.8291, 'grad_norm': 8.921359062194824, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 19:11:43 client2-1  | {'loss': 0.323, 'grad_norm': 5.874537467956543, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 19:11:43 client2-1  | {'train_runtime': 508.957, 'train_samples_per_second': 1.965, 'train_steps_per_second': 0.982, 'train_loss': 0.5785560953617096, 'epoch': 1.0}
2025-05-13 19:11:46 client2-1  | INFO :      Sent reply
2025-05-13 19:11:57 client2-1  | INFO :      
2025-05-13 19:11:57 client2-1  | INFO :      Received: train message 2bd633f7-81f9-4046-8ae9-318defd7daef
2025-05-13 19:12:10 client2-1  | {'loss': 0.1207, 'grad_norm': 3.846846103668213, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 19:12:20 client2-1  | {'loss': 0.157, 'grad_norm': 4.177078723907471, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 19:12:29 client2-1  | {'loss': 0.2281, 'grad_norm': 12.263198852539062, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 19:12:43 client2-1  | {'loss': 0.2218, 'grad_norm': 5.748537063598633, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 19:12:52 client2-1  | {'loss': 0.2095, 'grad_norm': 7.932793617248535, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 19:11:08 client1-1  | {'loss': 1.2014, 'grad_norm': 13.37527084350586, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 19:13:02 client2-1  | {'loss': 0.2098, 'grad_norm': 7.6526007652282715, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 19:13:11 client2-1  | {'loss': 0.2067, 'grad_norm': 5.235026836395264, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 19:11:17 client1-1  | {'loss': 1.1168, 'grad_norm': 9.622968673706055, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 19:11:27 client1-1  | {'loss': 0.82, 'grad_norm': 9.415968894958496, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 19:13:21 client2-1  | {'loss': 0.222, 'grad_norm': 7.708988666534424, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 19:13:31 client2-1  | {'loss': 0.1824, 'grad_norm': 6.087586402893066, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 19:13:40 client2-1  | {'loss': 0.216, 'grad_norm': 9.154680252075195, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 19:13:50 client2-1  | {'loss': 0.2393, 'grad_norm': 6.115818500518799, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 19:14:03 client2-1  | {'loss': 0.2457, 'grad_norm': 7.273046016693115, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 19:14:13 client2-1  | {'loss': 0.2944, 'grad_norm': 8.169012069702148, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 19:14:22 client2-1  | {'loss': 0.2697, 'grad_norm': 7.3603901863098145, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 19:14:32 client2-1  | {'loss': 0.3183, 'grad_norm': 7.551296234130859, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 19:14:42 client2-1  | {'loss': 0.2394, 'grad_norm': 9.428092956542969, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 19:14:51 client2-1  | {'loss': 0.3385, 'grad_norm': 10.355854034423828, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 19:15:01 client2-1  | {'loss': 0.3041, 'grad_norm': 9.167105674743652, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 19:11:40 client1-1  | {'loss': 0.3094, 'grad_norm': 7.266896724700928, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 19:11:40 client1-1  | {'train_runtime': 507.6496, 'train_samples_per_second': 1.97, 'train_steps_per_second': 0.985, 'train_loss': 0.5816844713687896, 'epoch': 1.0}
2025-05-13 19:11:44 client1-1  | INFO :      Sent reply
2025-05-13 19:15:14 client2-1  | {'loss': 0.3415, 'grad_norm': 8.897512435913086, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 19:15:23 client2-1  | {'loss': 0.3376, 'grad_norm': 8.484896659851074, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 19:11:57 client1-1  | INFO :      
2025-05-13 19:11:57 client1-1  | INFO :      Received: train message b3a357f1-5219-4357-9242-7aebcfc4d747
2025-05-13 19:12:08 client1-1  | {'loss': 0.118, 'grad_norm': 2.72702956199646, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 19:12:22 client1-1  | {'loss': 0.154, 'grad_norm': 5.342158794403076, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 19:12:31 client1-1  | {'loss': 0.203, 'grad_norm': 8.769072532653809, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 19:12:41 client1-1  | {'loss': 0.1997, 'grad_norm': 4.264113426208496, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 19:12:50 client1-1  | {'loss': 0.213, 'grad_norm': 7.660369873046875, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 19:13:00 client1-1  | {'loss': 0.2035, 'grad_norm': 6.955164432525635, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 19:13:09 client1-1  | {'loss': 0.2742, 'grad_norm': 6.177282810211182, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 19:15:33 client2-1  | {'loss': 0.4205, 'grad_norm': 9.342423439025879, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 19:15:43 client2-1  | {'loss': 0.3964, 'grad_norm': 12.466936111450195, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 19:15:52 client2-1  | {'loss': 0.3761, 'grad_norm': 11.506762504577637, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 19:16:01 client2-1  | {'loss': 0.4464, 'grad_norm': 6.740231990814209, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 19:16:14 client2-1  | {'loss': 0.4091, 'grad_norm': 6.522054195404053, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 19:13:23 client1-1  | {'loss': 0.2009, 'grad_norm': 3.0428833961486816, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 19:13:32 client1-1  | {'loss': 0.2418, 'grad_norm': 7.855109691619873, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 19:13:42 client1-1  | {'loss': 0.2516, 'grad_norm': 4.990738868713379, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 19:16:24 client2-1  | {'loss': 0.3846, 'grad_norm': 10.838910102844238, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 19:13:52 client1-1  | {'loss': 0.2509, 'grad_norm': 8.016093254089355, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 19:14:01 client1-1  | {'loss': 0.2986, 'grad_norm': 11.006532669067383, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 19:14:11 client1-1  | {'loss': 0.2387, 'grad_norm': 6.986982345581055, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 19:16:33 client2-1  | {'loss': 0.441, 'grad_norm': 10.551349639892578, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 19:16:43 client2-1  | {'loss': 0.5182, 'grad_norm': 8.48914623260498, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 19:16:52 client2-1  | {'loss': 0.4681, 'grad_norm': 11.889837265014648, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 19:17:02 client2-1  | {'loss': 0.5063, 'grad_norm': 10.378519058227539, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 19:14:24 client1-1  | {'loss': 0.2973, 'grad_norm': 6.035068988800049, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 19:14:34 client1-1  | {'loss': 0.3885, 'grad_norm': 7.3969831466674805, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 19:14:44 client1-1  | {'loss': 0.2551, 'grad_norm': 7.805458068847656, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 19:17:15 client2-1  | {'loss': 0.5432, 'grad_norm': 10.834836959838867, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 19:17:24 client2-1  | {'loss': 0.5505, 'grad_norm': 10.514887809753418, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 19:17:34 client2-1  | {'loss': 0.5639, 'grad_norm': 10.591564178466797, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 19:14:53 client1-1  | {'loss': 0.2488, 'grad_norm': 7.4542083740234375, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 19:17:43 client2-1  | {'loss': 0.6549, 'grad_norm': 8.389379501342773, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 19:15:03 client1-1  | {'loss': 0.3009, 'grad_norm': 9.95172119140625, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 19:17:53 client2-1  | {'loss': 0.6282, 'grad_norm': 9.250539779663086, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 19:18:02 client2-1  | {'loss': 0.6187, 'grad_norm': 11.418377876281738, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 19:18:12 client2-1  | {'loss': 0.8389, 'grad_norm': 13.60731315612793, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 19:18:25 client2-1  | {'loss': 0.6696, 'grad_norm': 9.802397727966309, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 19:18:34 client2-1  | {'loss': 0.8105, 'grad_norm': 10.085763931274414, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 19:18:44 client2-1  | {'loss': 0.838, 'grad_norm': 12.126680374145508, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 19:18:53 client2-1  | {'loss': 0.9379, 'grad_norm': 11.791054725646973, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 19:19:03 client2-1  | {'loss': 0.9317, 'grad_norm': 11.150728225708008, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 19:19:12 client2-1  | {'loss': 0.9538, 'grad_norm': 14.022668838500977, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 19:15:12 client1-1  | {'loss': 0.3196, 'grad_norm': 7.74432373046875, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 19:15:22 client1-1  | {'loss': 0.377, 'grad_norm': 9.131278038024902, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 19:15:35 client1-1  | {'loss': 0.3188, 'grad_norm': 7.077809810638428, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 19:15:45 client1-1  | {'loss': 0.3694, 'grad_norm': 10.936683654785156, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 19:15:54 client1-1  | {'loss': 0.4095, 'grad_norm': 10.364089965820312, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 19:16:03 client1-1  | {'loss': 0.3358, 'grad_norm': 8.70014476776123, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 19:19:25 client2-1  | {'loss': 0.9965, 'grad_norm': 12.388958930969238, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 19:19:35 client2-1  | {'loss': 0.9695, 'grad_norm': 12.620574951171875, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 19:19:44 client2-1  | {'loss': 1.1635, 'grad_norm': 15.388382911682129, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 19:16:13 client1-1  | {'loss': 0.3719, 'grad_norm': 8.953105926513672, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 19:16:22 client1-1  | {'loss': 0.4555, 'grad_norm': 12.858531951904297, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 19:19:54 client2-1  | {'loss': 1.2591, 'grad_norm': 13.242877006530762, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 19:20:04 client2-1  | {'loss': 0.837, 'grad_norm': 8.758170127868652, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 19:20:13 client2-1  | {'loss': 0.7655, 'grad_norm': 8.266934394836426, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 19:20:23 client2-1  | {'loss': 0.2634, 'grad_norm': 4.129868507385254, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 19:16:35 client1-1  | {'loss': 0.4107, 'grad_norm': 9.218886375427246, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 19:16:45 client1-1  | {'loss': 0.46, 'grad_norm': 9.590747833251953, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 19:16:54 client1-1  | {'loss': 0.4975, 'grad_norm': 12.578241348266602, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 19:17:04 client1-1  | {'loss': 0.5173, 'grad_norm': 10.800742149353027, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 19:20:23 client2-1  | {'train_runtime': 505.1889, 'train_samples_per_second': 1.979, 'train_steps_per_second': 0.99, 'train_loss': 0.5012688360214234, 'epoch': 1.0}
2025-05-13 19:20:33 client2-1  | INFO :      Sent reply
2025-05-13 19:20:44 client2-1  | INFO :      
2025-05-13 19:17:13 client1-1  | {'loss': 0.5702, 'grad_norm': 6.590967655181885, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 19:17:23 client1-1  | {'loss': 0.5564, 'grad_norm': 7.137899875640869, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 19:17:32 client1-1  | {'loss': 0.4593, 'grad_norm': 8.146573066711426, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 19:17:45 client1-1  | {'loss': 0.5761, 'grad_norm': 9.947687149047852, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 19:17:55 client1-1  | {'loss': 0.6425, 'grad_norm': 12.822233200073242, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 19:18:04 client1-1  | {'loss': 0.714, 'grad_norm': 10.158666610717773, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 19:18:14 client1-1  | {'loss': 0.7685, 'grad_norm': 13.117298126220703, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 19:20:44 client2-1  | INFO :      Received: train message 93a6a8f1-cf2d-42f7-af8b-21967eba2c31
2025-05-13 19:18:23 client1-1  | {'loss': 0.7433, 'grad_norm': 14.976634979248047, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 19:20:55 client2-1  | {'loss': 0.0863, 'grad_norm': 2.300027370452881, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 19:21:05 client2-1  | {'loss': 0.1203, 'grad_norm': 4.922111511230469, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 19:21:18 client2-1  | {'loss': 0.1869, 'grad_norm': 16.069091796875, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 19:21:27 client2-1  | {'loss': 0.1703, 'grad_norm': 5.138688564300537, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 19:18:33 client1-1  | {'loss': 0.8247, 'grad_norm': 15.597734451293945, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 19:18:46 client1-1  | {'loss': 0.7516, 'grad_norm': 13.127809524536133, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 19:18:55 client1-1  | {'loss': 0.9304, 'grad_norm': 11.99825382232666, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 19:19:05 client1-1  | {'loss': 0.9998, 'grad_norm': 11.365008354187012, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 19:19:14 client1-1  | {'loss': 0.9408, 'grad_norm': 12.001815795898438, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 19:21:37 client2-1  | {'loss': 0.169, 'grad_norm': 7.8514509201049805, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 19:19:24 client1-1  | {'loss': 1.1544, 'grad_norm': 17.449926376342773, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 19:19:33 client1-1  | {'loss': 1.0324, 'grad_norm': 14.974264144897461, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 19:21:46 client2-1  | {'loss': 0.1614, 'grad_norm': 8.393324851989746, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 19:21:56 client2-1  | {'loss': 0.1598, 'grad_norm': 4.864823818206787, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 19:22:05 client2-1  | {'loss': 0.1672, 'grad_norm': 7.2582879066467285, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 19:19:43 client1-1  | {'loss': 1.172, 'grad_norm': 12.2625150680542, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 19:19:56 client1-1  | {'loss': 1.1805, 'grad_norm': 15.578475952148438, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 19:20:06 client1-1  | {'loss': 1.0827, 'grad_norm': 10.925435066223145, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 19:20:15 client1-1  | {'loss': 0.7353, 'grad_norm': 12.212443351745605, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 19:20:25 client1-1  | {'loss': 0.2454, 'grad_norm': 5.670843124389648, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 19:20:25 client1-1  | {'train_runtime': 506.2991, 'train_samples_per_second': 1.975, 'train_steps_per_second': 0.988, 'train_loss': 0.5052464327812195, 'epoch': 1.0}
2025-05-13 19:20:33 client1-1  | INFO :      Sent reply
2025-05-13 19:20:44 client1-1  | INFO :      
2025-05-13 19:20:44 client1-1  | INFO :      Received: train message a39c369a-4097-4d62-bfa2-9e2dc00f822c
2025-05-13 19:20:57 client1-1  | {'loss': 0.0952, 'grad_norm': 2.132469892501831, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 19:21:11 client1-1  | {'loss': 0.1257, 'grad_norm': 5.106348514556885, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 19:22:19 client2-1  | {'loss': 0.1577, 'grad_norm': 4.776861190795898, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 19:22:28 client2-1  | {'loss': 0.1787, 'grad_norm': 6.836810111999512, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 19:21:20 client1-1  | {'loss': 0.1531, 'grad_norm': 7.85923957824707, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 19:22:38 client2-1  | {'loss': 0.1797, 'grad_norm': 3.402589797973633, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 19:21:30 client1-1  | {'loss': 0.1637, 'grad_norm': 4.549935817718506, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 19:22:48 client2-1  | {'loss': 0.205, 'grad_norm': 5.502374172210693, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 19:22:57 client2-1  | {'loss': 0.225, 'grad_norm': 5.621515274047852, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 19:23:07 client2-1  | {'loss': 0.2106, 'grad_norm': 6.994902610778809, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 19:23:16 client2-1  | {'loss': 0.2515, 'grad_norm': 6.1647114753723145, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 19:23:29 client2-1  | {'loss': 0.2063, 'grad_norm': 6.987626552581787, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 19:23:39 client2-1  | {'loss': 0.2524, 'grad_norm': 9.367240905761719, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 19:23:48 client2-1  | {'loss': 0.2513, 'grad_norm': 9.335880279541016, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 19:21:39 client1-1  | {'loss': 0.1666, 'grad_norm': 8.128311157226562, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 19:21:49 client1-1  | {'loss': 0.1653, 'grad_norm': 5.006728172302246, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 19:21:58 client1-1  | {'loss': 0.2138, 'grad_norm': 6.098164081573486, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 19:23:58 client2-1  | {'loss': 0.2819, 'grad_norm': 9.991674423217773, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 19:24:07 client2-1  | {'loss': 0.2436, 'grad_norm': 7.32716703414917, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 19:24:17 client2-1  | {'loss': 0.3205, 'grad_norm': 7.905991554260254, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 19:24:30 client2-1  | {'loss': 0.3252, 'grad_norm': 14.538460731506348, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 19:24:40 client2-1  | {'loss': 0.3038, 'grad_norm': 10.04233169555664, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 19:24:49 client2-1  | {'loss': 0.3391, 'grad_norm': 6.614696979522705, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 19:22:12 client1-1  | {'loss': 0.1608, 'grad_norm': 3.6050403118133545, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 19:22:21 client1-1  | {'loss': 0.2085, 'grad_norm': 4.793792247772217, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 19:22:31 client1-1  | {'loss': 0.1986, 'grad_norm': 3.7403950691223145, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 19:22:41 client1-1  | {'loss': 0.2075, 'grad_norm': 6.343902587890625, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 19:22:50 client1-1  | {'loss': 0.2131, 'grad_norm': 9.749842643737793, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 19:24:58 client2-1  | {'loss': 0.3258, 'grad_norm': 6.344288349151611, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 19:25:08 client2-1  | {'loss': 0.3306, 'grad_norm': 9.549016952514648, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 19:23:00 client1-1  | {'loss': 0.199, 'grad_norm': 8.96285343170166, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 19:23:13 client1-1  | {'loss': 0.2281, 'grad_norm': 5.313038349151611, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 19:23:22 client1-1  | {'loss': 0.3205, 'grad_norm': 6.621101379394531, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 19:23:32 client1-1  | {'loss': 0.2088, 'grad_norm': 6.0768866539001465, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 19:25:18 client2-1  | {'loss': 0.3355, 'grad_norm': 10.641286849975586, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 19:25:31 client2-1  | {'loss': 0.4289, 'grad_norm': 9.90526294708252, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 19:25:41 client2-1  | {'loss': 0.3973, 'grad_norm': 10.004110336303711, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 19:25:50 client2-1  | {'loss': 0.4204, 'grad_norm': 9.151092529296875, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 19:26:00 client2-1  | {'loss': 0.4576, 'grad_norm': 13.977375984191895, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 19:26:09 client2-1  | {'loss': 0.4551, 'grad_norm': 10.26914119720459, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 19:26:22 client2-1  | {'loss': 0.4713, 'grad_norm': 8.937898635864258, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 19:23:41 client1-1  | {'loss': 0.1934, 'grad_norm': 5.164967060089111, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 19:23:51 client1-1  | {'loss': 0.2381, 'grad_norm': 10.77894115447998, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 19:24:00 client1-1  | {'loss': 0.2575, 'grad_norm': 7.985256195068359, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 19:24:14 client1-1  | {'loss': 0.3127, 'grad_norm': 7.278733730316162, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 19:26:32 client2-1  | {'loss': 0.5541, 'grad_norm': 9.919900894165039, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 19:24:23 client1-1  | {'loss': 0.2462, 'grad_norm': 5.035999774932861, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 19:24:33 client1-1  | {'loss': 0.3112, 'grad_norm': 12.481549263000488, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 19:24:42 client1-1  | {'loss': 0.3309, 'grad_norm': 8.765029907226562, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 19:24:52 client1-1  | {'loss': 0.2627, 'grad_norm': 5.516631603240967, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 19:26:41 client2-1  | {'loss': 0.5728, 'grad_norm': 9.68736457824707, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 19:26:51 client2-1  | {'loss': 0.5324, 'grad_norm': 11.654752731323242, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 19:27:00 client2-1  | {'loss': 0.7471, 'grad_norm': 11.60986042022705, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 19:27:10 client2-1  | {'loss': 0.5957, 'grad_norm': 8.937541961669922, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 19:25:01 client1-1  | {'loss': 0.3055, 'grad_norm': 7.3683390617370605, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 19:25:15 client1-1  | {'loss': 0.3783, 'grad_norm': 13.035148620605469, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 19:25:24 client1-1  | {'loss': 0.3503, 'grad_norm': 6.961762428283691, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 19:25:34 client1-1  | {'loss': 0.3712, 'grad_norm': 8.623185157775879, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 19:25:44 client1-1  | {'loss': 0.4113, 'grad_norm': 9.309481620788574, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 19:27:23 client2-1  | {'loss': 0.7211, 'grad_norm': 9.136957168579102, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 19:25:53 client1-1  | {'loss': 0.4153, 'grad_norm': 9.236676216125488, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 19:26:02 client1-1  | {'loss': 0.4695, 'grad_norm': 7.061617851257324, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 19:26:12 client1-1  | {'loss': 0.4683, 'grad_norm': 7.4759297370910645, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 19:26:25 client1-1  | {'loss': 0.3919, 'grad_norm': 7.851136684417725, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 19:27:32 client2-1  | {'loss': 0.7754, 'grad_norm': 14.346096992492676, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 19:27:42 client2-1  | {'loss': 0.873, 'grad_norm': 11.917132377624512, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 19:27:51 client2-1  | {'loss': 0.8657, 'grad_norm': 14.404562950134277, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 19:28:01 client2-1  | {'loss': 0.8867, 'grad_norm': 12.370323181152344, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 19:28:14 client2-1  | {'loss': 0.9508, 'grad_norm': 11.45291805267334, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 19:28:24 client2-1  | {'loss': 0.9343, 'grad_norm': 11.240341186523438, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 19:28:33 client2-1  | {'loss': 1.1405, 'grad_norm': 15.716432571411133, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 19:28:43 client2-1  | {'loss': 1.2608, 'grad_norm': 12.906233787536621, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 19:28:52 client2-1  | {'loss': 0.8036, 'grad_norm': 7.804057598114014, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 19:29:02 client2-1  | {'loss': 0.6878, 'grad_norm': 6.975973606109619, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 19:26:34 client1-1  | {'loss': 0.4811, 'grad_norm': 7.442103385925293, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 19:26:44 client1-1  | {'loss': 0.5695, 'grad_norm': 14.652703285217285, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 19:26:53 client1-1  | {'loss': 0.6073, 'grad_norm': 10.940418243408203, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 19:27:03 client1-1  | {'loss': 0.6913, 'grad_norm': 12.209295272827148, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 19:27:12 client1-1  | {'loss': 0.6685, 'grad_norm': 14.735748291015625, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 19:27:25 client1-1  | {'loss': 0.7647, 'grad_norm': 14.079878807067871, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 19:27:35 client1-1  | {'loss': 0.6883, 'grad_norm': 12.741339683532715, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 19:27:44 client1-1  | {'loss': 0.8502, 'grad_norm': 12.275073051452637, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 19:27:54 client1-1  | {'loss': 0.955, 'grad_norm': 12.459236145019531, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 19:28:03 client1-1  | {'loss': 0.8858, 'grad_norm': 10.562641143798828, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 19:29:15 client2-1  | {'loss': 0.2073, 'grad_norm': 4.565770149230957, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 19:29:15 client2-1  | {'train_runtime': 509.4895, 'train_samples_per_second': 1.963, 'train_steps_per_second': 0.981, 'train_loss': 0.43769207966327667, 'epoch': 1.0}
2025-05-13 19:29:20 client2-1  | INFO :      Sent reply
2025-05-13 19:29:32 client2-1  | INFO :      
2025-05-13 19:29:32 client2-1  | INFO :      Received: train message fa4bcac8-b2ce-4f21-8de7-b7461944842b
2025-05-13 19:29:42 client2-1  | {'loss': 0.0807, 'grad_norm': 2.1175034046173096, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 19:28:13 client1-1  | {'loss': 1.1154, 'grad_norm': 16.558006286621094, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 19:29:56 client2-1  | {'loss': 0.1037, 'grad_norm': 2.8404173851013184, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 19:30:05 client2-1  | {'loss': 0.1499, 'grad_norm': 6.029480934143066, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 19:28:26 client1-1  | {'loss': 1.0019, 'grad_norm': 16.7891845703125, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 19:28:36 client1-1  | {'loss': 1.1539, 'grad_norm': 13.007047653198242, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 19:28:45 client1-1  | {'loss': 1.1405, 'grad_norm': 13.779662132263184, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 19:30:15 client2-1  | {'loss': 0.1359, 'grad_norm': 8.438918113708496, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 19:30:24 client2-1  | {'loss': 0.1498, 'grad_norm': 6.210811138153076, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 19:30:34 client2-1  | {'loss': 0.1576, 'grad_norm': 5.703271865844727, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 19:30:44 client2-1  | {'loss': 0.1565, 'grad_norm': 3.5721514225006104, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 19:30:53 client2-1  | {'loss': 0.1488, 'grad_norm': 6.712169647216797, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 19:31:06 client2-1  | {'loss': 0.1428, 'grad_norm': 3.8839476108551025, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 19:28:55 client1-1  | {'loss': 1.0678, 'grad_norm': 11.43354606628418, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 19:31:16 client2-1  | {'loss': 0.1405, 'grad_norm': 7.964552879333496, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 19:29:04 client1-1  | {'loss': 0.6765, 'grad_norm': 9.84370231628418, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 19:29:13 client1-1  | {'loss': 0.1985, 'grad_norm': 5.320525646209717, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 19:29:13 client1-1  | {'train_runtime': 506.2616, 'train_samples_per_second': 1.975, 'train_steps_per_second': 0.988, 'train_loss': 0.44517307925224303, 'epoch': 1.0}
2025-05-13 19:31:26 client2-1  | {'loss': 0.1426, 'grad_norm': 3.3837366104125977, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 19:31:35 client2-1  | {'loss': 0.1572, 'grad_norm': 4.928815841674805, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 19:31:45 client2-1  | {'loss': 0.1803, 'grad_norm': 6.740036487579346, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 19:31:54 client2-1  | {'loss': 0.1701, 'grad_norm': 6.772510528564453, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 19:32:03 client2-1  | {'loss': 0.211, 'grad_norm': 6.588136196136475, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 19:32:17 client2-1  | {'loss': 0.1718, 'grad_norm': 9.494040489196777, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 19:32:26 client2-1  | {'loss': 0.2133, 'grad_norm': 7.622089385986328, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 19:32:35 client2-1  | {'loss': 0.189, 'grad_norm': 7.032465934753418, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 19:32:45 client2-1  | {'loss': 0.2207, 'grad_norm': 7.572933197021484, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 19:32:54 client2-1  | {'loss': 0.2169, 'grad_norm': 8.840978622436523, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 19:29:20 client1-1  | INFO :      Sent reply
2025-05-13 19:29:31 client1-1  | INFO :      
2025-05-13 19:29:31 client1-1  | INFO :      Received: train message cbdfb85c-44cd-446b-891b-d5cb73ca40fb
2025-05-13 19:33:04 client2-1  | {'loss': 0.2783, 'grad_norm': 11.44439697265625, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 19:29:44 client1-1  | {'loss': 0.085, 'grad_norm': 2.8619306087493896, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 19:33:14 client2-1  | {'loss': 0.2753, 'grad_norm': 14.239005088806152, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 19:29:54 client1-1  | {'loss': 0.0999, 'grad_norm': 4.745842456817627, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 19:30:04 client1-1  | {'loss': 0.1333, 'grad_norm': 6.647648334503174, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 19:33:27 client2-1  | {'loss': 0.2507, 'grad_norm': 12.727481842041016, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 19:33:36 client2-1  | {'loss': 0.293, 'grad_norm': 5.506157398223877, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 19:33:46 client2-1  | {'loss': 0.2737, 'grad_norm': 6.444189548492432, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 19:33:55 client2-1  | {'loss': 0.2553, 'grad_norm': 9.048126220703125, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 19:34:05 client2-1  | {'loss': 0.2785, 'grad_norm': 11.257790565490723, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 19:34:14 client2-1  | {'loss': 0.3523, 'grad_norm': 8.124115943908691, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 19:34:24 client2-1  | {'loss': 0.3049, 'grad_norm': 9.701679229736328, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 19:34:37 client2-1  | {'loss': 0.358, 'grad_norm': 9.640410423278809, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 19:30:17 client1-1  | {'loss': 0.1415, 'grad_norm': 4.597795486450195, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 19:30:26 client1-1  | {'loss': 0.1332, 'grad_norm': 5.058117389678955, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 19:30:36 client1-1  | {'loss': 0.1269, 'grad_norm': 4.784160614013672, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 19:30:46 client1-1  | {'loss': 0.1655, 'grad_norm': 7.847074508666992, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 19:34:46 client2-1  | {'loss': 0.3706, 'grad_norm': 12.555680274963379, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 19:34:56 client2-1  | {'loss': 0.3916, 'grad_norm': 11.80092716217041, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 19:35:05 client2-1  | {'loss': 0.4175, 'grad_norm': 7.6470465660095215, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 19:30:55 client1-1  | {'loss': 0.1446, 'grad_norm': 4.093409061431885, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 19:31:05 client1-1  | {'loss': 0.1656, 'grad_norm': 3.275033950805664, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 19:35:15 client2-1  | {'loss': 0.4854, 'grad_norm': 11.102492332458496, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 19:31:18 client1-1  | {'loss': 0.1717, 'grad_norm': 4.570643901824951, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 19:31:28 client1-1  | {'loss': 0.1756, 'grad_norm': 5.561885356903076, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 19:31:37 client1-1  | {'loss': 0.1624, 'grad_norm': 8.941224098205566, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 19:31:47 client1-1  | {'loss': 0.1562, 'grad_norm': 9.428109169006348, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 19:31:56 client1-1  | {'loss': 0.1943, 'grad_norm': 3.3046138286590576, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 19:32:06 client1-1  | {'loss': 0.2646, 'grad_norm': 7.61758279800415, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 19:32:15 client1-1  | {'loss': 0.1643, 'grad_norm': 5.325057029724121, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 19:32:28 client1-1  | {'loss': 0.1757, 'grad_norm': 3.4074652194976807, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 19:32:38 client1-1  | {'loss': 0.1956, 'grad_norm': 9.508447647094727, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 19:35:24 client2-1  | {'loss': 0.4903, 'grad_norm': 8.773664474487305, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 19:32:47 client1-1  | {'loss': 0.2134, 'grad_norm': 6.404685020446777, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 19:35:34 client2-1  | {'loss': 0.4795, 'grad_norm': 10.460330963134766, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 19:35:47 client2-1  | {'loss': 0.6697, 'grad_norm': 11.836368560791016, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 19:35:57 client2-1  | {'loss': 0.549, 'grad_norm': 9.444624900817871, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 19:36:06 client2-1  | {'loss': 0.6767, 'grad_norm': 9.336613655090332, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 19:32:57 client1-1  | {'loss': 0.2463, 'grad_norm': 6.031190395355225, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 19:36:16 client2-1  | {'loss': 0.7164, 'grad_norm': 18.292362213134766, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 19:36:26 client2-1  | {'loss': 0.7972, 'grad_norm': 13.309189796447754, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 19:36:35 client2-1  | {'loss': 0.8182, 'grad_norm': 12.276004791259766, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 19:36:44 client2-1  | {'loss': 0.8337, 'grad_norm': 12.279425621032715, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 19:36:54 client2-1  | {'loss': 0.9161, 'grad_norm': 11.290352821350098, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 19:37:07 client2-1  | {'loss': 0.9071, 'grad_norm': 12.306177139282227, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 19:33:06 client1-1  | {'loss': 0.2065, 'grad_norm': 5.789524555206299, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 19:33:16 client1-1  | {'loss': 0.2269, 'grad_norm': 11.167373657226562, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 19:33:25 client1-1  | {'loss': 0.2613, 'grad_norm': 8.043277740478516, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 19:33:39 client1-1  | {'loss': 0.2052, 'grad_norm': 4.330876350402832, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 19:37:16 client2-1  | {'loss': 1.1068, 'grad_norm': 16.279525756835938, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 19:37:26 client2-1  | {'loss': 1.2346, 'grad_norm': 14.963326454162598, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 19:33:48 client1-1  | {'loss': 0.2395, 'grad_norm': 6.0357346534729, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 19:33:58 client1-1  | {'loss': 0.2933, 'grad_norm': 10.87104606628418, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 19:34:07 client1-1  | {'loss': 0.2842, 'grad_norm': 5.190159320831299, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 19:34:17 client1-1  | {'loss': 0.3092, 'grad_norm': 8.524018287658691, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 19:37:35 client2-1  | {'loss': 0.781, 'grad_norm': 8.177372932434082, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 19:37:45 client2-1  | {'loss': 0.6346, 'grad_norm': 6.835404396057129, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 19:34:26 client1-1  | {'loss': 0.3409, 'grad_norm': 8.554259300231934, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 19:37:54 client2-1  | {'loss': 0.165, 'grad_norm': 2.701967477798462, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 19:34:39 client1-1  | {'loss': 0.3491, 'grad_norm': 8.812169075012207, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 19:37:54 client2-1  | {'train_runtime': 501.2985, 'train_samples_per_second': 1.995, 'train_steps_per_second': 0.997, 'train_loss': 0.3920053436756134, 'epoch': 1.0}
2025-05-13 19:38:02 client2-1  | INFO :      Sent reply
2025-05-13 19:38:14 client2-1  | INFO :      
2025-05-13 19:38:14 client2-1  | INFO :      Received: train message b18fa82a-bea9-4d76-bb09-0f2cff50ff85
2025-05-13 19:34:49 client1-1  | {'loss': 0.4023, 'grad_norm': 6.428786754608154, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 19:34:58 client1-1  | {'loss': 0.3996, 'grad_norm': 5.56641960144043, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 19:38:26 client2-1  | {'loss': 0.0731, 'grad_norm': 1.9575930833816528, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 19:38:39 client2-1  | {'loss': 0.0902, 'grad_norm': 2.2923479080200195, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 19:35:08 client1-1  | {'loss': 0.3303, 'grad_norm': 8.598998069763184, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 19:35:17 client1-1  | {'loss': 0.408, 'grad_norm': 9.061254501342773, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 19:35:27 client1-1  | {'loss': 0.4913, 'grad_norm': 13.264250755310059, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 19:35:36 client1-1  | {'loss': 0.5274, 'grad_norm': 10.394831657409668, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 19:35:50 client1-1  | {'loss': 0.6052, 'grad_norm': 12.886725425720215, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 19:38:49 client2-1  | {'loss': 0.1481, 'grad_norm': 11.583450317382812, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 19:38:58 client2-1  | {'loss': 0.1311, 'grad_norm': 7.090421199798584, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 19:39:08 client2-1  | {'loss': 0.1403, 'grad_norm': 7.0750627517700195, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 19:39:17 client2-1  | {'loss': 0.1221, 'grad_norm': 4.684131622314453, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 19:39:27 client2-1  | {'loss': 0.1273, 'grad_norm': 7.780203342437744, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 19:39:40 client2-1  | {'loss': 0.1392, 'grad_norm': 5.283475399017334, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 19:39:50 client2-1  | {'loss': 0.1218, 'grad_norm': 8.671142578125, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 19:39:59 client2-1  | {'loss': 0.1182, 'grad_norm': 4.9481306076049805, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 19:35:59 client1-1  | {'loss': 0.6068, 'grad_norm': 14.62425708770752, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 19:36:09 client1-1  | {'loss': 0.6884, 'grad_norm': 14.705214500427246, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 19:40:09 client2-1  | {'loss': 0.1259, 'grad_norm': 4.009927272796631, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 19:40:18 client2-1  | {'loss': 0.1546, 'grad_norm': 5.663377285003662, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 19:40:28 client2-1  | {'loss': 0.1602, 'grad_norm': 7.844029903411865, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 19:40:41 client2-1  | {'loss': 0.1407, 'grad_norm': 3.403203010559082, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 19:40:51 client2-1  | {'loss': 0.185, 'grad_norm': 5.395589828491211, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 19:41:00 client2-1  | {'loss': 0.1433, 'grad_norm': 5.746744155883789, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 19:41:10 client2-1  | {'loss': 0.1652, 'grad_norm': 7.261127948760986, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 19:41:19 client2-1  | {'loss': 0.1709, 'grad_norm': 4.989716529846191, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 19:41:29 client2-1  | {'loss': 0.1998, 'grad_norm': 7.421208381652832, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 19:41:42 client2-1  | {'loss': 0.1772, 'grad_norm': 6.937760829925537, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 19:41:51 client2-1  | {'loss': 0.2112, 'grad_norm': 8.025486946105957, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 19:42:00 client2-1  | {'loss': 0.2161, 'grad_norm': 12.931894302368164, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 19:42:10 client2-1  | {'loss': 0.1911, 'grad_norm': 8.624523162841797, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 19:36:18 client1-1  | {'loss': 0.6509, 'grad_norm': 11.708394050598145, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 19:36:28 client1-1  | {'loss': 0.807, 'grad_norm': 12.456143379211426, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 19:36:37 client1-1  | {'loss': 0.8915, 'grad_norm': 12.748641967773438, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 19:36:51 client1-1  | {'loss': 0.8349, 'grad_norm': 10.555323600769043, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 19:37:00 client1-1  | {'loss': 1.0656, 'grad_norm': 17.060483932495117, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 19:42:19 client2-1  | {'loss': 0.244, 'grad_norm': 6.262292861938477, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 19:37:10 client1-1  | {'loss': 0.963, 'grad_norm': 16.064542770385742, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 19:37:19 client1-1  | {'loss': 1.1219, 'grad_norm': 12.687129974365234, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 19:37:29 client1-1  | {'loss': 1.1082, 'grad_norm': 15.249874114990234, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 19:42:28 client2-1  | {'loss': 0.2312, 'grad_norm': 5.285948753356934, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 19:42:38 client2-1  | {'loss': 0.2019, 'grad_norm': 8.6733980178833, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 19:42:51 client2-1  | {'loss': 0.238, 'grad_norm': 8.837475776672363, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 19:37:38 client1-1  | {'loss': 1.025, 'grad_norm': 10.054182052612305, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 19:37:48 client1-1  | {'loss': 0.609, 'grad_norm': 10.342729568481445, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 19:43:00 client2-1  | {'loss': 0.2708, 'grad_norm': 7.389167308807373, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 19:43:10 client2-1  | {'loss': 0.2733, 'grad_norm': 9.592247009277344, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 19:43:19 client2-1  | {'loss': 0.2951, 'grad_norm': 9.848073959350586, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 19:38:01 client1-1  | {'loss': 0.1704, 'grad_norm': 3.2674052715301514, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 19:38:01 client1-1  | {'train_runtime': 508.5359, 'train_samples_per_second': 1.966, 'train_steps_per_second': 0.983, 'train_loss': 0.3947677749395371, 'epoch': 1.0}
2025-05-13 19:38:04 client1-1  | INFO :      Sent reply
2025-05-13 19:38:14 client1-1  | INFO :      
2025-05-13 19:43:29 client2-1  | {'loss': 0.3117, 'grad_norm': 12.256017684936523, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 19:43:38 client2-1  | {'loss': 0.32, 'grad_norm': 9.684041023254395, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 19:38:14 client1-1  | INFO :      Received: train message 05f6cf36-8814-45bd-bce3-8f0e8a790c27
2025-05-13 19:43:51 client2-1  | {'loss': 0.3493, 'grad_norm': 6.723934650421143, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 19:44:01 client2-1  | {'loss': 0.4322, 'grad_norm': 11.339975357055664, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 19:44:10 client2-1  | {'loss': 0.4302, 'grad_norm': 10.437193870544434, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 19:38:28 client1-1  | {'loss': 0.0812, 'grad_norm': 1.3531877994537354, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 19:38:37 client1-1  | {'loss': 0.0946, 'grad_norm': 2.7681546211242676, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 19:38:47 client1-1  | {'loss': 0.1154, 'grad_norm': 3.7654924392700195, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 19:39:00 client1-1  | {'loss': 0.1409, 'grad_norm': 6.073286533355713, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 19:39:10 client1-1  | {'loss': 0.1276, 'grad_norm': 6.196876049041748, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 19:44:20 client2-1  | {'loss': 0.424, 'grad_norm': 11.11582088470459, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 19:44:29 client2-1  | {'loss': 0.6145, 'grad_norm': 11.063382148742676, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 19:44:39 client2-1  | {'loss': 0.4869, 'grad_norm': 8.38728141784668, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 19:39:19 client1-1  | {'loss': 0.1173, 'grad_norm': 4.382692813873291, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 19:39:29 client1-1  | {'loss': 0.1588, 'grad_norm': 5.439889430999756, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 19:39:38 client1-1  | {'loss': 0.131, 'grad_norm': 3.0111770629882812, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 19:39:48 client1-1  | {'loss': 0.1534, 'grad_norm': 2.9957900047302246, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 19:44:48 client2-1  | {'loss': 0.6153, 'grad_norm': 8.74696159362793, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 19:45:01 client2-1  | {'loss': 0.6423, 'grad_norm': 13.611061096191406, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 19:45:11 client2-1  | {'loss': 0.7416, 'grad_norm': 12.0263090133667, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 19:40:01 client1-1  | {'loss': 0.1483, 'grad_norm': 5.850634574890137, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 19:40:11 client1-1  | {'loss': 0.1581, 'grad_norm': 5.174561500549316, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 19:40:20 client1-1  | {'loss': 0.1399, 'grad_norm': 7.612504959106445, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 19:40:30 client1-1  | {'loss': 0.14, 'grad_norm': 7.2261528968811035, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 19:45:20 client2-1  | {'loss': 0.7831, 'grad_norm': 11.865114212036133, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 19:40:39 client1-1  | {'loss': 0.169, 'grad_norm': 8.054157257080078, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 19:45:30 client2-1  | {'loss': 0.7971, 'grad_norm': 12.862062454223633, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 19:40:49 client1-1  | {'loss': 0.2213, 'grad_norm': 7.966238498687744, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 19:41:02 client1-1  | {'loss': 0.147, 'grad_norm': 5.115030765533447, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 19:41:12 client1-1  | {'loss': 0.1426, 'grad_norm': 2.5307834148406982, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 19:45:39 client2-1  | {'loss': 0.8834, 'grad_norm': 12.786422729492188, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 19:45:49 client2-1  | {'loss': 0.8614, 'grad_norm': 12.573369979858398, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 19:45:58 client2-1  | {'loss': 1.0818, 'grad_norm': 17.23196029663086, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 19:46:12 client2-1  | {'loss': 1.2038, 'grad_norm': 14.742995262145996, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 19:46:21 client2-1  | {'loss': 0.749, 'grad_norm': 7.508885860443115, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 19:46:31 client2-1  | {'loss': 0.5785, 'grad_norm': 6.533562660217285, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 19:46:41 client2-1  | {'loss': 0.1432, 'grad_norm': 3.9232723712921143, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 19:46:41 client2-1  | {'train_runtime': 504.3666, 'train_samples_per_second': 1.983, 'train_steps_per_second': 0.991, 'train_loss': 0.3531201294660568, 'epoch': 1.0}
2025-05-13 19:46:49 client2-1  | INFO :      Sent reply
2025-05-13 19:47:01 client2-1  | INFO :      
2025-05-13 19:41:21 client1-1  | {'loss': 0.183, 'grad_norm': 6.848745346069336, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 19:41:31 client1-1  | {'loss': 0.1834, 'grad_norm': 5.215826034545898, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 19:41:40 client1-1  | {'loss': 0.2045, 'grad_norm': 7.097240924835205, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 19:47:01 client2-1  | INFO :      Received: train message 670f0a93-8b68-4aaf-b1dd-3a3c13f8cc31
2025-05-13 19:41:49 client1-1  | {'loss': 0.1647, 'grad_norm': 3.5587143898010254, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 19:47:12 client2-1  | {'loss': 0.0672, 'grad_norm': 3.0257396697998047, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 19:42:02 client1-1  | {'loss': 0.2011, 'grad_norm': 9.750736236572266, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 19:42:12 client1-1  | {'loss': 0.2275, 'grad_norm': 8.052868843078613, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 19:42:21 client1-1  | {'loss': 0.1711, 'grad_norm': 3.7126429080963135, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 19:42:30 client1-1  | {'loss': 0.1818, 'grad_norm': 4.5466203689575195, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 19:42:40 client1-1  | {'loss': 0.242, 'grad_norm': 10.377534866333008, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 19:42:49 client1-1  | {'loss': 0.2263, 'grad_norm': 4.292240142822266, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 19:47:21 client2-1  | {'loss': 0.0817, 'grad_norm': 2.5035929679870605, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 19:47:34 client2-1  | {'loss': 0.1168, 'grad_norm': 9.29073429107666, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 19:47:44 client2-1  | {'loss': 0.1074, 'grad_norm': 3.2986040115356445, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 19:42:59 client1-1  | {'loss': 0.2693, 'grad_norm': 7.346335411071777, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 19:43:12 client1-1  | {'loss': 0.2931, 'grad_norm': 9.771998405456543, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 19:43:21 client1-1  | {'loss': 0.2858, 'grad_norm': 8.240106582641602, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 19:43:31 client1-1  | {'loss': 0.3391, 'grad_norm': 5.097256660461426, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 19:43:40 client1-1  | {'loss': 0.3485, 'grad_norm': 4.928640365600586, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 19:47:54 client2-1  | {'loss': 0.1138, 'grad_norm': 4.846398830413818, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 19:43:50 client1-1  | {'loss': 0.2806, 'grad_norm': 6.658801078796387, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 19:43:59 client1-1  | {'loss': 0.3668, 'grad_norm': 7.940769195556641, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 19:44:12 client1-1  | {'loss': 0.4139, 'grad_norm': 11.707812309265137, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 19:48:03 client2-1  | {'loss': 0.1114, 'grad_norm': 3.3412437438964844, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 19:44:22 client1-1  | {'loss': 0.4547, 'grad_norm': 8.682246208190918, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 19:48:13 client2-1  | {'loss': 0.1133, 'grad_norm': 3.1708457469940186, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 19:44:31 client1-1  | {'loss': 0.5399, 'grad_norm': 14.004633903503418, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 19:44:41 client1-1  | {'loss': 0.5343, 'grad_norm': 14.345359802246094, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 19:44:50 client1-1  | {'loss': 0.6295, 'grad_norm': 13.636951446533203, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 19:48:22 client2-1  | {'loss': 0.1196, 'grad_norm': 5.342988014221191, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 19:48:36 client2-1  | {'loss': 0.1029, 'grad_norm': 5.623427867889404, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 19:45:00 client1-1  | {'loss': 0.5707, 'grad_norm': 11.891310691833496, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 19:45:13 client1-1  | {'loss': 0.738, 'grad_norm': 13.188952445983887, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 19:45:22 client1-1  | {'loss': 0.8353, 'grad_norm': 11.485909461975098, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 19:45:32 client1-1  | {'loss': 0.7853, 'grad_norm': 12.064327239990234, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 19:45:41 client1-1  | {'loss': 1.0315, 'grad_norm': 16.021207809448242, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 19:45:51 client1-1  | {'loss': 0.94, 'grad_norm': 16.658512115478516, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 19:46:01 client1-1  | {'loss': 1.0861, 'grad_norm': 13.384830474853516, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 19:48:45 client2-1  | {'loss': 0.1093, 'grad_norm': 3.8063313961029053, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 19:46:14 client1-1  | {'loss': 1.0978, 'grad_norm': 15.267546653747559, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 19:48:55 client2-1  | {'loss': 0.1191, 'grad_norm': 4.130466938018799, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 19:49:04 client2-1  | {'loss': 0.1254, 'grad_norm': 3.8438687324523926, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 19:49:14 client2-1  | {'loss': 0.1193, 'grad_norm': 4.398027420043945, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 19:46:24 client1-1  | {'loss': 0.9483, 'grad_norm': 11.793789863586426, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 19:46:34 client1-1  | {'loss': 0.5529, 'grad_norm': 8.454483985900879, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 19:49:23 client2-1  | {'loss': 0.1301, 'grad_norm': 3.06675386428833, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 19:49:37 client2-1  | {'loss': 0.1536, 'grad_norm': 7.226968288421631, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 19:49:46 client2-1  | {'loss': 0.1349, 'grad_norm': 7.510830879211426, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 19:49:56 client2-1  | {'loss': 0.1531, 'grad_norm': 8.37810230255127, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 19:46:43 client1-1  | {'loss': 0.1343, 'grad_norm': 4.71312141418457, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 19:46:43 client1-1  | {'train_runtime': 507.7617, 'train_samples_per_second': 1.969, 'train_steps_per_second': 0.985, 'train_loss': 0.3569510478973389, 'epoch': 1.0}
2025-05-13 19:46:50 client1-1  | INFO :      Sent reply
2025-05-13 19:47:01 client1-1  | INFO :      
2025-05-13 19:47:01 client1-1  | INFO :      Received: train message 5966211b-c43a-4cb9-b63d-4b13405a2a88
2025-05-13 19:50:05 client2-1  | {'loss': 0.1513, 'grad_norm': 6.153984546661377, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 19:50:14 client2-1  | {'loss': 0.187, 'grad_norm': 7.076351642608643, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 19:50:24 client2-1  | {'loss': 0.1554, 'grad_norm': 8.602983474731445, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 19:47:14 client1-1  | {'loss': 0.0856, 'grad_norm': 1.9332975149154663, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 19:50:33 client2-1  | {'loss': 0.1879, 'grad_norm': 8.84950065612793, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 19:47:23 client1-1  | {'loss': 0.0918, 'grad_norm': 1.9113866090774536, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 19:47:33 client1-1  | {'loss': 0.104, 'grad_norm': 4.92067813873291, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 19:47:46 client1-1  | {'loss': 0.1163, 'grad_norm': 4.217600345611572, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 19:47:56 client1-1  | {'loss': 0.1119, 'grad_norm': 4.3578691482543945, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 19:48:05 client1-1  | {'loss': 0.1076, 'grad_norm': 5.905045986175537, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 19:50:46 client2-1  | {'loss': 0.1763, 'grad_norm': 10.339838981628418, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 19:50:56 client2-1  | {'loss': 0.1679, 'grad_norm': 6.2599029541015625, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 19:48:15 client1-1  | {'loss': 0.1332, 'grad_norm': 4.066195964813232, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 19:51:05 client2-1  | {'loss': 0.1995, 'grad_norm': 6.754398345947266, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 19:51:14 client2-1  | {'loss': 0.1966, 'grad_norm': 4.84458065032959, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 19:51:24 client2-1  | {'loss': 0.1888, 'grad_norm': 11.598063468933105, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 19:51:33 client2-1  | {'loss': 0.1911, 'grad_norm': 11.590095520019531, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 19:51:43 client2-1  | {'loss': 0.2355, 'grad_norm': 6.33526611328125, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 19:51:56 client2-1  | {'loss': 0.2314, 'grad_norm': 10.538501739501953, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 19:48:24 client1-1  | {'loss': 0.1086, 'grad_norm': 2.5323708057403564, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 19:52:05 client2-1  | {'loss': 0.2385, 'grad_norm': 8.3327054977417, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 19:48:34 client1-1  | {'loss': 0.1393, 'grad_norm': 4.9463090896606445, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 19:52:15 client2-1  | {'loss': 0.2601, 'grad_norm': 11.04467487335205, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 19:52:24 client2-1  | {'loss': 0.2555, 'grad_norm': 9.964594841003418, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 19:52:34 client2-1  | {'loss': 0.2818, 'grad_norm': 8.311820030212402, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 19:52:43 client2-1  | {'loss': 0.367, 'grad_norm': 10.089652061462402, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 19:52:52 client2-1  | {'loss': 0.3768, 'grad_norm': 7.769001483917236, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 19:53:05 client2-1  | {'loss': 0.3603, 'grad_norm': 10.133402824401855, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 19:53:15 client2-1  | {'loss': 0.5364, 'grad_norm': 11.453168869018555, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 19:53:24 client2-1  | {'loss': 0.4244, 'grad_norm': 9.625744819641113, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 19:48:44 client1-1  | {'loss': 0.1428, 'grad_norm': 3.063978433609009, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 19:53:34 client2-1  | {'loss': 0.5525, 'grad_norm': 8.967270851135254, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 19:48:57 client1-1  | {'loss': 0.1428, 'grad_norm': 4.221229076385498, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 19:49:06 client1-1  | {'loss': 0.1383, 'grad_norm': 9.516918182373047, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 19:49:16 client1-1  | {'loss': 0.1211, 'grad_norm': 5.198321342468262, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 19:49:25 client1-1  | {'loss': 0.1336, 'grad_norm': 3.58947491645813, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 19:49:35 client1-1  | {'loss': 0.1866, 'grad_norm': 5.61070442199707, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 19:49:45 client1-1  | {'loss': 0.1246, 'grad_norm': 5.506211280822754, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 19:49:54 client1-1  | {'loss': 0.1139, 'grad_norm': 3.7340025901794434, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 19:50:07 client1-1  | {'loss': 0.1439, 'grad_norm': 6.9302897453308105, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 19:50:16 client1-1  | {'loss': 0.1499, 'grad_norm': 4.861894130706787, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 19:50:26 client1-1  | {'loss': 0.1746, 'grad_norm': 5.636173725128174, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 19:53:43 client2-1  | {'loss': 0.6064, 'grad_norm': 19.47083854675293, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 19:53:53 client2-1  | {'loss': 0.6961, 'grad_norm': 13.340822219848633, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 19:54:02 client2-1  | {'loss': 0.7285, 'grad_norm': 12.301652908325195, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 19:50:35 client1-1  | {'loss': 0.139, 'grad_norm': 2.5040862560272217, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 19:50:44 client1-1  | {'loss': 0.1627, 'grad_norm': 8.07823371887207, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 19:54:15 client2-1  | {'loss': 0.7486, 'grad_norm': 13.587777137756348, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 19:54:25 client2-1  | {'loss': 0.843, 'grad_norm': 11.946976661682129, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 19:50:54 client1-1  | {'loss': 0.1959, 'grad_norm': 7.190443515777588, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 19:51:07 client1-1  | {'loss': 0.15, 'grad_norm': 4.523080825805664, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 19:51:16 client1-1  | {'loss': 0.1709, 'grad_norm': 3.632370710372925, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 19:51:26 client1-1  | {'loss': 0.2079, 'grad_norm': 12.739433288574219, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 19:51:35 client1-1  | {'loss': 0.1913, 'grad_norm': 4.503016471862793, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 19:51:45 client1-1  | {'loss': 0.2203, 'grad_norm': 7.695331573486328, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 19:51:54 client1-1  | {'loss': 0.2555, 'grad_norm': 8.35386848449707, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 19:52:08 client1-1  | {'loss': 0.2294, 'grad_norm': 6.373444557189941, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 19:52:17 client1-1  | {'loss': 0.2916, 'grad_norm': 5.868600845336914, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 19:52:26 client1-1  | {'loss': 0.2855, 'grad_norm': 4.038851261138916, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 19:52:36 client1-1  | {'loss': 0.2262, 'grad_norm': 7.607985973358154, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 19:52:45 client1-1  | {'loss': 0.3124, 'grad_norm': 7.434456825256348, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 19:52:58 client1-1  | {'loss': 0.366, 'grad_norm': 11.629693031311035, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 19:54:34 client2-1  | {'loss': 0.8278, 'grad_norm': 13.74166202545166, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 19:54:44 client2-1  | {'loss': 1.0692, 'grad_norm': 17.86903190612793, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 19:53:08 client1-1  | {'loss': 0.405, 'grad_norm': 9.629291534423828, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 19:54:53 client2-1  | {'loss': 1.1533, 'grad_norm': 15.579629898071289, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 19:53:17 client1-1  | {'loss': 0.475, 'grad_norm': 11.724145889282227, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 19:53:27 client1-1  | {'loss': 0.48, 'grad_norm': 17.397523880004883, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 19:53:36 client1-1  | {'loss': 0.5808, 'grad_norm': 13.986310958862305, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 19:55:03 client2-1  | {'loss': 0.7267, 'grad_norm': 7.8319902420043945, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 19:55:12 client2-1  | {'loss': 0.5194, 'grad_norm': 6.6757097244262695, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 19:53:46 client1-1  | {'loss': 0.5131, 'grad_norm': 12.272838592529297, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 19:55:26 client2-1  | {'loss': 0.1314, 'grad_norm': 5.110487937927246, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 19:53:59 client1-1  | {'loss': 0.6945, 'grad_norm': 12.085737228393555, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 19:55:26 client2-1  | {'train_runtime': 503.4657, 'train_samples_per_second': 1.986, 'train_steps_per_second': 0.993, 'train_loss': 0.3190271055698395, 'epoch': 1.0}
2025-05-13 19:55:31 client2-1  | INFO :      Sent reply
2025-05-13 19:55:43 client2-1  | INFO :      
2025-05-13 19:54:09 client1-1  | {'loss': 0.7872, 'grad_norm': 12.451437950134277, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 19:55:43 client2-1  | INFO :      Received: train message 978d7150-1623-4511-a7a3-fbece55e0a69
2025-05-13 19:54:18 client1-1  | {'loss': 0.7262, 'grad_norm': 11.854098320007324, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 19:55:54 client2-1  | {'loss': 0.0688, 'grad_norm': 1.794573187828064, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 19:56:04 client2-1  | {'loss': 0.0759, 'grad_norm': 1.8813164234161377, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 19:54:28 client1-1  | {'loss': 0.9845, 'grad_norm': 16.53278160095215, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 19:54:37 client1-1  | {'loss': 0.9074, 'grad_norm': 16.63304328918457, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 19:54:47 client1-1  | {'loss': 1.0437, 'grad_norm': 13.08305835723877, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 19:55:00 client1-1  | {'loss': 1.0623, 'grad_norm': 17.64036750793457, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 19:55:09 client1-1  | {'loss': 0.9191, 'grad_norm': 9.24781608581543, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 19:55:19 client1-1  | {'loss': 0.4701, 'grad_norm': 6.611222743988037, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 19:55:28 client1-1  | {'loss': 0.1234, 'grad_norm': 7.795823097229004, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 19:55:28 client1-1  | {'train_runtime': 506.6442, 'train_samples_per_second': 1.974, 'train_steps_per_second': 0.987, 'train_loss': 0.3229481745958328, 'epoch': 1.0}
2025-05-13 19:55:31 client1-1  | INFO :      Sent reply
2025-05-13 19:56:17 client2-1  | {'loss': 0.1003, 'grad_norm': 4.5385847091674805, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 19:55:43 client1-1  | INFO :      
2025-05-13 19:55:43 client1-1  | INFO :      Received: train message cb626de7-f133-41fb-b0a8-d1e8c9ef3595
2025-05-13 19:55:56 client1-1  | {'loss': 0.0766, 'grad_norm': 1.8598555326461792, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 19:56:27 client2-1  | {'loss': 0.1069, 'grad_norm': 5.624330997467041, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 19:56:37 client2-1  | {'loss': 0.12, 'grad_norm': 5.29412841796875, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 19:56:46 client2-1  | {'loss': 0.1029, 'grad_norm': 6.36543083190918, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 19:56:56 client2-1  | {'loss': 0.105, 'grad_norm': 2.531285524368286, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 19:57:09 client2-1  | {'loss': 0.1029, 'grad_norm': 4.281069278717041, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 19:56:06 client1-1  | {'loss': 0.0781, 'grad_norm': 1.905531883239746, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 19:57:18 client2-1  | {'loss': 0.0869, 'grad_norm': 3.527601957321167, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 19:57:28 client2-1  | {'loss': 0.1099, 'grad_norm': 5.207964897155762, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 19:57:37 client2-1  | {'loss': 0.1212, 'grad_norm': 5.259578704833984, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 19:57:47 client2-1  | {'loss': 0.1297, 'grad_norm': 2.924046754837036, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 19:57:56 client2-1  | {'loss': 0.115, 'grad_norm': 6.065047740936279, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 19:58:06 client2-1  | {'loss': 0.1172, 'grad_norm': 2.394892454147339, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 19:58:15 client2-1  | {'loss': 0.1328, 'grad_norm': 4.263392448425293, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 19:56:19 client1-1  | {'loss': 0.1115, 'grad_norm': 4.6477885246276855, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 19:56:29 client1-1  | {'loss': 0.0985, 'grad_norm': 3.553083896636963, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 19:58:28 client2-1  | {'loss': 0.1188, 'grad_norm': 3.2794747352600098, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 19:58:37 client2-1  | {'loss': 0.1339, 'grad_norm': 8.317455291748047, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 19:58:47 client2-1  | {'loss': 0.138, 'grad_norm': 5.523324966430664, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 19:58:56 client2-1  | {'loss': 0.1507, 'grad_norm': 5.7884745597839355, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 19:56:39 client1-1  | {'loss': 0.1144, 'grad_norm': 6.4102349281311035, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 19:56:48 client1-1  | {'loss': 0.0986, 'grad_norm': 5.550545692443848, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 19:59:06 client2-1  | {'loss': 0.1401, 'grad_norm': 5.116838455200195, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 19:59:15 client2-1  | {'loss': 0.1775, 'grad_norm': 6.397514820098877, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 19:59:25 client2-1  | {'loss': 0.1646, 'grad_norm': 12.939675331115723, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 19:59:38 client2-1  | {'loss': 0.1431, 'grad_norm': 8.819385528564453, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 19:59:48 client2-1  | {'loss': 0.1597, 'grad_norm': 4.618686199188232, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 19:56:58 client1-1  | {'loss': 0.1215, 'grad_norm': 2.9889841079711914, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 19:57:07 client1-1  | {'loss': 0.0934, 'grad_norm': 1.656562328338623, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 19:57:17 client1-1  | {'loss': 0.1316, 'grad_norm': 3.388282060623169, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 19:57:30 client1-1  | {'loss': 0.1192, 'grad_norm': 3.2345869541168213, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 19:57:40 client1-1  | {'loss': 0.1261, 'grad_norm': 5.053160667419434, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 19:59:57 client2-1  | {'loss': 0.1593, 'grad_norm': 4.528489589691162, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 20:00:07 client2-1  | {'loss': 0.1627, 'grad_norm': 7.844100475311279, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 19:57:49 client1-1  | {'loss': 0.125, 'grad_norm': 5.424890995025635, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 19:57:58 client1-1  | {'loss': 0.1074, 'grad_norm': 4.861943244934082, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 19:58:08 client1-1  | {'loss': 0.1232, 'grad_norm': 3.487514019012451, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 19:58:17 client1-1  | {'loss': 0.1616, 'grad_norm': 4.926933288574219, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 19:58:30 client1-1  | {'loss': 0.1199, 'grad_norm': 5.865431308746338, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 20:00:16 client2-1  | {'loss': 0.1809, 'grad_norm': 10.248451232910156, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 20:00:26 client2-1  | {'loss': 0.2033, 'grad_norm': 6.268903732299805, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 19:58:40 client1-1  | {'loss': 0.117, 'grad_norm': 2.8065905570983887, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 19:58:49 client1-1  | {'loss': 0.1423, 'grad_norm': 6.461563587188721, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 19:58:59 client1-1  | {'loss': 0.1396, 'grad_norm': 3.7880194187164307, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 19:59:08 client1-1  | {'loss': 0.1652, 'grad_norm': 4.895910739898682, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 19:59:22 client1-1  | {'loss': 0.1326, 'grad_norm': 4.077884197235107, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 19:59:31 client1-1  | {'loss': 0.1514, 'grad_norm': 11.718244552612305, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 19:59:41 client1-1  | {'loss': 0.1635, 'grad_norm': 6.114681243896484, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 19:59:50 client1-1  | {'loss': 0.1472, 'grad_norm': 5.650852203369141, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 20:00:00 client1-1  | {'loss': 0.1581, 'grad_norm': 11.39673900604248, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 20:00:35 client2-1  | {'loss': 0.1932, 'grad_norm': 9.54133415222168, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 20:00:44 client2-1  | {'loss': 0.2097, 'grad_norm': 8.48556137084961, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 20:00:58 client2-1  | {'loss': 0.2353, 'grad_norm': 11.983135223388672, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 20:00:09 client1-1  | {'loss': 0.1757, 'grad_norm': 11.26733112335205, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 20:01:07 client2-1  | {'loss': 0.2286, 'grad_norm': 9.302948951721191, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 20:00:19 client1-1  | {'loss': 0.1657, 'grad_norm': 3.95177960395813, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 20:01:17 client2-1  | {'loss': 0.2417, 'grad_norm': 5.208597183227539, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 20:01:26 client2-1  | {'loss': 0.3174, 'grad_norm': 10.366085052490234, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 20:01:35 client2-1  | {'loss': 0.3199, 'grad_norm': 6.095419883728027, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 20:01:45 client2-1  | {'loss': 0.3318, 'grad_norm': 10.802321434020996, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 20:01:54 client2-1  | {'loss': 0.483, 'grad_norm': 11.671773910522461, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 20:02:07 client2-1  | {'loss': 0.3756, 'grad_norm': 8.1229887008667, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 20:02:17 client2-1  | {'loss': 0.4902, 'grad_norm': 8.38266372680664, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 20:00:32 client1-1  | {'loss': 0.1847, 'grad_norm': 5.902764320373535, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 20:02:26 client2-1  | {'loss': 0.5238, 'grad_norm': 16.53366470336914, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 20:02:36 client2-1  | {'loss': 0.6083, 'grad_norm': 13.480703353881836, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 20:02:45 client2-1  | {'loss': 0.6806, 'grad_norm': 13.436490058898926, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 20:02:55 client2-1  | {'loss': 0.6966, 'grad_norm': 13.220365524291992, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 20:03:08 client2-1  | {'loss': 0.7992, 'grad_norm': 12.178096771240234, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 20:03:18 client2-1  | {'loss': 0.8123, 'grad_norm': 12.889902114868164, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 20:03:27 client2-1  | {'loss': 1.0287, 'grad_norm': 17.47281837463379, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 20:03:37 client2-1  | {'loss': 1.1344, 'grad_norm': 14.451761245727539, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 20:03:46 client2-1  | {'loss': 0.6736, 'grad_norm': 6.855171203613281, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 20:03:56 client2-1  | {'loss': 0.4702, 'grad_norm': 5.911712646484375, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 20:04:09 client2-1  | {'loss': 0.1172, 'grad_norm': 4.059147357940674, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 20:00:41 client1-1  | {'loss': 0.1939, 'grad_norm': 7.912331581115723, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 20:04:09 client2-1  | {'train_runtime': 504.856, 'train_samples_per_second': 1.981, 'train_steps_per_second': 0.99, 'train_loss': 0.29198795795440674, 'epoch': 1.0}
2025-05-13 20:04:14 client2-1  | INFO :      Sent reply
2025-05-13 20:04:26 client2-1  | INFO :      
2025-05-13 20:04:26 client2-1  | INFO :      Received: train message 2c484bc0-0290-491e-94b9-3a415a610304
2025-05-13 20:04:37 client2-1  | {'loss': 0.0657, 'grad_norm': 1.776265263557434, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 20:04:51 client2-1  | {'loss': 0.071, 'grad_norm': 3.678973436355591, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 20:05:00 client2-1  | {'loss': 0.1042, 'grad_norm': 7.1141157150268555, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 20:05:10 client2-1  | {'loss': 0.1185, 'grad_norm': 4.526418209075928, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 20:05:19 client2-1  | {'loss': 0.1017, 'grad_norm': 5.3418869972229, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 20:05:29 client2-1  | {'loss': 0.1046, 'grad_norm': 4.6565632820129395, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 20:05:38 client2-1  | {'loss': 0.0985, 'grad_norm': 3.27645206451416, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 20:05:48 client2-1  | {'loss': 0.1058, 'grad_norm': 3.1896345615386963, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 20:06:01 client2-1  | {'loss': 0.0919, 'grad_norm': 4.4966630935668945, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 20:06:11 client2-1  | {'loss': 0.1024, 'grad_norm': 6.9456048011779785, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 20:06:20 client2-1  | {'loss': 0.113, 'grad_norm': 3.9594027996063232, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 20:06:30 client2-1  | {'loss': 0.1173, 'grad_norm': 4.519459247589111, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 20:06:39 client2-1  | {'loss': 0.1068, 'grad_norm': 3.740668296813965, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 20:06:49 client2-1  | {'loss': 0.0993, 'grad_norm': 3.4326858520507812, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 20:07:02 client2-1  | {'loss': 0.1291, 'grad_norm': 5.675925254821777, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 20:07:12 client2-1  | {'loss': 0.1129, 'grad_norm': 3.111950635910034, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 20:00:51 client1-1  | {'loss': 0.1967, 'grad_norm': 7.24600076675415, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 20:07:21 client2-1  | {'loss': 0.1249, 'grad_norm': 7.045942306518555, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 20:01:00 client1-1  | {'loss': 0.2252, 'grad_norm': 5.110939025878906, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 20:01:10 client1-1  | {'loss': 0.2591, 'grad_norm': 7.030148506164551, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 20:01:19 client1-1  | {'loss': 0.2012, 'grad_norm': 6.171935558319092, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 20:01:29 client1-1  | {'loss': 0.2587, 'grad_norm': 7.069362163543701, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 20:01:42 client1-1  | {'loss': 0.315, 'grad_norm': 12.895055770874023, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 20:01:51 client1-1  | {'loss': 0.3425, 'grad_norm': 7.4877214431762695, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 20:07:31 client2-1  | {'loss': 0.1353, 'grad_norm': 5.914148807525635, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 20:07:40 client2-1  | {'loss': 0.1293, 'grad_norm': 5.592911720275879, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 20:07:50 client2-1  | {'loss': 0.119, 'grad_norm': 4.302080154418945, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 20:07:59 client2-1  | {'loss': 0.1561, 'grad_norm': 7.54920768737793, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 20:02:01 client1-1  | {'loss': 0.4047, 'grad_norm': 11.514723777770996, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 20:02:10 client1-1  | {'loss': 0.4303, 'grad_norm': 15.47472095489502, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 20:02:20 client1-1  | {'loss': 0.5127, 'grad_norm': 12.010007858276367, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 20:08:12 client2-1  | {'loss': 0.1602, 'grad_norm': 13.539860725402832, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 20:08:22 client2-1  | {'loss': 0.1459, 'grad_norm': 8.335245132446289, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 20:02:33 client1-1  | {'loss': 0.4503, 'grad_norm': 10.9799222946167, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 20:08:32 client2-1  | {'loss': 0.1728, 'grad_norm': 3.830328941345215, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 20:02:43 client1-1  | {'loss': 0.6279, 'grad_norm': 12.658208847045898, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 20:02:52 client1-1  | {'loss': 0.7386, 'grad_norm': 11.52112102508545, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 20:08:41 client2-1  | {'loss': 0.1622, 'grad_norm': 4.818036079406738, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 20:08:51 client2-1  | {'loss': 0.1493, 'grad_norm': 9.604548454284668, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 20:09:00 client2-1  | {'loss': 0.156, 'grad_norm': 9.473409652709961, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 20:09:10 client2-1  | {'loss': 0.179, 'grad_norm': 7.970542907714844, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 20:09:23 client2-1  | {'loss': 0.1716, 'grad_norm': 6.288366794586182, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 20:09:32 client2-1  | {'loss': 0.1818, 'grad_norm': 8.035096168518066, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 20:03:02 client1-1  | {'loss': 0.6829, 'grad_norm': 11.021143913269043, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 20:03:11 client1-1  | {'loss': 0.9302, 'grad_norm': 17.580814361572266, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 20:03:21 client1-1  | {'loss': 0.873, 'grad_norm': 19.055557250976562, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 20:09:42 client2-1  | {'loss': 0.1854, 'grad_norm': 9.338113784790039, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 20:09:52 client2-1  | {'loss': 0.2149, 'grad_norm': 9.374090194702148, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 20:10:01 client2-1  | {'loss': 0.2006, 'grad_norm': 6.159064292907715, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 20:03:30 client1-1  | {'loss': 1.0417, 'grad_norm': 14.555913925170898, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 20:03:44 client1-1  | {'loss': 1.0373, 'grad_norm': 16.052003860473633, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 20:03:53 client1-1  | {'loss': 0.8859, 'grad_norm': 10.182432174682617, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 20:04:03 client1-1  | {'loss': 0.4299, 'grad_norm': 8.020381927490234, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 20:04:12 client1-1  | {'loss': 0.1094, 'grad_norm': 3.697100877761841, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 20:04:12 client1-1  | {'train_runtime': 508.1295, 'train_samples_per_second': 1.968, 'train_steps_per_second': 0.984, 'train_loss': 0.29593988120555875, 'epoch': 1.0}
2025-05-13 20:10:10 client2-1  | {'loss': 0.2653, 'grad_norm': 7.666459083557129, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 20:10:20 client2-1  | {'loss': 0.283, 'grad_norm': 7.020114421844482, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 20:10:33 client2-1  | {'loss': 0.2848, 'grad_norm': 8.719321250915527, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 20:04:15 client1-1  | INFO :      Sent reply
2025-05-13 20:04:26 client1-1  | INFO :      
2025-05-13 20:04:26 client1-1  | INFO :      Received: train message 4467f5e6-7a36-47cd-887c-06f02dda90a3
2025-05-13 20:10:43 client2-1  | {'loss': 0.4214, 'grad_norm': 11.278512001037598, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 20:04:39 client1-1  | {'loss': 0.0673, 'grad_norm': 1.1125500202178955, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 20:04:49 client1-1  | {'loss': 0.078, 'grad_norm': 2.6696879863739014, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 20:04:58 client1-1  | {'loss': 0.0904, 'grad_norm': 5.087760925292969, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 20:05:08 client1-1  | {'loss': 0.0917, 'grad_norm': 3.7466297149658203, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 20:05:21 client1-1  | {'loss': 0.0968, 'grad_norm': 4.202372074127197, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 20:05:31 client1-1  | {'loss': 0.0858, 'grad_norm': 3.4955356121063232, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 20:05:40 client1-1  | {'loss': 0.1227, 'grad_norm': 4.1928815841674805, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 20:05:50 client1-1  | {'loss': 0.083, 'grad_norm': 2.719933271408081, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 20:10:52 client2-1  | {'loss': 0.3416, 'grad_norm': 6.717021942138672, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 20:06:00 client1-1  | {'loss': 0.1222, 'grad_norm': 3.780172348022461, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 20:06:09 client1-1  | {'loss': 0.0905, 'grad_norm': 4.414221286773682, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 20:06:22 client1-1  | {'loss': 0.1231, 'grad_norm': 5.665816307067871, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 20:11:02 client2-1  | {'loss': 0.4195, 'grad_norm': 7.001326084136963, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 20:06:32 client1-1  | {'loss': 0.1131, 'grad_norm': 6.012859344482422, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 20:11:12 client2-1  | {'loss': 0.4774, 'grad_norm': 13.554276466369629, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 20:06:41 client1-1  | {'loss': 0.1233, 'grad_norm': 5.268117427825928, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 20:11:21 client2-1  | {'loss': 0.5613, 'grad_norm': 11.242324829101562, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 20:11:35 client2-1  | {'loss': 0.6121, 'grad_norm': 10.473315238952637, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 20:11:44 client2-1  | {'loss': 0.6539, 'grad_norm': 12.735450744628906, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 20:11:54 client2-1  | {'loss': 0.778, 'grad_norm': 12.511159896850586, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 20:12:03 client2-1  | {'loss': 0.7779, 'grad_norm': 11.637626647949219, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 20:06:51 client1-1  | {'loss': 0.1164, 'grad_norm': 2.261401653289795, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 20:07:00 client1-1  | {'loss': 0.1508, 'grad_norm': 5.225737571716309, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 20:12:13 client2-1  | {'loss': 0.9905, 'grad_norm': 18.048316955566406, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 20:12:22 client2-1  | {'loss': 1.1083, 'grad_norm': 14.19416618347168, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 20:12:35 client2-1  | {'loss': 0.656, 'grad_norm': 7.646926403045654, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 20:07:10 client1-1  | {'loss': 0.0946, 'grad_norm': 2.121854543685913, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 20:07:23 client1-1  | {'loss': 0.0945, 'grad_norm': 1.8198007345199585, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 20:07:33 client1-1  | {'loss': 0.1259, 'grad_norm': 9.373560905456543, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 20:07:42 client1-1  | {'loss': 0.1266, 'grad_norm': 3.8321127891540527, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 20:12:44 client2-1  | {'loss': 0.4183, 'grad_norm': 5.575473785400391, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 20:12:54 client2-1  | {'loss': 0.0978, 'grad_norm': 2.5830249786376953, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 20:07:52 client1-1  | {'loss': 0.1309, 'grad_norm': 5.328008651733398, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 20:12:54 client2-1  | {'train_runtime': 506.0374, 'train_samples_per_second': 1.976, 'train_steps_per_second': 0.988, 'train_loss': 0.2706756466627121, 'epoch': 1.0}
2025-05-13 20:08:01 client1-1  | {'loss': 0.1185, 'grad_norm': 2.258843421936035, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 20:08:11 client1-1  | {'loss': 0.132, 'grad_norm': 7.88337516784668, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 20:12:59 client2-1  | INFO :      Sent reply
2025-05-13 20:13:12 client2-1  | INFO :      
2025-05-13 20:13:12 client2-1  | INFO :      Received: train message 464b69cb-664d-411f-ad78-568e9ec40e41
2025-05-13 20:13:22 client2-1  | {'loss': 0.06, 'grad_norm': 1.6106082201004028, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 20:13:32 client2-1  | {'loss': 0.0702, 'grad_norm': 2.9081759452819824, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 20:13:46 client2-1  | {'loss': 0.0922, 'grad_norm': 5.626384258270264, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 20:08:20 client1-1  | {'loss': 0.1467, 'grad_norm': 7.01587438583374, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 20:08:34 client1-1  | {'loss': 0.1155, 'grad_norm': 3.935760498046875, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 20:08:43 client1-1  | {'loss': 0.1242, 'grad_norm': 3.451016664505005, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 20:08:53 client1-1  | {'loss': 0.1572, 'grad_norm': 8.883844375610352, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 20:09:02 client1-1  | {'loss': 0.1507, 'grad_norm': 4.611855983734131, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 20:09:12 client1-1  | {'loss': 0.1813, 'grad_norm': 6.658356666564941, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 20:09:21 client1-1  | {'loss': 0.1704, 'grad_norm': 5.273966312408447, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 20:09:35 client1-1  | {'loss': 0.1724, 'grad_norm': 5.860124111175537, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 20:13:55 client2-1  | {'loss': 0.0892, 'grad_norm': 3.652339220046997, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 20:14:05 client2-1  | {'loss': 0.0982, 'grad_norm': 2.8592793941497803, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 20:09:44 client1-1  | {'loss': 0.2154, 'grad_norm': 6.65241813659668, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 20:09:54 client1-1  | {'loss': 0.2169, 'grad_norm': 6.220063209533691, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 20:10:03 client1-1  | {'loss': 0.1606, 'grad_norm': 5.079936504364014, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 20:10:13 client1-1  | {'loss': 0.2403, 'grad_norm': 9.117232322692871, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 20:10:22 client1-1  | {'loss': 0.2745, 'grad_norm': 10.735837936401367, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 20:14:15 client2-1  | {'loss': 0.098, 'grad_norm': 4.431227207183838, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 20:10:36 client1-1  | {'loss': 0.2957, 'grad_norm': 8.664767265319824, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 20:14:24 client2-1  | {'loss': 0.0891, 'grad_norm': 1.6691945791244507, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 20:10:45 client1-1  | {'loss': 0.3677, 'grad_norm': 10.274575233459473, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 20:10:55 client1-1  | {'loss': 0.3709, 'grad_norm': 14.671448707580566, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 20:14:34 client2-1  | {'loss': 0.0938, 'grad_norm': 3.0884227752685547, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 20:14:47 client2-1  | {'loss': 0.093, 'grad_norm': 4.900214672088623, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 20:14:57 client2-1  | {'loss': 0.0931, 'grad_norm': 5.006024360656738, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 20:15:06 client2-1  | {'loss': 0.0937, 'grad_norm': 2.8566555976867676, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 20:15:16 client2-1  | {'loss': 0.1083, 'grad_norm': 3.6624596118927, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 20:15:25 client2-1  | {'loss': 0.1035, 'grad_norm': 5.913211822509766, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 20:11:04 client1-1  | {'loss': 0.4616, 'grad_norm': 14.937350273132324, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 20:11:14 client1-1  | {'loss': 0.4191, 'grad_norm': 12.875055313110352, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 20:15:35 client2-1  | {'loss': 0.1191, 'grad_norm': 4.003990650177002, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 20:11:23 client1-1  | {'loss': 0.5718, 'grad_norm': 14.759027481079102, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 20:15:48 client2-1  | {'loss': 0.1185, 'grad_norm': 2.810720682144165, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 20:11:37 client1-1  | {'loss': 0.6834, 'grad_norm': 11.441060066223145, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 20:11:47 client1-1  | {'loss': 0.643, 'grad_norm': 11.665942192077637, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 20:11:56 client1-1  | {'loss': 0.8845, 'grad_norm': 16.306150436401367, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 20:12:05 client1-1  | {'loss': 0.8277, 'grad_norm': 18.751821517944336, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 20:15:58 client2-1  | {'loss': 0.0947, 'grad_norm': 6.758512020111084, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 20:16:08 client2-1  | {'loss': 0.1125, 'grad_norm': 4.985330104827881, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 20:16:17 client2-1  | {'loss': 0.1247, 'grad_norm': 5.569271087646484, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 20:12:15 client1-1  | {'loss': 1.0058, 'grad_norm': 14.588048934936523, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 20:16:27 client2-1  | {'loss': 0.1213, 'grad_norm': 4.684091567993164, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 20:16:36 client2-1  | {'loss': 0.1174, 'grad_norm': 4.602004051208496, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 20:16:49 client2-1  | {'loss': 0.1383, 'grad_norm': 4.014676094055176, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 20:12:24 client1-1  | {'loss': 0.9732, 'grad_norm': 16.67790412902832, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 20:12:34 client1-1  | {'loss': 0.8577, 'grad_norm': 8.911653518676758, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 20:12:47 client1-1  | {'loss': 0.3901, 'grad_norm': 6.816762924194336, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 20:12:56 client1-1  | {'loss': 0.1217, 'grad_norm': 4.738945484161377, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 20:12:56 client1-1  | {'train_runtime': 508.6782, 'train_samples_per_second': 1.966, 'train_steps_per_second': 0.983, 'train_loss': 0.27156127977371214, 'epoch': 1.0}
2025-05-13 20:13:00 client1-1  | INFO :      Sent reply
2025-05-13 20:16:59 client2-1  | {'loss': 0.1291, 'grad_norm': 7.6996235847473145, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 20:17:08 client2-1  | {'loss': 0.129, 'grad_norm': 6.2684645652771, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 20:17:18 client2-1  | {'loss': 0.138, 'grad_norm': 4.205964088439941, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 20:13:11 client1-1  | INFO :      
2025-05-13 20:13:11 client1-1  | INFO :      Received: train message 305c6c64-a04f-4c8c-a384-82f854e1b311
2025-05-13 20:17:28 client2-1  | {'loss': 0.1499, 'grad_norm': 4.33540153503418, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 20:17:41 client2-1  | {'loss': 0.1196, 'grad_norm': 6.918641090393066, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 20:17:51 client2-1  | {'loss': 0.1451, 'grad_norm': 7.485189914703369, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 20:18:00 client2-1  | {'loss': 0.1638, 'grad_norm': 5.606402397155762, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 20:18:10 client2-1  | {'loss': 0.1677, 'grad_norm': 5.7769694328308105, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 20:13:24 client1-1  | {'loss': 0.0729, 'grad_norm': 1.971985101699829, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 20:18:19 client2-1  | {'loss': 0.1596, 'grad_norm': 8.791635513305664, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 20:18:33 client2-1  | {'loss': 0.1695, 'grad_norm': 7.336282730102539, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 20:13:34 client1-1  | {'loss': 0.0696, 'grad_norm': 3.0682408809661865, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 20:18:42 client2-1  | {'loss': 0.167, 'grad_norm': 8.55713939666748, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 20:18:52 client2-1  | {'loss': 0.1834, 'grad_norm': 4.581449508666992, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 20:19:01 client2-1  | {'loss': 0.2254, 'grad_norm': 8.778274536132812, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 20:19:11 client2-1  | {'loss': 0.2598, 'grad_norm': 5.817267417907715, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 20:13:44 client1-1  | {'loss': 0.0795, 'grad_norm': 2.9897053241729736, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 20:13:53 client1-1  | {'loss': 0.0832, 'grad_norm': 4.679872989654541, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 20:14:07 client1-1  | {'loss': 0.0918, 'grad_norm': 4.5988264083862305, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 20:14:17 client1-1  | {'loss': 0.0899, 'grad_norm': 3.71771240234375, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 20:19:24 client2-1  | {'loss': 0.254, 'grad_norm': 13.293240547180176, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 20:19:34 client2-1  | {'loss': 0.3865, 'grad_norm': 11.984777450561523, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 20:14:26 client1-1  | {'loss': 0.1098, 'grad_norm': 3.3413634300231934, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 20:14:36 client1-1  | {'loss': 0.0845, 'grad_norm': 1.68888258934021, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 20:14:45 client1-1  | {'loss': 0.1021, 'grad_norm': 7.581782341003418, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 20:19:43 client2-1  | {'loss': 0.2868, 'grad_norm': 6.811761379241943, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 20:19:53 client2-1  | {'loss': 0.3908, 'grad_norm': 7.426513671875, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 20:14:55 client1-1  | {'loss': 0.0978, 'grad_norm': 5.654287338256836, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 20:15:08 client1-1  | {'loss': 0.1156, 'grad_norm': 3.8597538471221924, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 20:15:18 client1-1  | {'loss': 0.1037, 'grad_norm': 7.54461145401001, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 20:20:02 client2-1  | {'loss': 0.442, 'grad_norm': 14.007797241210938, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 20:15:27 client1-1  | {'loss': 0.1087, 'grad_norm': 6.355137825012207, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 20:20:12 client2-1  | {'loss': 0.5423, 'grad_norm': 11.899362564086914, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 20:20:21 client2-1  | {'loss': 0.5561, 'grad_norm': 10.21537971496582, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 20:20:34 client2-1  | {'loss': 0.6158, 'grad_norm': 13.255910873413086, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 20:20:44 client2-1  | {'loss': 0.7387, 'grad_norm': 11.667434692382812, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 20:20:53 client2-1  | {'loss': 0.75, 'grad_norm': 12.537145614624023, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 20:15:37 client1-1  | {'loss': 0.1228, 'grad_norm': 2.655902147293091, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 20:21:03 client2-1  | {'loss': 0.9839, 'grad_norm': 18.563369750976562, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 20:15:46 client1-1  | {'loss': 0.1563, 'grad_norm': 5.46476936340332, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 20:21:12 client2-1  | {'loss': 1.0816, 'grad_norm': 12.882637977600098, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 20:21:22 client2-1  | {'loss': 0.6021, 'grad_norm': 6.883753299713135, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 20:21:32 client2-1  | {'loss': 0.3864, 'grad_norm': 6.225306510925293, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 20:15:56 client1-1  | {'loss': 0.1014, 'grad_norm': 5.8331170082092285, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 20:21:45 client2-1  | {'loss': 0.1092, 'grad_norm': 2.886435031890869, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 20:21:45 client2-1  | {'train_runtime': 512.2219, 'train_samples_per_second': 1.952, 'train_steps_per_second': 0.976, 'train_loss': 0.2512364830970764, 'epoch': 1.0}
2025-05-13 20:21:51 client2-1  | INFO :      Sent reply
2025-05-13 20:22:02 client2-1  | INFO :      
2025-05-13 20:16:10 client1-1  | {'loss': 0.0868, 'grad_norm': 1.9587032794952393, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 20:22:02 client2-1  | INFO :      Received: train message 417fcc9f-d513-4581-ac0d-db15a5c782f9
2025-05-13 20:22:13 client2-1  | {'loss': 0.061, 'grad_norm': 1.659956693649292, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 20:22:22 client2-1  | {'loss': 0.0653, 'grad_norm': 1.8769195079803467, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 20:22:36 client2-1  | {'loss': 0.0787, 'grad_norm': 4.9831743240356445, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 20:22:45 client2-1  | {'loss': 0.0829, 'grad_norm': 2.8711740970611572, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 20:22:55 client2-1  | {'loss': 0.0804, 'grad_norm': 3.494342565536499, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 20:23:04 client2-1  | {'loss': 0.087, 'grad_norm': 2.766322374343872, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 20:23:14 client2-1  | {'loss': 0.092, 'grad_norm': 2.2122862339019775, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 20:23:23 client2-1  | {'loss': 0.0947, 'grad_norm': 2.7399771213531494, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 20:23:33 client2-1  | {'loss': 0.0741, 'grad_norm': 2.647474527359009, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 20:23:46 client2-1  | {'loss': 0.1007, 'grad_norm': 4.582066535949707, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 20:23:55 client2-1  | {'loss': 0.0934, 'grad_norm': 2.0396173000335693, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 20:24:05 client2-1  | {'loss': 0.1084, 'grad_norm': 4.172574520111084, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 20:16:19 client1-1  | {'loss': 0.1095, 'grad_norm': 6.082623481750488, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 20:16:29 client1-1  | {'loss': 0.1075, 'grad_norm': 4.186582565307617, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 20:16:38 client1-1  | {'loss': 0.1188, 'grad_norm': 3.4814577102661133, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 20:16:48 client1-1  | {'loss': 0.1179, 'grad_norm': 4.92622709274292, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 20:24:15 client2-1  | {'loss': 0.101, 'grad_norm': 3.6731908321380615, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 20:24:24 client2-1  | {'loss': 0.1006, 'grad_norm': 3.362993001937866, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 20:16:57 client1-1  | {'loss': 0.1328, 'grad_norm': 9.67801570892334, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 20:24:34 client2-1  | {'loss': 0.1128, 'grad_norm': 3.5038695335388184, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 20:17:10 client1-1  | {'loss': 0.1455, 'grad_norm': 9.313140869140625, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 20:24:47 client2-1  | {'loss': 0.091, 'grad_norm': 1.7500368356704712, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 20:24:56 client2-1  | {'loss': 0.1012, 'grad_norm': 4.6698408126831055, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 20:25:06 client2-1  | {'loss': 0.1143, 'grad_norm': 5.114837169647217, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 20:25:15 client2-1  | {'loss': 0.1156, 'grad_norm': 6.2917094230651855, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 20:25:25 client2-1  | {'loss': 0.1076, 'grad_norm': 4.906355857849121, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 20:17:20 client1-1  | {'loss': 0.1173, 'grad_norm': 3.106940507888794, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 20:17:29 client1-1  | {'loss': 0.1284, 'grad_norm': 3.7648608684539795, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 20:17:39 client1-1  | {'loss': 0.1429, 'grad_norm': 5.621986389160156, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 20:25:34 client2-1  | {'loss': 0.1292, 'grad_norm': 5.172613143920898, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 20:25:47 client2-1  | {'loss': 0.1349, 'grad_norm': 7.719181537628174, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 20:25:56 client2-1  | {'loss': 0.1167, 'grad_norm': 8.541614532470703, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 20:26:06 client2-1  | {'loss': 0.1289, 'grad_norm': 3.4739315509796143, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 20:26:15 client2-1  | {'loss': 0.1303, 'grad_norm': 5.1495041847229, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 20:26:25 client2-1  | {'loss': 0.1205, 'grad_norm': 6.413809299468994, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 20:17:49 client1-1  | {'loss': 0.1341, 'grad_norm': 5.377615451812744, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 20:17:58 client1-1  | {'loss': 0.149, 'grad_norm': 5.216859340667725, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 20:26:35 client2-1  | {'loss': 0.138, 'grad_norm': 8.462967872619629, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 20:26:48 client2-1  | {'loss': 0.1464, 'grad_norm': 3.923914909362793, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 20:26:57 client2-1  | {'loss': 0.1301, 'grad_norm': 6.226641654968262, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 20:27:07 client2-1  | {'loss': 0.1502, 'grad_norm': 4.805834770202637, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 20:18:08 client1-1  | {'loss': 0.1617, 'grad_norm': 5.9912285804748535, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 20:18:21 client1-1  | {'loss': 0.1609, 'grad_norm': 8.556507110595703, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 20:18:30 client1-1  | {'loss': 0.1982, 'grad_norm': 4.690884113311768, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 20:18:40 client1-1  | {'loss': 0.1928, 'grad_norm': 5.193600177764893, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 20:18:49 client1-1  | {'loss': 0.1459, 'grad_norm': 2.8376288414001465, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 20:18:59 client1-1  | {'loss': 0.2008, 'grad_norm': 5.50349235534668, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 20:19:09 client1-1  | {'loss': 0.2259, 'grad_norm': 9.669222831726074, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 20:27:16 client2-1  | {'loss': 0.1521, 'grad_norm': 6.637563705444336, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 20:27:26 client2-1  | {'loss': 0.1483, 'grad_norm': 8.313586235046387, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 20:27:35 client2-1  | {'loss': 0.1634, 'grad_norm': 5.374578475952148, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 20:27:45 client2-1  | {'loss': 0.2065, 'grad_norm': 9.21217155456543, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 20:19:22 client1-1  | {'loss': 0.2514, 'grad_norm': 7.264404296875, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 20:27:58 client2-1  | {'loss': 0.2185, 'grad_norm': 5.577975749969482, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 20:19:31 client1-1  | {'loss': 0.3117, 'grad_norm': 10.151960372924805, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 20:28:07 client2-1  | {'loss': 0.2306, 'grad_norm': 10.020630836486816, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 20:28:17 client2-1  | {'loss': 0.3336, 'grad_norm': 11.639532089233398, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 20:28:26 client2-1  | {'loss': 0.2697, 'grad_norm': 7.334653854370117, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 20:28:36 client2-1  | {'loss': 0.3447, 'grad_norm': 7.219764232635498, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 20:28:45 client2-1  | {'loss': 0.3868, 'grad_norm': 13.62798023223877, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 20:19:41 client1-1  | {'loss': 0.3381, 'grad_norm': 13.784664154052734, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 20:19:51 client1-1  | {'loss': 0.412, 'grad_norm': 13.49710464477539, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 20:20:00 client1-1  | {'loss': 0.3732, 'grad_norm': 11.899248123168945, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 20:20:10 client1-1  | {'loss': 0.5293, 'grad_norm': 11.311561584472656, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 20:20:23 client1-1  | {'loss': 0.6364, 'grad_norm': 13.798910140991211, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 20:20:32 client1-1  | {'loss': 0.6031, 'grad_norm': 12.507718086242676, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 20:28:55 client2-1  | {'loss': 0.4858, 'grad_norm': 11.750642776489258, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 20:29:08 client2-1  | {'loss': 0.5213, 'grad_norm': 9.800848960876465, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 20:20:42 client1-1  | {'loss': 0.8519, 'grad_norm': 18.294301986694336, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 20:29:18 client2-1  | {'loss': 0.5702, 'grad_norm': 14.92601203918457, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 20:20:51 client1-1  | {'loss': 0.7957, 'grad_norm': 18.97648048400879, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 20:29:27 client2-1  | {'loss': 0.6661, 'grad_norm': 12.264917373657227, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 20:29:37 client2-1  | {'loss': 0.7169, 'grad_norm': 13.365251541137695, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 20:29:46 client2-1  | {'loss': 0.9536, 'grad_norm': 18.557437896728516, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 20:29:56 client2-1  | {'loss': 1.0567, 'grad_norm': 15.075082778930664, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 20:30:05 client2-1  | {'loss': 0.5779, 'grad_norm': 7.29818868637085, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 20:30:15 client2-1  | {'loss': 0.3396, 'grad_norm': 5.803190231323242, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 20:30:28 client2-1  | {'loss': 0.091, 'grad_norm': 1.4799915552139282, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 20:30:28 client2-1  | {'train_runtime': 504.5953, 'train_samples_per_second': 1.982, 'train_steps_per_second': 0.991, 'train_loss': 0.23242369973659516, 'epoch': 1.0}
2025-05-13 20:21:05 client1-1  | {'loss': 1.0029, 'grad_norm': 20.993440628051758, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 20:21:14 client1-1  | {'loss': 0.9701, 'grad_norm': 17.996078491210938, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 20:21:24 client1-1  | {'loss': 0.8079, 'grad_norm': 9.635100364685059, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 20:21:34 client1-1  | {'loss': 0.3477, 'grad_norm': 6.2532172203063965, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 20:21:43 client1-1  | {'loss': 0.0982, 'grad_norm': 3.3924710750579834, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 20:21:43 client1-1  | {'train_runtime': 511.6102, 'train_samples_per_second': 1.955, 'train_steps_per_second': 0.977, 'train_loss': 0.25391767275333404, 'epoch': 1.0}
2025-05-13 20:21:52 client1-1  | INFO :      Sent reply
2025-05-13 20:30:32 client2-1  | INFO :      Sent reply
2025-05-13 20:30:47 client2-1  | INFO :      
2025-05-13 20:30:47 client2-1  | INFO :      Received: train message 70bfff37-0844-46bb-b286-b4545d1cf13b
2025-05-13 20:30:58 client2-1  | {'loss': 0.0603, 'grad_norm': 2.3101184368133545, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 20:22:03 client1-1  | INFO :      
2025-05-13 20:22:03 client1-1  | INFO :      Received: train message dcb18d24-9937-42b8-9a19-7c7a73822e55
2025-05-13 20:31:11 client2-1  | {'loss': 0.061, 'grad_norm': 1.410934567451477, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 20:22:15 client1-1  | {'loss': 0.084, 'grad_norm': 1.6487081050872803, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 20:22:29 client1-1  | {'loss': 0.0657, 'grad_norm': 2.8562183380126953, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 20:31:21 client2-1  | {'loss': 0.081, 'grad_norm': 3.677187919616699, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 20:31:31 client2-1  | {'loss': 0.0718, 'grad_norm': 3.390681505203247, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 20:31:40 client2-1  | {'loss': 0.0947, 'grad_norm': 4.161658763885498, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 20:31:50 client2-1  | {'loss': 0.0895, 'grad_norm': 3.1408233642578125, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 20:31:59 client2-1  | {'loss': 0.0806, 'grad_norm': 2.8841302394866943, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 20:32:09 client2-1  | {'loss': 0.0826, 'grad_norm': 4.179038047790527, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 20:22:38 client1-1  | {'loss': 0.0756, 'grad_norm': 4.144062519073486, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 20:22:48 client1-1  | {'loss': 0.0879, 'grad_norm': 3.4247138500213623, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 20:32:22 client2-1  | {'loss': 0.0763, 'grad_norm': 2.783071517944336, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 20:32:32 client2-1  | {'loss': 0.0798, 'grad_norm': 5.013922691345215, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 20:32:41 client2-1  | {'loss': 0.0855, 'grad_norm': 3.6545281410217285, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 20:22:58 client1-1  | {'loss': 0.091, 'grad_norm': 4.295949935913086, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 20:23:11 client1-1  | {'loss': 0.0798, 'grad_norm': 4.800788402557373, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 20:32:51 client2-1  | {'loss': 0.1026, 'grad_norm': 7.349813938140869, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 20:33:00 client2-1  | {'loss': 0.0934, 'grad_norm': 3.4774043560028076, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 20:33:10 client2-1  | {'loss': 0.1132, 'grad_norm': 2.513094663619995, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 20:33:19 client2-1  | {'loss': 0.1113, 'grad_norm': 5.885168075561523, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 20:33:32 client2-1  | {'loss': 0.0836, 'grad_norm': 2.685495138168335, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 20:33:42 client2-1  | {'loss': 0.112, 'grad_norm': 3.98616361618042, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 20:33:51 client2-1  | {'loss': 0.1072, 'grad_norm': 4.119380950927734, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 20:34:01 client2-1  | {'loss': 0.1204, 'grad_norm': 4.710864067077637, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 20:34:10 client2-1  | {'loss': 0.1091, 'grad_norm': 3.518164873123169, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 20:34:20 client2-1  | {'loss': 0.1109, 'grad_norm': 5.4713921546936035, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 20:23:20 client1-1  | {'loss': 0.0986, 'grad_norm': 4.403717994689941, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 20:23:30 client1-1  | {'loss': 0.0877, 'grad_norm': 1.5988632440567017, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 20:23:39 client1-1  | {'loss': 0.1162, 'grad_norm': 3.783533811569214, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 20:23:49 client1-1  | {'loss': 0.0972, 'grad_norm': 4.369753360748291, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 20:23:58 client1-1  | {'loss': 0.112, 'grad_norm': 3.640622615814209, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 20:34:29 client2-1  | {'loss': 0.1308, 'grad_norm': 9.962213516235352, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 20:34:42 client2-1  | {'loss': 0.1193, 'grad_norm': 5.476573944091797, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 20:34:52 client2-1  | {'loss': 0.1198, 'grad_norm': 3.2066216468811035, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 20:24:12 client1-1  | {'loss': 0.0979, 'grad_norm': 5.91849422454834, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 20:24:21 client1-1  | {'loss': 0.0957, 'grad_norm': 4.435359001159668, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 20:24:31 client1-1  | {'loss': 0.1072, 'grad_norm': 2.7113122940063477, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 20:24:40 client1-1  | {'loss': 0.135, 'grad_norm': 4.371364116668701, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 20:24:50 client1-1  | {'loss': 0.0982, 'grad_norm': 3.1072874069213867, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 20:35:02 client2-1  | {'loss': 0.1135, 'grad_norm': 3.2575862407684326, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 20:25:03 client1-1  | {'loss': 0.0926, 'grad_norm': 3.497676134109497, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 20:35:11 client2-1  | {'loss': 0.1165, 'grad_norm': 4.785229206085205, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 20:25:13 client1-1  | {'loss': 0.1035, 'grad_norm': 4.126058578491211, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 20:25:22 client1-1  | {'loss': 0.1047, 'grad_norm': 4.920253276824951, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 20:25:32 client1-1  | {'loss': 0.126, 'grad_norm': 5.514129638671875, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 20:35:21 client2-1  | {'loss': 0.1395, 'grad_norm': 8.42467975616455, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 20:25:41 client1-1  | {'loss': 0.1028, 'grad_norm': 3.947932720184326, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 20:35:30 client2-1  | {'loss': 0.1426, 'grad_norm': 5.452300548553467, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 20:35:43 client2-1  | {'loss': 0.1169, 'grad_norm': 4.660799980163574, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 20:35:53 client2-1  | {'loss': 0.1427, 'grad_norm': 5.07656192779541, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 20:36:02 client2-1  | {'loss': 0.1359, 'grad_norm': 6.400074005126953, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 20:25:54 client1-1  | {'loss': 0.1063, 'grad_norm': 6.4058918952941895, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 20:26:03 client1-1  | {'loss': 0.1306, 'grad_norm': 5.716437816619873, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 20:36:12 client2-1  | {'loss': 0.1453, 'grad_norm': 7.580229759216309, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 20:36:21 client2-1  | {'loss': 0.1522, 'grad_norm': 3.180136203765869, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 20:36:31 client2-1  | {'loss': 0.2007, 'grad_norm': 6.672465801239014, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 20:26:13 client1-1  | {'loss': 0.1037, 'grad_norm': 3.127469539642334, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 20:26:23 client1-1  | {'loss': 0.1277, 'grad_norm': 4.537451267242432, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 20:26:32 client1-1  | {'loss': 0.1265, 'grad_norm': 6.203249931335449, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 20:36:40 client2-1  | {'loss': 0.1934, 'grad_norm': 5.533212661743164, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 20:36:53 client2-1  | {'loss': 0.1952, 'grad_norm': 8.873067855834961, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 20:37:03 client2-1  | {'loss': 0.3083, 'grad_norm': 10.58961009979248, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 20:37:12 client2-1  | {'loss': 0.2327, 'grad_norm': 8.518187522888184, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 20:37:22 client2-1  | {'loss': 0.3161, 'grad_norm': 6.562309741973877, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 20:26:46 client1-1  | {'loss': 0.1237, 'grad_norm': 3.6037657260894775, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 20:26:55 client1-1  | {'loss': 0.1433, 'grad_norm': 4.2390594482421875, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 20:27:05 client1-1  | {'loss': 0.1347, 'grad_norm': 6.426894187927246, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 20:37:31 client2-1  | {'loss': 0.3479, 'grad_norm': 12.905021667480469, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 20:27:14 client1-1  | {'loss': 0.1505, 'grad_norm': 7.420894622802734, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 20:37:41 client2-1  | {'loss': 0.4376, 'grad_norm': 11.843892097473145, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 20:37:50 client2-1  | {'loss': 0.4924, 'grad_norm': 10.268260955810547, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 20:38:03 client2-1  | {'loss': 0.5332, 'grad_norm': 14.005953788757324, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 20:27:24 client1-1  | {'loss': 0.1625, 'grad_norm': 3.271440029144287, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 20:27:37 client1-1  | {'loss': 0.1652, 'grad_norm': 4.570775508880615, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 20:38:13 client2-1  | {'loss': 0.6601, 'grad_norm': 13.524334907531738, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 20:38:23 client2-1  | {'loss': 0.674, 'grad_norm': 12.996091842651367, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 20:38:32 client2-1  | {'loss': 0.9261, 'grad_norm': 18.775165557861328, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 20:27:47 client1-1  | {'loss': 0.1452, 'grad_norm': 3.935594081878662, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 20:27:56 client1-1  | {'loss': 0.1912, 'grad_norm': 6.811161994934082, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 20:38:42 client2-1  | {'loss': 1.0489, 'grad_norm': 13.893454551696777, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 20:38:51 client2-1  | {'loss': 0.5674, 'grad_norm': 8.602665901184082, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 20:39:01 client2-1  | {'loss': 0.3089, 'grad_norm': 4.836667537689209, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 20:28:06 client1-1  | {'loss': 0.2157, 'grad_norm': 9.479877471923828, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 20:39:14 client2-1  | {'loss': 0.0742, 'grad_norm': 2.1984283924102783, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 20:39:14 client2-1  | {'train_runtime': 504.9131, 'train_samples_per_second': 1.981, 'train_steps_per_second': 0.99, 'train_loss': 0.22037299644947053, 'epoch': 1.0}
2025-05-13 20:39:20 client2-1  | INFO :      Sent reply
2025-05-13 20:39:32 client2-1  | INFO :      
2025-05-13 20:39:32 client2-1  | INFO :      Received: train message baf00d5f-de2e-485a-9496-4564d93fb99c
2025-05-13 20:39:43 client2-1  | {'loss': 0.0623, 'grad_norm': 2.167509078979492, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 20:39:56 client2-1  | {'loss': 0.061, 'grad_norm': 1.3585973978042603, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 20:40:06 client2-1  | {'loss': 0.0648, 'grad_norm': 3.278761625289917, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 20:40:15 client2-1  | {'loss': 0.0713, 'grad_norm': 3.3208229541778564, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 20:28:15 client1-1  | {'loss': 0.23, 'grad_norm': 6.110065460205078, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 20:40:25 client2-1  | {'loss': 0.0775, 'grad_norm': 3.302778959274292, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 20:40:34 client2-1  | {'loss': 0.078, 'grad_norm': 2.3059258460998535, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 20:28:28 client1-1  | {'loss': 0.2945, 'grad_norm': 11.834712982177734, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 20:40:44 client2-1  | {'loss': 0.073, 'grad_norm': 2.678560972213745, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 20:28:38 client1-1  | {'loss': 0.305, 'grad_norm': 13.703189849853516, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 20:40:57 client2-1  | {'loss': 0.0831, 'grad_norm': 4.2666707038879395, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 20:41:07 client2-1  | {'loss': 0.0642, 'grad_norm': 2.305793523788452, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 20:41:17 client2-1  | {'loss': 0.0861, 'grad_norm': 3.2831099033355713, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 20:41:26 client2-1  | {'loss': 0.0895, 'grad_norm': 3.4352633953094482, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 20:41:36 client2-1  | {'loss': 0.0938, 'grad_norm': 3.704446315765381, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 20:41:45 client2-1  | {'loss': 0.0931, 'grad_norm': 4.076883792877197, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 20:41:54 client2-1  | {'loss': 0.0975, 'grad_norm': 2.264763593673706, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 20:42:08 client2-1  | {'loss': 0.1114, 'grad_norm': 4.644765853881836, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 20:42:17 client2-1  | {'loss': 0.0798, 'grad_norm': 2.971925973892212, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 20:28:47 client1-1  | {'loss': 0.3876, 'grad_norm': 14.275084495544434, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 20:42:26 client2-1  | {'loss': 0.0992, 'grad_norm': 4.017569065093994, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 20:28:57 client1-1  | {'loss': 0.3692, 'grad_norm': 12.648072242736816, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 20:29:11 client1-1  | {'loss': 0.4803, 'grad_norm': 11.68088150024414, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 20:42:36 client2-1  | {'loss': 0.1114, 'grad_norm': 4.955331325531006, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 20:29:20 client1-1  | {'loss': 0.5988, 'grad_norm': 14.600224494934082, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 20:29:30 client1-1  | {'loss': 0.5753, 'grad_norm': 12.127839088439941, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 20:29:39 client1-1  | {'loss': 0.7991, 'grad_norm': 21.0467472076416, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 20:29:49 client1-1  | {'loss': 0.7738, 'grad_norm': 18.137468338012695, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 20:30:02 client1-1  | {'loss': 0.9484, 'grad_norm': 13.703960418701172, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 20:30:12 client1-1  | {'loss': 0.9434, 'grad_norm': 17.662660598754883, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 20:30:21 client1-1  | {'loss': 0.7703, 'grad_norm': 9.74079418182373, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 20:42:45 client2-1  | {'loss': 0.1133, 'grad_norm': 6.006981372833252, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 20:30:31 client1-1  | {'loss': 0.3126, 'grad_norm': 5.998746871948242, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 20:42:55 client2-1  | {'loss': 0.0959, 'grad_norm': 3.6103522777557373, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 20:30:33 client1-1  | {'loss': 0.0985, 'grad_norm': 4.6669206619262695, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 20:43:08 client2-1  | {'loss': 0.1262, 'grad_norm': 7.2344560623168945, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 20:43:17 client2-1  | {'loss': 0.1124, 'grad_norm': 7.256618976593018, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 20:43:27 client2-1  | {'loss': 0.1022, 'grad_norm': 6.336052417755127, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 20:43:36 client2-1  | {'loss': 0.1278, 'grad_norm': 2.8897924423217773, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 20:43:46 client2-1  | {'loss': 0.1112, 'grad_norm': 3.150622844696045, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 20:30:33 client1-1  | {'train_runtime': 507.9397, 'train_samples_per_second': 1.969, 'train_steps_per_second': 0.984, 'train_loss': 0.23946636748313904, 'epoch': 1.0}
2025-05-13 20:30:36 client1-1  | INFO :      Sent reply
2025-05-13 20:30:47 client1-1  | INFO :      
2025-05-13 20:30:47 client1-1  | INFO :      Received: train message 278fe1dc-582d-4371-855b-2436bd45e576
2025-05-13 20:43:55 client2-1  | {'loss': 0.1112, 'grad_norm': 4.564030647277832, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 20:31:00 client1-1  | {'loss': 0.0735, 'grad_norm': 1.24622642993927, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 20:44:10 client2-1  | {'loss': 0.1251, 'grad_norm': 7.530580997467041, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 20:31:10 client1-1  | {'loss': 0.0674, 'grad_norm': 2.113117218017578, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 20:31:19 client1-1  | {'loss': 0.0819, 'grad_norm': 2.895097255706787, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 20:31:29 client1-1  | {'loss': 0.0886, 'grad_norm': 2.7931623458862305, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 20:44:19 client2-1  | {'loss': 0.1271, 'grad_norm': 5.142092227935791, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 20:44:29 client2-1  | {'loss': 0.1192, 'grad_norm': 6.619725704193115, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 20:44:42 client2-1  | {'loss': 0.1311, 'grad_norm': 6.064075946807861, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 20:44:51 client2-1  | {'loss': 0.1297, 'grad_norm': 7.264559745788574, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 20:31:42 client1-1  | {'loss': 0.0771, 'grad_norm': 1.5916991233825684, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 20:31:52 client1-1  | {'loss': 0.0769, 'grad_norm': 5.386348724365234, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 20:45:01 client2-1  | {'loss': 0.1305, 'grad_norm': 6.704163074493408, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 20:45:10 client2-1  | {'loss': 0.1392, 'grad_norm': 3.6706624031066895, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 20:45:20 client2-1  | {'loss': 0.1643, 'grad_norm': 8.073400497436523, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 20:45:29 client2-1  | {'loss': 0.1723, 'grad_norm': 5.931402206420898, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 20:45:39 client2-1  | {'loss': 0.1744, 'grad_norm': 9.213733673095703, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 20:45:52 client2-1  | {'loss': 0.2674, 'grad_norm': 9.447142601013184, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 20:46:02 client2-1  | {'loss': 0.2142, 'grad_norm': 5.97878885269165, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 20:46:12 client2-1  | {'loss': 0.2793, 'grad_norm': 4.860511779785156, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 20:46:21 client2-1  | {'loss': 0.3144, 'grad_norm': 13.598200798034668, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 20:46:31 client2-1  | {'loss': 0.4129, 'grad_norm': 10.570760726928711, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 20:46:41 client2-1  | {'loss': 0.4507, 'grad_norm': 8.89690113067627, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 20:46:54 client2-1  | {'loss': 0.5156, 'grad_norm': 13.088918685913086, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 20:32:01 client1-1  | {'loss': 0.1125, 'grad_norm': 2.865919828414917, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 20:32:11 client1-1  | {'loss': 0.0808, 'grad_norm': 1.9244338274002075, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 20:32:20 client1-1  | {'loss': 0.1055, 'grad_norm': 2.8394739627838135, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 20:32:30 client1-1  | {'loss': 0.0884, 'grad_norm': 3.235856294631958, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 20:32:39 client1-1  | {'loss': 0.1018, 'grad_norm': 4.378687381744385, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 20:47:03 client2-1  | {'loss': 0.6167, 'grad_norm': 10.419055938720703, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 20:47:12 client2-1  | {'loss': 0.6422, 'grad_norm': 11.176980018615723, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 20:47:22 client2-1  | {'loss': 0.8965, 'grad_norm': 20.153884887695312, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 20:32:53 client1-1  | {'loss': 0.0929, 'grad_norm': 6.0039496421813965, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 20:33:02 client1-1  | {'loss': 0.0921, 'grad_norm': 4.084794044494629, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 20:33:12 client1-1  | {'loss': 0.1061, 'grad_norm': 3.1119837760925293, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 20:33:21 client1-1  | {'loss': 0.14, 'grad_norm': 5.224315166473389, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 20:47:31 client2-1  | {'loss': 0.9789, 'grad_norm': 14.437911033630371, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 20:47:41 client2-1  | {'loss': 0.5357, 'grad_norm': 6.895656108856201, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 20:47:51 client2-1  | {'loss': 0.2906, 'grad_norm': 4.683544635772705, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 20:48:04 client2-1  | {'loss': 0.0825, 'grad_norm': 1.430430293083191, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 20:48:04 client2-1  | {'train_runtime': 510.4662, 'train_samples_per_second': 1.959, 'train_steps_per_second': 0.979, 'train_loss': 0.20615811216831206, 'epoch': 1.0}
2025-05-13 20:48:06 client2-1  | INFO :      Sent reply
2025-05-13 20:48:18 client2-1  | INFO :      
2025-05-13 20:48:18 client2-1  | INFO :      Received: train message acc270e8-a2d6-4a94-bc4e-1d56e6d8bb12
2025-05-13 20:48:31 client2-1  | {'loss': 0.0558, 'grad_norm': 1.5857787132263184, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 20:48:44 client2-1  | {'loss': 0.058, 'grad_norm': 1.4267531633377075, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 20:48:53 client2-1  | {'loss': 0.071, 'grad_norm': 3.1922597885131836, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 20:33:31 client1-1  | {'loss': 0.0977, 'grad_norm': 3.0989575386047363, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 20:33:40 client1-1  | {'loss': 0.0878, 'grad_norm': 4.035436153411865, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 20:33:53 client1-1  | {'loss': 0.1059, 'grad_norm': 4.342589378356934, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 20:49:03 client2-1  | {'loss': 0.0642, 'grad_norm': 2.4620139598846436, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 20:34:03 client1-1  | {'loss': 0.0939, 'grad_norm': 3.6446127891540527, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 20:49:13 client2-1  | {'loss': 0.0809, 'grad_norm': 2.0540120601654053, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 20:49:26 client2-1  | {'loss': 0.0735, 'grad_norm': 2.3234310150146484, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 20:49:35 client2-1  | {'loss': 0.071, 'grad_norm': 2.7949883937835693, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 20:49:45 client2-1  | {'loss': 0.0912, 'grad_norm': 3.7657134532928467, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 20:34:12 client1-1  | {'loss': 0.1089, 'grad_norm': 8.79918384552002, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 20:49:54 client2-1  | {'loss': 0.0669, 'grad_norm': 2.3423328399658203, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 20:50:04 client2-1  | {'loss': 0.077, 'grad_norm': 5.304306983947754, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 20:50:17 client2-1  | {'loss': 0.071, 'grad_norm': 2.082749605178833, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 20:50:27 client2-1  | {'loss': 0.0887, 'grad_norm': 2.4374306201934814, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 20:50:36 client2-1  | {'loss': 0.0814, 'grad_norm': 3.789511203765869, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 20:50:46 client2-1  | {'loss': 0.0953, 'grad_norm': 4.3251142501831055, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 20:50:55 client2-1  | {'loss': 0.0973, 'grad_norm': 2.135897159576416, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 20:51:08 client2-1  | {'loss': 0.0835, 'grad_norm': 3.4901208877563477, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 20:51:18 client2-1  | {'loss': 0.0889, 'grad_norm': 5.331294536590576, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 20:51:27 client2-1  | {'loss': 0.0924, 'grad_norm': 3.964690923690796, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 20:51:37 client2-1  | {'loss': 0.1073, 'grad_norm': 5.732936382293701, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 20:51:46 client2-1  | {'loss': 0.096, 'grad_norm': 4.791781902313232, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 20:52:00 client2-1  | {'loss': 0.1115, 'grad_norm': 7.06376314163208, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 20:52:09 client2-1  | {'loss': 0.1208, 'grad_norm': 6.6243672370910645, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 20:52:19 client2-1  | {'loss': 0.0971, 'grad_norm': 4.714766979217529, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 20:52:28 client2-1  | {'loss': 0.1147, 'grad_norm': 3.6271045207977295, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 20:52:38 client2-1  | {'loss': 0.1164, 'grad_norm': 1.9619966745376587, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 20:34:22 client1-1  | {'loss': 0.1099, 'grad_norm': 5.841894626617432, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 20:34:31 client1-1  | {'loss': 0.114, 'grad_norm': 6.622889995574951, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 20:52:47 client2-1  | {'loss': 0.1097, 'grad_norm': 4.339947700500488, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 20:34:41 client1-1  | {'loss': 0.1134, 'grad_norm': 6.018653392791748, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 20:34:54 client1-1  | {'loss': 0.0868, 'grad_norm': 1.642459511756897, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 20:35:04 client1-1  | {'loss': 0.1134, 'grad_norm': 3.0246689319610596, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 20:35:13 client1-1  | {'loss': 0.1104, 'grad_norm': 4.4865827560424805, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 20:53:00 client2-1  | {'loss': 0.1012, 'grad_norm': 4.391537666320801, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 20:35:23 client1-1  | {'loss': 0.115, 'grad_norm': 4.434921741485596, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 20:53:10 client2-1  | {'loss': 0.1183, 'grad_norm': 4.6862263679504395, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 20:35:32 client1-1  | {'loss': 0.1246, 'grad_norm': 6.4286274909973145, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 20:35:42 client1-1  | {'loss': 0.1418, 'grad_norm': 6.067520618438721, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 20:35:55 client1-1  | {'loss': 0.132, 'grad_norm': 5.606700420379639, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 20:53:19 client2-1  | {'loss': 0.1277, 'grad_norm': 4.5505900382995605, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 20:53:29 client2-1  | {'loss': 0.124, 'grad_norm': 7.480785846710205, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 20:53:38 client2-1  | {'loss': 0.1265, 'grad_norm': 6.535251617431641, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 20:36:04 client1-1  | {'loss': 0.1599, 'grad_norm': 4.26481294631958, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 20:36:14 client1-1  | {'loss': 0.1508, 'grad_norm': 2.2607011795043945, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 20:53:52 client2-1  | {'loss': 0.1268, 'grad_norm': 6.473221302032471, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 20:54:01 client2-1  | {'loss': 0.1247, 'grad_norm': 6.176448822021484, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 20:54:11 client2-1  | {'loss': 0.1525, 'grad_norm': 10.217284202575684, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 20:54:21 client2-1  | {'loss': 0.1676, 'grad_norm': 2.904770612716675, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 20:54:30 client2-1  | {'loss': 0.155, 'grad_norm': 7.727462291717529, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 20:36:23 client1-1  | {'loss': 0.1354, 'grad_norm': 2.7662956714630127, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 20:54:44 client2-1  | {'loss': 0.2562, 'grad_norm': 8.96272087097168, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 20:36:33 client1-1  | {'loss': 0.156, 'grad_norm': 5.6867218017578125, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 20:36:42 client1-1  | {'loss': 0.1915, 'grad_norm': 9.009710311889648, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 20:36:52 client1-1  | {'loss': 0.2198, 'grad_norm': 5.313624382019043, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 20:54:53 client2-1  | {'loss': 0.1942, 'grad_norm': 6.544090747833252, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 20:55:03 client2-1  | {'loss': 0.2458, 'grad_norm': 4.734269618988037, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 20:37:05 client1-1  | {'loss': 0.251, 'grad_norm': 10.206225395202637, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 20:37:14 client1-1  | {'loss': 0.2664, 'grad_norm': 13.18402099609375, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 20:55:12 client2-1  | {'loss': 0.2677, 'grad_norm': 12.075063705444336, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 20:55:22 client2-1  | {'loss': 0.3712, 'grad_norm': 11.126741409301758, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 20:55:35 client2-1  | {'loss': 0.4116, 'grad_norm': 9.575591087341309, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 20:37:24 client1-1  | {'loss': 0.3309, 'grad_norm': 12.06529426574707, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 20:37:33 client1-1  | {'loss': 0.3136, 'grad_norm': 10.598662376403809, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 20:37:43 client1-1  | {'loss': 0.429, 'grad_norm': 11.370400428771973, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 20:37:52 client1-1  | {'loss': 0.5501, 'grad_norm': 13.073790550231934, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 20:38:02 client1-1  | {'loss': 0.5434, 'grad_norm': 12.213059425354004, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 20:38:15 client1-1  | {'loss': 0.7711, 'grad_norm': 17.880403518676758, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 20:38:25 client1-1  | {'loss': 0.7306, 'grad_norm': 19.825300216674805, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 20:38:34 client1-1  | {'loss': 0.9309, 'grad_norm': 13.022775650024414, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 20:38:44 client1-1  | {'loss': 0.9019, 'grad_norm': 17.589916229248047, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 20:55:45 client2-1  | {'loss': 0.4683, 'grad_norm': 13.216108322143555, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 20:55:54 client2-1  | {'loss': 0.6032, 'grad_norm': 13.699679374694824, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 20:56:04 client2-1  | {'loss': 0.6405, 'grad_norm': 15.846985816955566, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 20:38:53 client1-1  | {'loss': 0.7321, 'grad_norm': 10.090806007385254, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 20:39:03 client1-1  | {'loss': 0.2807, 'grad_norm': 3.93863844871521, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 20:39:16 client1-1  | {'loss': 0.0905, 'grad_norm': 2.744765520095825, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 20:39:16 client1-1  | {'train_runtime': 507.7622, 'train_samples_per_second': 1.969, 'train_steps_per_second': 0.985, 'train_loss': 0.22352890586853028, 'epoch': 1.0}
2025-05-13 20:39:20 client1-1  | INFO :      Sent reply
2025-05-13 20:39:31 client1-1  | INFO :      
2025-05-13 20:39:31 client1-1  | INFO :      Received: train message 6aa009d5-c10d-4a40-8b02-68db64a2e099
2025-05-13 20:56:13 client2-1  | {'loss': 0.846, 'grad_norm': 17.7216854095459, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 20:39:45 client1-1  | {'loss': 0.0736, 'grad_norm': 1.5645042657852173, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 20:56:26 client2-1  | {'loss': 0.9483, 'grad_norm': 14.22622013092041, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 20:56:36 client2-1  | {'loss': 0.4947, 'grad_norm': 6.434109687805176, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 20:56:45 client2-1  | {'loss': 0.2614, 'grad_norm': 3.4513511657714844, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 20:39:54 client1-1  | {'loss': 0.0648, 'grad_norm': 2.075536012649536, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 20:40:04 client1-1  | {'loss': 0.0633, 'grad_norm': 3.0237884521484375, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 20:40:13 client1-1  | {'loss': 0.0842, 'grad_norm': 3.4901411533355713, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 20:40:27 client1-1  | {'loss': 0.0687, 'grad_norm': 3.4393012523651123, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 20:40:36 client1-1  | {'loss': 0.0768, 'grad_norm': 4.174067497253418, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 20:40:46 client1-1  | {'loss': 0.088, 'grad_norm': 3.9914493560791016, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 20:40:55 client1-1  | {'loss': 0.0776, 'grad_norm': 1.5378661155700684, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 20:41:05 client1-1  | {'loss': 0.0819, 'grad_norm': 5.097245693206787, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 20:41:15 client1-1  | {'loss': 0.0896, 'grad_norm': 3.279291868209839, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 20:56:51 client2-1  | {'loss': 0.076, 'grad_norm': 2.623145818710327, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 20:56:51 client2-1  | {'train_runtime': 510.8765, 'train_samples_per_second': 1.957, 'train_steps_per_second': 0.979, 'train_loss': 0.1938166047334671, 'epoch': 1.0}
2025-05-13 20:41:24 client1-1  | {'loss': 0.0952, 'grad_norm': 4.34990119934082, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 20:41:37 client1-1  | {'loss': 0.093, 'grad_norm': 4.371474266052246, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 20:56:54 client2-1  | INFO :      Sent reply
2025-05-13 20:57:06 client2-1  | INFO :      
2025-05-13 20:57:06 client2-1  | INFO :      Received: train message fb6f1127-ec52-483e-a4cf-6833d868cd23
2025-05-13 20:57:19 client2-1  | {'loss': 0.0573, 'grad_norm': 1.4890942573547363, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 20:57:28 client2-1  | {'loss': 0.0559, 'grad_norm': 1.114317536354065, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 20:41:47 client1-1  | {'loss': 0.0904, 'grad_norm': 5.652392864227295, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 20:41:56 client1-1  | {'loss': 0.0937, 'grad_norm': 1.5331053733825684, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 20:42:06 client1-1  | {'loss': 0.118, 'grad_norm': 4.512210369110107, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 20:42:15 client1-1  | {'loss': 0.0891, 'grad_norm': 4.59326171875, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 20:42:24 client1-1  | {'loss': 0.0849, 'grad_norm': 1.8488430976867676, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 20:57:38 client2-1  | {'loss': 0.0574, 'grad_norm': 3.300906181335449, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 20:57:47 client2-1  | {'loss': 0.0703, 'grad_norm': 4.391899585723877, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 20:42:37 client1-1  | {'loss': 0.1076, 'grad_norm': 4.863958835601807, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 20:42:47 client1-1  | {'loss': 0.102, 'grad_norm': 4.9688334465026855, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 20:42:56 client1-1  | {'loss': 0.1077, 'grad_norm': 3.3553831577301025, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 20:43:06 client1-1  | {'loss': 0.0893, 'grad_norm': 1.6574960947036743, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 20:43:15 client1-1  | {'loss': 0.111, 'grad_norm': 6.3693928718566895, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 20:43:25 client1-1  | {'loss': 0.1125, 'grad_norm': 4.017033576965332, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 20:58:01 client2-1  | {'loss': 0.0726, 'grad_norm': 3.8803915977478027, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 20:58:10 client2-1  | {'loss': 0.0783, 'grad_norm': 2.61659836769104, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 20:58:20 client2-1  | {'loss': 0.0684, 'grad_norm': 2.8126275539398193, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 20:43:34 client1-1  | {'loss': 0.0861, 'grad_norm': 1.7996411323547363, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 20:43:47 client1-1  | {'loss': 0.1099, 'grad_norm': 2.400782823562622, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 20:43:57 client1-1  | {'loss': 0.1129, 'grad_norm': 4.898026943206787, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 20:44:07 client1-1  | {'loss': 0.1091, 'grad_norm': 2.6038055419921875, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 20:44:17 client1-1  | {'loss': 0.1244, 'grad_norm': 6.358738899230957, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 20:44:26 client1-1  | {'loss': 0.1267, 'grad_norm': 9.876531600952148, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 20:44:36 client1-1  | {'loss': 0.127, 'grad_norm': 7.4781413078308105, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 20:44:45 client1-1  | {'loss': 0.1507, 'grad_norm': 6.879309177398682, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 20:58:29 client2-1  | {'loss': 0.072, 'grad_norm': 2.2847816944122314, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 20:58:39 client2-1  | {'loss': 0.0609, 'grad_norm': 2.8763887882232666, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 20:58:49 client2-1  | {'loss': 0.0796, 'grad_norm': 4.232030391693115, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 20:44:58 client1-1  | {'loss': 0.1477, 'grad_norm': 3.344132661819458, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 20:45:08 client1-1  | {'loss': 0.1173, 'grad_norm': 2.691443920135498, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 20:45:17 client1-1  | {'loss': 0.1378, 'grad_norm': 5.518052577972412, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 20:45:27 client1-1  | {'loss': 0.1699, 'grad_norm': 6.9194769859313965, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 20:58:58 client2-1  | {'loss': 0.0719, 'grad_norm': 1.0938016176223755, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 20:45:36 client1-1  | {'loss': 0.1723, 'grad_norm': 5.703030109405518, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 20:45:46 client1-1  | {'loss': 0.224, 'grad_norm': 8.178913116455078, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 20:45:55 client1-1  | {'loss': 0.2285, 'grad_norm': 11.950592994689941, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 20:46:09 client1-1  | {'loss': 0.2873, 'grad_norm': 12.474652290344238, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 20:46:18 client1-1  | {'loss': 0.2799, 'grad_norm': 9.572793960571289, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 20:46:28 client1-1  | {'loss': 0.4115, 'grad_norm': 11.038320541381836, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 20:46:38 client1-1  | {'loss': 0.4889, 'grad_norm': 12.805803298950195, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 20:46:47 client1-1  | {'loss': 0.5053, 'grad_norm': 11.839057922363281, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 20:46:57 client1-1  | {'loss': 0.735, 'grad_norm': 17.73783302307129, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 20:47:06 client1-1  | {'loss': 0.7158, 'grad_norm': 16.711994171142578, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 20:47:19 client1-1  | {'loss': 0.8994, 'grad_norm': 13.500079154968262, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 20:47:29 client1-1  | {'loss': 0.8826, 'grad_norm': 16.017887115478516, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 20:47:38 client1-1  | {'loss': 0.6885, 'grad_norm': 11.82862377166748, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 20:59:11 client2-1  | {'loss': 0.0799, 'grad_norm': 4.376563549041748, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 20:59:21 client2-1  | {'loss': 0.0784, 'grad_norm': 3.1937646865844727, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 20:59:30 client2-1  | {'loss': 0.0898, 'grad_norm': 3.9276721477508545, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 20:47:48 client1-1  | {'loss': 0.2471, 'grad_norm': 5.542613983154297, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 20:59:40 client2-1  | {'loss': 0.0933, 'grad_norm': 1.7476763725280762, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 20:59:49 client2-1  | {'loss': 0.0855, 'grad_norm': 1.176419734954834, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 20:59:59 client2-1  | {'loss': 0.0937, 'grad_norm': 4.561619758605957, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 21:00:08 client2-1  | {'loss': 0.1068, 'grad_norm': 6.04408073425293, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 21:00:21 client2-1  | {'loss': 0.0974, 'grad_norm': 1.8652153015136719, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 21:00:31 client2-1  | {'loss': 0.0902, 'grad_norm': 3.5105104446411133, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 20:47:57 client1-1  | {'loss': 0.0933, 'grad_norm': 5.729248046875, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 20:47:58 client1-1  | {'train_runtime': 505.3488, 'train_samples_per_second': 1.979, 'train_steps_per_second': 0.989, 'train_loss': 0.20883572971820832, 'epoch': 1.0}
2025-05-13 20:48:05 client1-1  | INFO :      Sent reply
2025-05-13 20:48:17 client1-1  | INFO :      
2025-05-13 20:48:17 client1-1  | INFO :      Received: train message 1ba384eb-8e7d-4182-a7fb-72174b42727d
2025-05-13 20:48:28 client1-1  | {'loss': 0.0657, 'grad_norm': 2.34159517288208, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 20:48:38 client1-1  | {'loss': 0.0624, 'grad_norm': 1.6866281032562256, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 21:00:40 client2-1  | {'loss': 0.1008, 'grad_norm': 5.148935794830322, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 20:48:51 client1-1  | {'loss': 0.0632, 'grad_norm': 2.891120433807373, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 20:49:00 client1-1  | {'loss': 0.0798, 'grad_norm': 2.9395620822906494, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 21:00:50 client2-1  | {'loss': 0.1032, 'grad_norm': 4.461277484893799, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 21:00:59 client2-1  | {'loss': 0.087, 'grad_norm': 4.184380054473877, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 21:01:09 client2-1  | {'loss': 0.1108, 'grad_norm': 4.350489139556885, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 21:01:22 client2-1  | {'loss': 0.1001, 'grad_norm': 2.202047824859619, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 20:49:10 client1-1  | {'loss': 0.0668, 'grad_norm': 1.5425957441329956, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 20:49:19 client1-1  | {'loss': 0.0649, 'grad_norm': 1.9964638948440552, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 20:49:29 client1-1  | {'loss': 0.0904, 'grad_norm': 2.5809597969055176, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 20:49:42 client1-1  | {'loss': 0.0777, 'grad_norm': 3.3754734992980957, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 20:49:52 client1-1  | {'loss': 0.0795, 'grad_norm': 4.524099349975586, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 21:01:31 client2-1  | {'loss': 0.0958, 'grad_norm': 8.981916427612305, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 21:01:41 client2-1  | {'loss': 0.1082, 'grad_norm': 7.300468921661377, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 20:50:01 client1-1  | {'loss': 0.0849, 'grad_norm': 2.7922704219818115, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 20:50:11 client1-1  | {'loss': 0.0845, 'grad_norm': 4.139394760131836, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 20:50:20 client1-1  | {'loss': 0.0855, 'grad_norm': 3.935849189758301, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 21:01:50 client2-1  | {'loss': 0.1184, 'grad_norm': 2.319518566131592, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 21:02:00 client2-1  | {'loss': 0.117, 'grad_norm': 6.3364667892456055, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 21:02:09 client2-1  | {'loss': 0.1177, 'grad_norm': 5.384070873260498, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 21:02:19 client2-1  | {'loss': 0.1205, 'grad_norm': 6.180577754974365, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 21:02:32 client2-1  | {'loss': 0.1223, 'grad_norm': 5.587447643280029, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 20:50:30 client1-1  | {'loss': 0.0939, 'grad_norm': 4.659376621246338, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 20:50:39 client1-1  | {'loss': 0.0908, 'grad_norm': 2.2229082584381104, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 20:50:52 client1-1  | {'loss': 0.1155, 'grad_norm': 5.02472448348999, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 20:51:02 client1-1  | {'loss': 0.0847, 'grad_norm': 4.041940212249756, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 20:51:11 client1-1  | {'loss': 0.0767, 'grad_norm': 1.3878334760665894, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 21:02:41 client2-1  | {'loss': 0.1272, 'grad_norm': 5.666601657867432, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 21:02:51 client2-1  | {'loss': 0.1356, 'grad_norm': 6.274683475494385, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 21:03:01 client2-1  | {'loss': 0.1392, 'grad_norm': 2.8337130546569824, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 20:51:20 client1-1  | {'loss': 0.0923, 'grad_norm': 4.395617485046387, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 21:03:10 client2-1  | {'loss': 0.1509, 'grad_norm': 7.454394340515137, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 20:51:30 client1-1  | {'loss': 0.0914, 'grad_norm': 5.0193772315979, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 21:03:20 client2-1  | {'loss': 0.2508, 'grad_norm': 8.993249893188477, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 20:51:39 client1-1  | {'loss': 0.0955, 'grad_norm': 5.112525463104248, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 20:51:53 client1-1  | {'loss': 0.1085, 'grad_norm': 2.406466245651245, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 20:52:02 client1-1  | {'loss': 0.1028, 'grad_norm': 3.3564703464508057, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 20:52:12 client1-1  | {'loss': 0.1117, 'grad_norm': 5.99086332321167, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 20:52:21 client1-1  | {'loss': 0.0911, 'grad_norm': 5.073643207550049, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 20:52:31 client1-1  | {'loss': 0.098, 'grad_norm': 3.7371881008148193, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 21:03:29 client2-1  | {'loss': 0.1914, 'grad_norm': 5.801275253295898, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 21:03:39 client2-1  | {'loss': 0.2319, 'grad_norm': 5.288782119750977, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 21:03:52 client2-1  | {'loss': 0.25, 'grad_norm': 10.777359008789062, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 21:04:01 client2-1  | {'loss': 0.3367, 'grad_norm': 9.850377082824707, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 21:04:11 client2-1  | {'loss': 0.3917, 'grad_norm': 10.179542541503906, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 21:04:20 client2-1  | {'loss': 0.4533, 'grad_norm': 14.61259651184082, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 21:04:30 client2-1  | {'loss': 0.5523, 'grad_norm': 12.287172317504883, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 20:52:40 client1-1  | {'loss': 0.1125, 'grad_norm': 4.815639495849609, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 20:52:50 client1-1  | {'loss': 0.0989, 'grad_norm': 4.02908992767334, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 20:53:03 client1-1  | {'loss': 0.1194, 'grad_norm': 5.599853038787842, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 20:53:12 client1-1  | {'loss': 0.1274, 'grad_norm': 7.607940196990967, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 20:53:22 client1-1  | {'loss': 0.1228, 'grad_norm': 6.16224479675293, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 21:04:39 client2-1  | {'loss': 0.5923, 'grad_norm': 12.67779541015625, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 21:04:52 client2-1  | {'loss': 0.8323, 'grad_norm': 18.880876541137695, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 20:53:31 client1-1  | {'loss': 0.1342, 'grad_norm': 4.833973407745361, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 20:53:41 client1-1  | {'loss': 0.1441, 'grad_norm': 3.486593246459961, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 20:53:54 client1-1  | {'loss': 0.1131, 'grad_norm': 2.851229429244995, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 20:54:03 client1-1  | {'loss': 0.1551, 'grad_norm': 4.442561626434326, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 20:54:13 client1-1  | {'loss': 0.162, 'grad_norm': 8.042804718017578, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 20:54:23 client1-1  | {'loss': 0.1613, 'grad_norm': 6.101459980010986, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 20:54:32 client1-1  | {'loss': 0.2057, 'grad_norm': 8.464831352233887, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 21:05:02 client2-1  | {'loss': 0.936, 'grad_norm': 17.824020385742188, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 20:54:42 client1-1  | {'loss': 0.2146, 'grad_norm': 11.346878051757812, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 21:05:11 client2-1  | {'loss': 0.485, 'grad_norm': 6.13031005859375, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 21:05:21 client2-1  | {'loss': 0.2466, 'grad_norm': 3.8691394329071045, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 20:54:52 client1-1  | {'loss': 0.2552, 'grad_norm': 14.620261192321777, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 20:55:05 client1-1  | {'loss': 0.2579, 'grad_norm': 10.305401802062988, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 21:05:31 client2-1  | {'loss': 0.0827, 'grad_norm': 2.738013505935669, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 21:05:31 client2-1  | {'train_runtime': 503.9622, 'train_samples_per_second': 1.984, 'train_steps_per_second': 0.992, 'train_loss': 0.18494631767272948, 'epoch': 1.0}
2025-05-13 21:05:40 client2-1  | INFO :      Sent reply
2025-05-13 21:05:52 client2-1  | INFO :      
2025-05-13 21:05:52 client2-1  | INFO :      Received: train message c88fe8e8-468b-4c8f-b3b2-e931ad56943b
2025-05-13 20:55:14 client1-1  | {'loss': 0.361, 'grad_norm': 11.188291549682617, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 21:06:03 client2-1  | {'loss': 0.0575, 'grad_norm': 1.3172169923782349, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 20:55:24 client1-1  | {'loss': 0.4515, 'grad_norm': 12.23657512664795, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 20:55:33 client1-1  | {'loss': 0.4799, 'grad_norm': 11.77546215057373, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 20:55:43 client1-1  | {'loss': 0.7137, 'grad_norm': 17.32931900024414, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 21:06:17 client2-1  | {'loss': 0.0566, 'grad_norm': 1.5009276866912842, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 21:06:26 client2-1  | {'loss': 0.0572, 'grad_norm': 2.893076181411743, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 20:55:52 client1-1  | {'loss': 0.6953, 'grad_norm': 20.206737518310547, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 21:06:36 client2-1  | {'loss': 0.0776, 'grad_norm': 4.583682060241699, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 21:06:45 client2-1  | {'loss': 0.086, 'grad_norm': 4.007617473602295, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 21:06:55 client2-1  | {'loss': 0.0665, 'grad_norm': 2.5832221508026123, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 21:07:04 client2-1  | {'loss': 0.0655, 'grad_norm': 2.896967649459839, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 21:07:17 client2-1  | {'loss': 0.0862, 'grad_norm': 3.721388816833496, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 21:07:27 client2-1  | {'loss': 0.0639, 'grad_norm': 3.0647106170654297, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 21:07:36 client2-1  | {'loss': 0.0727, 'grad_norm': 2.827242136001587, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 20:56:05 client1-1  | {'loss': 0.859, 'grad_norm': 15.008512496948242, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 20:56:15 client1-1  | {'loss': 0.8678, 'grad_norm': 17.96210289001465, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 21:07:46 client2-1  | {'loss': 0.0702, 'grad_norm': 2.1281466484069824, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 21:07:56 client2-1  | {'loss': 0.083, 'grad_norm': 3.5043201446533203, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 20:56:24 client1-1  | {'loss': 0.6456, 'grad_norm': 10.888066291809082, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 20:56:34 client1-1  | {'loss': 0.2269, 'grad_norm': 4.921088695526123, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 20:56:43 client1-1  | {'loss': 0.0866, 'grad_norm': 3.5476274490356445, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 20:56:43 client1-1  | {'train_runtime': 504.6496, 'train_samples_per_second': 1.982, 'train_steps_per_second': 0.991, 'train_loss': 0.19860017240047456, 'epoch': 1.0}
2025-05-13 20:56:51 client1-1  | INFO :      Sent reply
2025-05-13 21:08:05 client2-1  | {'loss': 0.0901, 'grad_norm': 2.613471746444702, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 21:08:15 client2-1  | {'loss': 0.0853, 'grad_norm': 2.8464648723602295, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 20:57:06 client1-1  | INFO :      
2025-05-13 20:57:06 client1-1  | INFO :      Received: train message 014573b6-bffe-4397-83a9-21aa4046cc0c
2025-05-13 20:57:17 client1-1  | {'loss': 0.0709, 'grad_norm': 1.2289785146713257, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 20:57:30 client1-1  | {'loss': 0.0611, 'grad_norm': 1.4657682180404663, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 20:57:40 client1-1  | {'loss': 0.0649, 'grad_norm': 3.3872809410095215, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 20:57:49 client1-1  | {'loss': 0.0722, 'grad_norm': 2.085750102996826, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 20:57:59 client1-1  | {'loss': 0.0645, 'grad_norm': 2.5400304794311523, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 20:58:09 client1-1  | {'loss': 0.065, 'grad_norm': 1.9737375974655151, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 21:08:28 client2-1  | {'loss': 0.0981, 'grad_norm': 5.48443603515625, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 21:08:37 client2-1  | {'loss': 0.0766, 'grad_norm': 2.2004501819610596, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 21:08:47 client2-1  | {'loss': 0.0881, 'grad_norm': 4.363224029541016, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 21:08:56 client2-1  | {'loss': 0.0967, 'grad_norm': 3.3911092281341553, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 20:58:22 client1-1  | {'loss': 0.0764, 'grad_norm': 2.7142248153686523, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 20:58:31 client1-1  | {'loss': 0.0718, 'grad_norm': 4.64550256729126, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 20:58:41 client1-1  | {'loss': 0.0869, 'grad_norm': 2.563619613647461, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 20:58:51 client1-1  | {'loss': 0.082, 'grad_norm': 2.6854846477508545, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 20:59:00 client1-1  | {'loss': 0.0881, 'grad_norm': 2.543278932571411, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 20:59:10 client1-1  | {'loss': 0.093, 'grad_norm': 8.38902759552002, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 20:59:19 client1-1  | {'loss': 0.0761, 'grad_norm': 4.463688850402832, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 20:59:32 client1-1  | {'loss': 0.0784, 'grad_norm': 2.1398427486419678, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 20:59:42 client1-1  | {'loss': 0.0963, 'grad_norm': 4.385915279388428, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 21:09:06 client2-1  | {'loss': 0.0944, 'grad_norm': 3.979139804840088, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 21:09:15 client2-1  | {'loss': 0.0999, 'grad_norm': 3.0571539402008057, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 20:59:51 client1-1  | {'loss': 0.0746, 'grad_norm': 3.640883445739746, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 21:09:28 client2-1  | {'loss': 0.113, 'grad_norm': 2.7398810386657715, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 21:09:38 client2-1  | {'loss': 0.1175, 'grad_norm': 8.721314430236816, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 21:09:47 client2-1  | {'loss': 0.1016, 'grad_norm': 2.9433062076568604, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 21:00:01 client1-1  | {'loss': 0.0717, 'grad_norm': 2.20298171043396, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 21:00:11 client1-1  | {'loss': 0.1015, 'grad_norm': 4.97520637512207, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 21:00:24 client1-1  | {'loss': 0.0877, 'grad_norm': 2.710477828979492, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 21:00:33 client1-1  | {'loss': 0.0986, 'grad_norm': 2.9731605052948, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 21:00:43 client1-1  | {'loss': 0.0847, 'grad_norm': 8.819550514221191, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 21:00:52 client1-1  | {'loss': 0.0946, 'grad_norm': 3.518322229385376, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 21:01:02 client1-1  | {'loss': 0.102, 'grad_norm': 11.139628410339355, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 21:01:11 client1-1  | {'loss': 0.0888, 'grad_norm': 3.0210471153259277, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 21:09:57 client2-1  | {'loss': 0.1164, 'grad_norm': 4.514910697937012, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 21:10:06 client2-1  | {'loss': 0.0954, 'grad_norm': 1.8854209184646606, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 21:10:16 client2-1  | {'loss': 0.1, 'grad_norm': 6.120225429534912, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 21:10:25 client2-1  | {'loss': 0.1051, 'grad_norm': 5.571295738220215, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 21:10:38 client2-1  | {'loss': 0.1147, 'grad_norm': 4.539881706237793, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 21:10:48 client2-1  | {'loss': 0.1088, 'grad_norm': 2.1541481018066406, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 21:10:57 client2-1  | {'loss': 0.1232, 'grad_norm': 7.200158596038818, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 21:11:07 client2-1  | {'loss': 0.1174, 'grad_norm': 8.473448753356934, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 21:11:16 client2-1  | {'loss': 0.1141, 'grad_norm': 6.055936336517334, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 21:01:24 client1-1  | {'loss': 0.1021, 'grad_norm': 6.714561462402344, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 21:11:26 client2-1  | {'loss': 0.1204, 'grad_norm': 2.6704018115997314, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 21:11:35 client2-1  | {'loss': 0.1358, 'grad_norm': 13.048038482666016, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 21:11:49 client2-1  | {'loss': 0.1468, 'grad_norm': 4.993374347686768, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 21:01:34 client1-1  | {'loss': 0.1041, 'grad_norm': 5.110413074493408, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 21:01:43 client1-1  | {'loss': 0.0957, 'grad_norm': 2.4037981033325195, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 21:01:53 client1-1  | {'loss': 0.1226, 'grad_norm': 5.9271626472473145, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 21:02:02 client1-1  | {'loss': 0.1225, 'grad_norm': 5.6469879150390625, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 21:02:16 client1-1  | {'loss': 0.1174, 'grad_norm': 4.129348278045654, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 21:11:58 client2-1  | {'loss': 0.1602, 'grad_norm': 7.52933931350708, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 21:12:07 client2-1  | {'loss': 0.21, 'grad_norm': 5.691049575805664, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 21:12:17 client2-1  | {'loss': 0.164, 'grad_norm': 5.087608337402344, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 21:12:26 client2-1  | {'loss': 0.2043, 'grad_norm': 4.585046291351318, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 21:12:36 client2-1  | {'loss': 0.2288, 'grad_norm': 10.904244422912598, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 21:12:49 client2-1  | {'loss': 0.3158, 'grad_norm': 10.488576889038086, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 21:12:58 client2-1  | {'loss': 0.3677, 'grad_norm': 9.595513343811035, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 21:13:08 client2-1  | {'loss': 0.4, 'grad_norm': 12.125239372253418, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 21:02:25 client1-1  | {'loss': 0.1369, 'grad_norm': 5.356877326965332, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 21:02:35 client1-1  | {'loss': 0.1323, 'grad_norm': 4.864746570587158, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 21:02:44 client1-1  | {'loss': 0.1117, 'grad_norm': 1.6724869012832642, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 21:02:54 client1-1  | {'loss': 0.1215, 'grad_norm': 3.0648891925811768, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 21:13:17 client2-1  | {'loss': 0.5405, 'grad_norm': 10.579244613647461, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 21:13:27 client2-1  | {'loss': 0.5485, 'grad_norm': 12.810657501220703, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 21:13:36 client2-1  | {'loss': 0.8287, 'grad_norm': 18.014585494995117, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 21:13:50 client2-1  | {'loss': 0.9091, 'grad_norm': 14.914592742919922, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 21:13:59 client2-1  | {'loss': 0.4739, 'grad_norm': 6.381469249725342, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 21:14:09 client2-1  | {'loss': 0.2307, 'grad_norm': 4.306033611297607, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 21:03:07 client1-1  | {'loss': 0.14, 'grad_norm': 7.841940879821777, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 21:03:17 client1-1  | {'loss': 0.1483, 'grad_norm': 5.649628639221191, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 21:03:26 client1-1  | {'loss': 0.1802, 'grad_norm': 11.173821449279785, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 21:03:36 client1-1  | {'loss': 0.1902, 'grad_norm': 10.173419952392578, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 21:03:45 client1-1  | {'loss': 0.2218, 'grad_norm': 11.21179485321045, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 21:03:55 client1-1  | {'loss': 0.2512, 'grad_norm': 10.161016464233398, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 21:04:08 client1-1  | {'loss': 0.3315, 'grad_norm': 9.53174114227295, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 21:14:18 client2-1  | {'loss': 0.0715, 'grad_norm': 2.3107833862304688, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 21:04:18 client1-1  | {'loss': 0.4182, 'grad_norm': 13.625648498535156, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 21:04:27 client1-1  | {'loss': 0.451, 'grad_norm': 10.000616073608398, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 21:14:18 client2-1  | {'train_runtime': 505.1469, 'train_samples_per_second': 1.98, 'train_steps_per_second': 0.99, 'train_loss': 0.17911637580394746, 'epoch': 1.0}
2025-05-13 21:14:29 client2-1  | INFO :      Sent reply
2025-05-13 21:14:42 client2-1  | INFO :      
2025-05-13 21:14:42 client2-1  | INFO :      Received: train message e18bcabd-df29-4eb8-91cf-f03459cdd9f0
2025-05-13 21:04:37 client1-1  | {'loss': 0.647, 'grad_norm': 17.852699279785156, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 21:04:46 client1-1  | {'loss': 0.658, 'grad_norm': 18.891950607299805, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 21:14:53 client2-1  | {'loss': 0.0584, 'grad_norm': 1.3618961572647095, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 21:04:55 client1-1  | {'loss': 0.8375, 'grad_norm': 16.74701499938965, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 21:15:03 client2-1  | {'loss': 0.0573, 'grad_norm': 2.3782758712768555, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 21:05:05 client1-1  | {'loss': 0.8199, 'grad_norm': 19.954580307006836, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 21:05:18 client1-1  | {'loss': 0.5989, 'grad_norm': 7.256278038024902, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 21:05:28 client1-1  | {'loss': 0.2091, 'grad_norm': 4.000607967376709, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 21:15:16 client2-1  | {'loss': 0.0696, 'grad_norm': 4.858410835266113, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 21:15:26 client2-1  | {'loss': 0.0738, 'grad_norm': 6.881339073181152, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 21:05:37 client1-1  | {'loss': 0.072, 'grad_norm': 3.6031973361968994, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 21:05:37 client1-1  | {'train_runtime': 509.4647, 'train_samples_per_second': 1.963, 'train_steps_per_second': 0.981, 'train_loss': 0.1858613418340683, 'epoch': 1.0}
2025-05-13 21:05:41 client1-1  | INFO :      Sent reply
2025-05-13 21:15:35 client2-1  | {'loss': 0.0831, 'grad_norm': 2.732921600341797, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 21:15:45 client2-1  | {'loss': 0.0622, 'grad_norm': 1.9039667844772339, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 21:15:55 client2-1  | {'loss': 0.0684, 'grad_norm': 1.3383324146270752, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 21:05:52 client1-1  | INFO :      
2025-05-13 21:05:52 client1-1  | INFO :      Received: train message 8cc69873-f641-4630-a4c0-8cbc0e0c5d6f
2025-05-13 21:16:04 client2-1  | {'loss': 0.0835, 'grad_norm': 4.0102081298828125, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 21:06:05 client1-1  | {'loss': 0.0654, 'grad_norm': 1.4421769380569458, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 21:16:17 client2-1  | {'loss': 0.0686, 'grad_norm': 1.9321414232254028, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 21:16:27 client2-1  | {'loss': 0.0698, 'grad_norm': 2.2029876708984375, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 21:16:37 client2-1  | {'loss': 0.0645, 'grad_norm': 1.4261921644210815, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 21:16:46 client2-1  | {'loss': 0.086, 'grad_norm': 4.439714431762695, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 21:16:56 client2-1  | {'loss': 0.0757, 'grad_norm': 2.437366008758545, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 21:17:05 client2-1  | {'loss': 0.0737, 'grad_norm': 4.2635111808776855, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 21:17:15 client2-1  | {'loss': 0.0919, 'grad_norm': 2.5690555572509766, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 21:17:28 client2-1  | {'loss': 0.079, 'grad_norm': 1.193844199180603, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 21:17:38 client2-1  | {'loss': 0.0987, 'grad_norm': 4.665804386138916, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 21:17:47 client2-1  | {'loss': 0.0918, 'grad_norm': 6.093025207519531, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 21:17:57 client2-1  | {'loss': 0.0862, 'grad_norm': 5.559779167175293, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 21:06:15 client1-1  | {'loss': 0.0573, 'grad_norm': 1.5651588439941406, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 21:06:24 client1-1  | {'loss': 0.0723, 'grad_norm': 3.961425304412842, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 21:18:07 client2-1  | {'loss': 0.0837, 'grad_norm': 4.381363868713379, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 21:18:16 client2-1  | {'loss': 0.1006, 'grad_norm': 2.9411401748657227, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 21:06:34 client1-1  | {'loss': 0.0708, 'grad_norm': 2.9797422885894775, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 21:18:25 client2-1  | {'loss': 0.1034, 'grad_norm': 6.289535045623779, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 21:18:35 client2-1  | {'loss': 0.0848, 'grad_norm': 5.095069885253906, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 21:18:48 client2-1  | {'loss': 0.0955, 'grad_norm': 2.7384371757507324, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 21:06:47 client1-1  | {'loss': 0.0702, 'grad_norm': 2.677640199661255, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 21:06:57 client1-1  | {'loss': 0.0675, 'grad_norm': 3.8527917861938477, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 21:07:06 client1-1  | {'loss': 0.0738, 'grad_norm': 1.9595739841461182, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 21:07:16 client1-1  | {'loss': 0.0743, 'grad_norm': 1.6366164684295654, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 21:18:57 client2-1  | {'loss': 0.0948, 'grad_norm': 1.8514537811279297, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 21:07:25 client1-1  | {'loss': 0.0814, 'grad_norm': 5.160472393035889, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 21:07:35 client1-1  | {'loss': 0.0817, 'grad_norm': 3.7003164291381836, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 21:07:44 client1-1  | {'loss': 0.0852, 'grad_norm': 4.253270626068115, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 21:07:57 client1-1  | {'loss': 0.0817, 'grad_norm': 4.60652494430542, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 21:08:07 client1-1  | {'loss': 0.073, 'grad_norm': 4.152004718780518, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 21:08:17 client1-1  | {'loss': 0.0889, 'grad_norm': 1.4008541107177734, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 21:19:07 client2-1  | {'loss': 0.0852, 'grad_norm': 7.346214294433594, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 21:19:16 client2-1  | {'loss': 0.1157, 'grad_norm': 5.894742012023926, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 21:19:26 client2-1  | {'loss': 0.1189, 'grad_norm': 3.8375747203826904, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 21:19:35 client2-1  | {'loss': 0.0985, 'grad_norm': 3.4574532508850098, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 21:19:45 client2-1  | {'loss': 0.1163, 'grad_norm': 5.3289690017700195, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 21:08:26 client1-1  | {'loss': 0.1295, 'grad_norm': 4.666423797607422, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 21:08:35 client1-1  | {'loss': 0.0842, 'grad_norm': 7.138357162475586, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 21:08:45 client1-1  | {'loss': 0.073, 'grad_norm': 5.4069037437438965, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 21:08:54 client1-1  | {'loss': 0.0833, 'grad_norm': 3.518078088760376, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 21:09:08 client1-1  | {'loss': 0.082, 'grad_norm': 3.135291814804077, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 21:19:58 client2-1  | {'loss': 0.1105, 'grad_norm': 3.5366668701171875, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 21:20:08 client2-1  | {'loss': 0.1107, 'grad_norm': 6.709655284881592, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 21:09:17 client1-1  | {'loss': 0.0999, 'grad_norm': 2.5108509063720703, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 21:20:17 client2-1  | {'loss': 0.1159, 'grad_norm': 4.423257350921631, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 21:09:27 client1-1  | {'loss': 0.1033, 'grad_norm': 1.7931760549545288, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 21:20:26 client2-1  | {'loss': 0.1231, 'grad_norm': 6.243624210357666, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 21:09:36 client1-1  | {'loss': 0.0911, 'grad_norm': 3.676461935043335, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 21:20:36 client2-1  | {'loss': 0.1356, 'grad_norm': 3.1537532806396484, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 21:09:45 client1-1  | {'loss': 0.0973, 'grad_norm': 4.3215718269348145, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 21:09:55 client1-1  | {'loss': 0.0791, 'grad_norm': 1.2594050168991089, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 21:20:45 client2-1  | {'loss': 0.1461, 'grad_norm': 7.831578731536865, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 21:20:55 client2-1  | {'loss': 0.2064, 'grad_norm': 6.0188703536987305, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 21:10:08 client1-1  | {'loss': 0.0957, 'grad_norm': 5.821951866149902, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 21:10:17 client1-1  | {'loss': 0.1044, 'grad_norm': 5.208968162536621, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 21:10:27 client1-1  | {'loss': 0.1083, 'grad_norm': 2.9934210777282715, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 21:10:36 client1-1  | {'loss': 0.1216, 'grad_norm': 7.414322853088379, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 21:10:46 client1-1  | {'loss': 0.1054, 'grad_norm': 3.799099922180176, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 21:10:55 client1-1  | {'loss': 0.105, 'grad_norm': 4.618441581726074, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 21:11:09 client1-1  | {'loss': 0.146, 'grad_norm': 4.740952968597412, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 21:21:08 client2-1  | {'loss': 0.1599, 'grad_norm': 6.415770530700684, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 21:21:17 client2-1  | {'loss': 0.1907, 'grad_norm': 3.421494960784912, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 21:21:27 client2-1  | {'loss': 0.2174, 'grad_norm': 12.717324256896973, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 21:21:36 client2-1  | {'loss': 0.2864, 'grad_norm': 8.643817901611328, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 21:21:46 client2-1  | {'loss': 0.328, 'grad_norm': 8.29531192779541, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 21:21:55 client2-1  | {'loss': 0.3752, 'grad_norm': 14.07375431060791, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 21:22:04 client2-1  | {'loss': 0.5012, 'grad_norm': 12.152565002441406, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 21:11:18 client1-1  | {'loss': 0.1162, 'grad_norm': 2.717945098876953, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 21:11:28 client1-1  | {'loss': 0.1009, 'grad_norm': 2.282060384750366, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 21:11:37 client1-1  | {'loss': 0.1199, 'grad_norm': 3.727628707885742, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 21:11:47 client1-1  | {'loss': 0.1398, 'grad_norm': 7.0678863525390625, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 21:11:56 client1-1  | {'loss': 0.1522, 'grad_norm': 5.1370697021484375, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 21:12:09 client1-1  | {'loss': 0.1624, 'grad_norm': 10.002397537231445, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 21:12:19 client1-1  | {'loss': 0.1863, 'grad_norm': 13.47293758392334, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 21:12:28 client1-1  | {'loss': 0.1957, 'grad_norm': 9.156722068786621, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 21:12:38 client1-1  | {'loss': 0.2201, 'grad_norm': 8.054134368896484, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 21:22:18 client2-1  | {'loss': 0.5192, 'grad_norm': 12.856386184692383, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 21:22:27 client2-1  | {'loss': 0.7611, 'grad_norm': 18.072940826416016, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 21:22:37 client2-1  | {'loss': 0.8693, 'grad_norm': 14.938791275024414, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 21:22:46 client2-1  | {'loss': 0.4443, 'grad_norm': 4.600473880767822, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 21:12:47 client1-1  | {'loss': 0.3099, 'grad_norm': 11.33460807800293, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 21:12:57 client1-1  | {'loss': 0.3935, 'grad_norm': 9.967742919921875, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 21:13:06 client1-1  | {'loss': 0.4091, 'grad_norm': 11.045929908752441, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 21:22:56 client2-1  | {'loss': 0.208, 'grad_norm': 4.455189228057861, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 21:23:05 client2-1  | {'loss': 0.0685, 'grad_norm': 4.646646499633789, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 21:23:05 client2-1  | {'train_runtime': 502.0336, 'train_samples_per_second': 1.992, 'train_steps_per_second': 0.996, 'train_loss': 0.16833886766433717, 'epoch': 1.0}
2025-05-13 21:13:19 client1-1  | {'loss': 0.6319, 'grad_norm': 19.011444091796875, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 21:13:29 client1-1  | {'loss': 0.618, 'grad_norm': 16.476327896118164, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 21:13:38 client1-1  | {'loss': 0.7997, 'grad_norm': 14.395121574401855, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 21:13:48 client1-1  | {'loss': 0.7901, 'grad_norm': 19.227130889892578, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 21:23:13 client2-1  | INFO :      Sent reply
2025-05-13 21:23:24 client2-1  | INFO :      
2025-05-13 21:13:58 client1-1  | {'loss': 0.5884, 'grad_norm': 10.166292190551758, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 21:14:07 client1-1  | {'loss': 0.2036, 'grad_norm': 4.97797155380249, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 21:23:24 client2-1  | INFO :      Received: train message 0d9a41bf-48c7-4225-8b91-28a37a5955d8
2025-05-13 21:23:37 client2-1  | {'loss': 0.0593, 'grad_norm': 1.2448636293411255, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 21:14:21 client1-1  | {'loss': 0.0727, 'grad_norm': 1.8373770713806152, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 21:14:21 client1-1  | {'train_runtime': 507.8763, 'train_samples_per_second': 1.969, 'train_steps_per_second': 0.984, 'train_loss': 0.17925643038749695, 'epoch': 1.0}
2025-05-13 21:14:28 client1-1  | INFO :      Sent reply
2025-05-13 21:14:40 client1-1  | INFO :      
2025-05-13 21:14:40 client1-1  | INFO :      Received: train message c3901072-5b53-4aed-86a6-e11a3f5ba358
2025-05-13 21:14:51 client1-1  | {'loss': 0.0696, 'grad_norm': 1.1449018716812134, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 21:15:04 client1-1  | {'loss': 0.0541, 'grad_norm': 1.9427438974380493, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 21:23:47 client2-1  | {'loss': 0.0545, 'grad_norm': 1.8300542831420898, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 21:23:56 client2-1  | {'loss': 0.0514, 'grad_norm': 2.6543569564819336, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 21:24:06 client2-1  | {'loss': 0.0763, 'grad_norm': 2.4793686866760254, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 21:24:19 client2-1  | {'loss': 0.0621, 'grad_norm': 1.7037813663482666, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 21:24:29 client2-1  | {'loss': 0.0608, 'grad_norm': 2.127476215362549, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 21:15:14 client1-1  | {'loss': 0.0594, 'grad_norm': 2.6176233291625977, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 21:24:38 client2-1  | {'loss': 0.0767, 'grad_norm': 1.8929277658462524, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 21:24:48 client2-1  | {'loss': 0.0794, 'grad_norm': 3.8122494220733643, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 21:24:57 client2-1  | {'loss': 0.0616, 'grad_norm': 1.9018993377685547, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 21:25:07 client2-1  | {'loss': 0.0709, 'grad_norm': 1.9264436960220337, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 21:25:21 client2-1  | {'loss': 0.067, 'grad_norm': 2.1091084480285645, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 21:25:30 client2-1  | {'loss': 0.0706, 'grad_norm': 1.2685426473617554, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 21:15:24 client1-1  | {'loss': 0.0591, 'grad_norm': 2.588883876800537, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 21:15:33 client1-1  | {'loss': 0.0663, 'grad_norm': 2.5895466804504395, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 21:15:43 client1-1  | {'loss': 0.0595, 'grad_norm': 1.4736080169677734, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 21:15:56 client1-1  | {'loss': 0.0815, 'grad_norm': 2.145474910736084, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 21:16:06 client1-1  | {'loss': 0.0666, 'grad_norm': 3.505021095275879, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 21:16:15 client1-1  | {'loss': 0.0863, 'grad_norm': 2.3624229431152344, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 21:16:25 client1-1  | {'loss': 0.0671, 'grad_norm': 2.5165305137634277, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 21:16:35 client1-1  | {'loss': 0.0901, 'grad_norm': 4.168371200561523, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 21:16:44 client1-1  | {'loss': 0.0803, 'grad_norm': 2.8869855403900146, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 21:16:57 client1-1  | {'loss': 0.0828, 'grad_norm': 4.324347019195557, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 21:25:40 client2-1  | {'loss': 0.0793, 'grad_norm': 1.4269863367080688, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 21:25:49 client2-1  | {'loss': 0.0808, 'grad_norm': 4.389128684997559, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 21:17:07 client1-1  | {'loss': 0.071, 'grad_norm': 1.3461755514144897, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 21:25:59 client2-1  | {'loss': 0.0904, 'grad_norm': 4.085618019104004, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 21:26:09 client2-1  | {'loss': 0.0736, 'grad_norm': 3.0179443359375, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 21:26:18 client2-1  | {'loss': 0.0868, 'grad_norm': 5.101961135864258, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 21:26:31 client2-1  | {'loss': 0.092, 'grad_norm': 4.96770715713501, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 21:26:41 client2-1  | {'loss': 0.0915, 'grad_norm': 3.5062062740325928, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 21:26:50 client2-1  | {'loss': 0.0888, 'grad_norm': 4.131223201751709, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 21:27:00 client2-1  | {'loss': 0.0946, 'grad_norm': 1.9892343282699585, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 21:27:09 client2-1  | {'loss': 0.0967, 'grad_norm': 6.5322957038879395, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 21:17:17 client1-1  | {'loss': 0.0974, 'grad_norm': 3.531898021697998, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 21:17:26 client1-1  | {'loss': 0.0785, 'grad_norm': 1.9960119724273682, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 21:27:19 client2-1  | {'loss': 0.0812, 'grad_norm': 4.1399736404418945, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 21:17:36 client1-1  | {'loss': 0.082, 'grad_norm': 4.521884918212891, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 21:17:45 client1-1  | {'loss': 0.0826, 'grad_norm': 3.4977502822875977, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 21:27:32 client2-1  | {'loss': 0.1052, 'grad_norm': 2.9575326442718506, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 21:27:42 client2-1  | {'loss': 0.111, 'grad_norm': 2.040027618408203, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 21:27:51 client2-1  | {'loss': 0.0934, 'grad_norm': 5.313024997711182, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 21:17:55 client1-1  | {'loss': 0.0958, 'grad_norm': 4.1698408126831055, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 21:18:08 client1-1  | {'loss': 0.0868, 'grad_norm': 1.6868077516555786, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 21:28:01 client2-1  | {'loss': 0.1041, 'grad_norm': 7.485022068023682, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 21:18:18 client1-1  | {'loss': 0.0894, 'grad_norm': 1.5352907180786133, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 21:18:27 client1-1  | {'loss': 0.0976, 'grad_norm': 3.33878231048584, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 21:28:10 client2-1  | {'loss': 0.1037, 'grad_norm': 2.394692897796631, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 21:28:20 client2-1  | {'loss': 0.0984, 'grad_norm': 2.044635057449341, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 21:28:33 client2-1  | {'loss': 0.101, 'grad_norm': 3.7876110076904297, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 21:28:42 client2-1  | {'loss': 0.1166, 'grad_norm': 4.5738606452941895, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 21:28:52 client2-1  | {'loss': 0.0955, 'grad_norm': 4.389929294586182, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 21:29:02 client2-1  | {'loss': 0.1077, 'grad_norm': 6.569794178009033, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 21:18:37 client1-1  | {'loss': 0.0888, 'grad_norm': 5.774498462677002, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 21:29:11 client2-1  | {'loss': 0.1245, 'grad_norm': 4.550206184387207, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 21:29:21 client2-1  | {'loss': 0.1288, 'grad_norm': 5.894909381866455, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 21:29:34 client2-1  | {'loss': 0.1392, 'grad_norm': 10.820054054260254, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 21:29:43 client2-1  | {'loss': 0.1845, 'grad_norm': 6.979031562805176, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 21:29:53 client2-1  | {'loss': 0.1641, 'grad_norm': 8.035752296447754, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 21:30:02 client2-1  | {'loss': 0.188, 'grad_norm': 3.8762667179107666, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 21:30:12 client2-1  | {'loss': 0.1914, 'grad_norm': 9.317625045776367, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 21:30:22 client2-1  | {'loss': 0.2679, 'grad_norm': 8.287997245788574, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 21:30:35 client2-1  | {'loss': 0.315, 'grad_norm': 8.2080078125, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 21:30:44 client2-1  | {'loss': 0.3425, 'grad_norm': 13.939751625061035, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 21:18:46 client1-1  | {'loss': 0.0728, 'grad_norm': 2.3973588943481445, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 21:18:56 client1-1  | {'loss': 0.096, 'grad_norm': 3.3955917358398438, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 21:19:05 client1-1  | {'loss': 0.1044, 'grad_norm': 5.143721103668213, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 21:19:18 client1-1  | {'loss': 0.0975, 'grad_norm': 3.364325523376465, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 21:19:28 client1-1  | {'loss': 0.1078, 'grad_norm': 5.625189304351807, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 21:30:54 client2-1  | {'loss': 0.4676, 'grad_norm': 14.242992401123047, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 21:31:03 client2-1  | {'loss': 0.5077, 'grad_norm': 12.223172187805176, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 21:31:13 client2-1  | {'loss': 0.7483, 'grad_norm': 17.31441307067871, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 21:19:37 client1-1  | {'loss': 0.1078, 'grad_norm': 5.189292907714844, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 21:19:47 client1-1  | {'loss': 0.1037, 'grad_norm': 5.8072190284729, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 21:19:56 client1-1  | {'loss': 0.1369, 'grad_norm': 3.1062700748443604, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 21:31:23 client2-1  | {'loss': 0.8262, 'grad_norm': 14.556198120117188, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 21:20:06 client1-1  | {'loss': 0.1139, 'grad_norm': 2.57058048248291, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 21:31:36 client2-1  | {'loss': 0.3987, 'grad_norm': 4.4687347412109375, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 21:31:45 client2-1  | {'loss': 0.1817, 'grad_norm': 2.243447780609131, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 21:31:55 client2-1  | {'loss': 0.0696, 'grad_norm': 3.6452577114105225, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 21:31:55 client2-1  | {'train_runtime': 509.6054, 'train_samples_per_second': 1.962, 'train_steps_per_second': 0.981, 'train_loss': 0.16057258665561677, 'epoch': 1.0}
2025-05-13 21:31:59 client2-1  | INFO :      Sent reply
2025-05-13 21:32:10 client2-1  | INFO :      
2025-05-13 21:32:10 client2-1  | INFO :      Received: train message a5b0ddfd-6e10-4f87-a868-6da3578a7fb2
2025-05-13 21:32:23 client2-1  | {'loss': 0.0584, 'grad_norm': 1.390175461769104, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 21:32:33 client2-1  | {'loss': 0.0563, 'grad_norm': 2.625905990600586, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 21:20:15 client1-1  | {'loss': 0.1072, 'grad_norm': 2.78378963470459, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 21:20:25 client1-1  | {'loss': 0.1079, 'grad_norm': 4.344242095947266, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 21:20:38 client1-1  | {'loss': 0.1407, 'grad_norm': 6.437091827392578, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 21:32:42 client2-1  | {'loss': 0.0491, 'grad_norm': 3.6567909717559814, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 21:32:52 client2-1  | {'loss': 0.0608, 'grad_norm': 2.439443349838257, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 21:20:47 client1-1  | {'loss': 0.1273, 'grad_norm': 7.549458026885986, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 21:20:56 client1-1  | {'loss': 0.1516, 'grad_norm': 10.219831466674805, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 21:21:06 client1-1  | {'loss': 0.1707, 'grad_norm': 9.350407600402832, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 21:21:15 client1-1  | {'loss': 0.1952, 'grad_norm': 10.880023956298828, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 21:33:05 client2-1  | {'loss': 0.0707, 'grad_norm': 3.294635534286499, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 21:33:15 client2-1  | {'loss': 0.0541, 'grad_norm': 2.2883639335632324, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 21:33:24 client2-1  | {'loss': 0.0719, 'grad_norm': 1.291405439376831, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 21:33:34 client2-1  | {'loss': 0.0716, 'grad_norm': 3.415811061859131, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 21:33:43 client2-1  | {'loss': 0.0586, 'grad_norm': 1.9100230932235718, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 21:33:53 client2-1  | {'loss': 0.0758, 'grad_norm': 1.2715976238250732, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 21:34:06 client2-1  | {'loss': 0.0743, 'grad_norm': 5.011761665344238, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 21:34:16 client2-1  | {'loss': 0.068, 'grad_norm': 0.9682712554931641, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 21:34:25 client2-1  | {'loss': 0.0692, 'grad_norm': 3.1664884090423584, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 21:34:35 client2-1  | {'loss': 0.082, 'grad_norm': 2.782653570175171, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 21:21:25 client1-1  | {'loss': 0.2026, 'grad_norm': 8.728538513183594, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 21:34:44 client2-1  | {'loss': 0.0867, 'grad_norm': 3.1340384483337402, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 21:21:34 client1-1  | {'loss': 0.2828, 'grad_norm': 10.296279907226562, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 21:21:44 client1-1  | {'loss': 0.3567, 'grad_norm': 10.831783294677734, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 21:21:57 client1-1  | {'loss': 0.3771, 'grad_norm': 8.503271102905273, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 21:22:06 client1-1  | {'loss': 0.5822, 'grad_norm': 20.611949920654297, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 21:34:54 client2-1  | {'loss': 0.073, 'grad_norm': 2.388226270675659, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 21:35:07 client2-1  | {'loss': 0.0927, 'grad_norm': 3.525942802429199, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 21:22:16 client1-1  | {'loss': 0.6, 'grad_norm': 20.682180404663086, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 21:22:25 client1-1  | {'loss': 0.7999, 'grad_norm': 14.563507080078125, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 21:22:35 client1-1  | {'loss': 0.7529, 'grad_norm': 16.59891128540039, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 21:35:17 client2-1  | {'loss': 0.0808, 'grad_norm': 4.963374614715576, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 21:35:27 client2-1  | {'loss': 0.0833, 'grad_norm': 2.9048922061920166, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 21:35:36 client2-1  | {'loss': 0.0828, 'grad_norm': 5.46589469909668, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 21:35:46 client2-1  | {'loss': 0.0993, 'grad_norm': 2.2420058250427246, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 21:22:44 client1-1  | {'loss': 0.5483, 'grad_norm': 12.367108345031738, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 21:22:54 client1-1  | {'loss': 0.1746, 'grad_norm': 4.99216890335083, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 21:23:07 client1-1  | {'loss': 0.0836, 'grad_norm': 4.672645568847656, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 21:35:55 client2-1  | {'loss': 0.1056, 'grad_norm': 6.837408065795898, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 21:23:07 client1-1  | {'train_runtime': 505.3325, 'train_samples_per_second': 1.979, 'train_steps_per_second': 0.989, 'train_loss': 0.1698935877084732, 'epoch': 1.0}
2025-05-13 21:36:09 client2-1  | {'loss': 0.0773, 'grad_norm': 3.621284246444702, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 21:36:19 client2-1  | {'loss': 0.0922, 'grad_norm': 2.9842333793640137, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 21:36:28 client2-1  | {'loss': 0.094, 'grad_norm': 1.3873921632766724, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 21:36:37 client2-1  | {'loss': 0.0963, 'grad_norm': 5.658836841583252, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 21:36:47 client2-1  | {'loss': 0.0899, 'grad_norm': 3.7221529483795166, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 21:36:56 client2-1  | {'loss': 0.1031, 'grad_norm': 5.958703517913818, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 21:37:09 client2-1  | {'loss': 0.0947, 'grad_norm': 7.624616622924805, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 21:37:19 client2-1  | {'loss': 0.104, 'grad_norm': 6.651913166046143, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 21:23:11 client1-1  | INFO :      Sent reply
2025-05-13 21:23:24 client1-1  | INFO :      
2025-05-13 21:37:28 client2-1  | {'loss': 0.1177, 'grad_norm': 3.9668076038360596, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 21:37:38 client2-1  | {'loss': 0.1063, 'grad_norm': 3.6910512447357178, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 21:37:47 client2-1  | {'loss': 0.1009, 'grad_norm': 3.8665037155151367, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 21:37:56 client2-1  | {'loss': 0.1168, 'grad_norm': 4.991138935089111, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 21:23:24 client1-1  | INFO :      Received: train message d49743fc-85bc-48d6-8e47-505c11e0d666
2025-05-13 21:23:35 client1-1  | {'loss': 0.0644, 'grad_norm': 1.3880993127822876, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 21:23:45 client1-1  | {'loss': 0.0609, 'grad_norm': 1.5315779447555542, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 21:38:10 client2-1  | {'loss': 0.1252, 'grad_norm': 3.2909352779388428, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 21:38:19 client2-1  | {'loss': 0.1358, 'grad_norm': 9.900415420532227, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 21:23:58 client1-1  | {'loss': 0.065, 'grad_norm': 3.201695442199707, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 21:24:07 client1-1  | {'loss': 0.0605, 'grad_norm': 1.9257681369781494, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 21:38:29 client2-1  | {'loss': 0.1641, 'grad_norm': 6.186541557312012, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 21:38:39 client2-1  | {'loss': 0.144, 'grad_norm': 9.639533996582031, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 21:38:48 client2-1  | {'loss': 0.1757, 'grad_norm': 2.458101272583008, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 21:38:57 client2-1  | {'loss': 0.188, 'grad_norm': 10.25267219543457, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 21:39:07 client2-1  | {'loss': 0.2427, 'grad_norm': 9.162140846252441, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 21:39:20 client2-1  | {'loss': 0.3031, 'grad_norm': 6.3854265213012695, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 21:39:30 client2-1  | {'loss': 0.3389, 'grad_norm': 13.189241409301758, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 21:39:39 client2-1  | {'loss': 0.4611, 'grad_norm': 9.335132598876953, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 21:39:49 client2-1  | {'loss': 0.4728, 'grad_norm': 10.424564361572266, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 21:24:17 client1-1  | {'loss': 0.06, 'grad_norm': 1.5568015575408936, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 21:24:26 client1-1  | {'loss': 0.0573, 'grad_norm': 3.7119293212890625, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 21:39:58 client2-1  | {'loss': 0.7242, 'grad_norm': 17.748533248901367, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 21:24:36 client1-1  | {'loss': 0.0817, 'grad_norm': 2.364978790283203, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 21:40:08 client2-1  | {'loss': 0.8094, 'grad_norm': 14.318534851074219, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 21:24:46 client1-1  | {'loss': 0.0645, 'grad_norm': 1.7839716672897339, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 21:40:17 client2-1  | {'loss': 0.384, 'grad_norm': 5.364238262176514, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 21:40:31 client2-1  | {'loss': 0.1812, 'grad_norm': 3.314030170440674, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 21:24:59 client1-1  | {'loss': 0.078, 'grad_norm': 2.3940579891204834, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 21:40:40 client2-1  | {'loss': 0.0738, 'grad_norm': 4.213410377502441, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 21:40:40 client2-1  | {'train_runtime': 509.0874, 'train_samples_per_second': 1.964, 'train_steps_per_second': 0.982, 'train_loss': 0.15484407162666322, 'epoch': 1.0}
2025-05-13 21:25:09 client1-1  | {'loss': 0.0787, 'grad_norm': 2.947763204574585, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 21:40:44 client2-1  | INFO :      Sent reply
2025-05-13 21:25:18 client1-1  | {'loss': 0.0725, 'grad_norm': 4.76761531829834, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 21:40:58 client2-1  | INFO :      
2025-05-13 21:40:58 client2-1  | INFO :      Received: train message 031ab2c8-523c-4745-81a1-df3926f9f04d
2025-05-13 21:41:09 client2-1  | {'loss': 0.0557, 'grad_norm': 2.8862624168395996, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 21:41:19 client2-1  | {'loss': 0.0666, 'grad_norm': 1.5483756065368652, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 21:41:32 client2-1  | {'loss': 0.0462, 'grad_norm': 5.577009677886963, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 21:41:41 client2-1  | {'loss': 0.0642, 'grad_norm': 2.9856836795806885, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 21:41:51 client2-1  | {'loss': 0.0764, 'grad_norm': 4.936882019042969, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 21:25:28 client1-1  | {'loss': 0.0989, 'grad_norm': 3.912445306777954, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 21:25:38 client1-1  | {'loss': 0.0814, 'grad_norm': 3.303541421890259, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 21:25:47 client1-1  | {'loss': 0.08, 'grad_norm': 4.406358242034912, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 21:25:57 client1-1  | {'loss': 0.1106, 'grad_norm': 9.17724609375, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 21:26:06 client1-1  | {'loss': 0.0766, 'grad_norm': 1.7791951894760132, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 21:26:19 client1-1  | {'loss': 0.0786, 'grad_norm': 2.8870458602905273, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 21:26:29 client1-1  | {'loss': 0.0884, 'grad_norm': 3.4668712615966797, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 21:42:00 client2-1  | {'loss': 0.0558, 'grad_norm': 1.953857421875, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 21:26:38 client1-1  | {'loss': 0.0909, 'grad_norm': 1.9411998987197876, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 21:26:48 client1-1  | {'loss': 0.0972, 'grad_norm': 3.230374813079834, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 21:26:57 client1-1  | {'loss': 0.0829, 'grad_norm': 1.3677194118499756, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 21:27:07 client1-1  | {'loss': 0.0807, 'grad_norm': 5.41740608215332, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 21:27:16 client1-1  | {'loss': 0.0833, 'grad_norm': 4.117518424987793, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 21:27:26 client1-1  | {'loss': 0.0746, 'grad_norm': 3.3720245361328125, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 21:27:39 client1-1  | {'loss': 0.0911, 'grad_norm': 3.45157790184021, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 21:27:49 client1-1  | {'loss': 0.0977, 'grad_norm': 5.265722751617432, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 21:42:10 client2-1  | {'loss': 0.0737, 'grad_norm': 1.471307635307312, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 21:27:58 client1-1  | {'loss': 0.0925, 'grad_norm': 2.009587049484253, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 21:28:08 client1-1  | {'loss': 0.1062, 'grad_norm': 6.19362211227417, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 21:42:19 client2-1  | {'loss': 0.0823, 'grad_norm': 3.0133512020111084, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 21:42:33 client2-1  | {'loss': 0.0567, 'grad_norm': 3.000915050506592, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 21:42:42 client2-1  | {'loss': 0.0736, 'grad_norm': 3.022252321243286, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 21:28:17 client1-1  | {'loss': 0.1169, 'grad_norm': 4.217614650726318, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 21:28:27 client1-1  | {'loss': 0.108, 'grad_norm': 3.9712510108947754, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 21:42:52 client2-1  | {'loss': 0.0632, 'grad_norm': 2.527094841003418, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 21:28:40 client1-1  | {'loss': 0.1081, 'grad_norm': 3.5168821811676025, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 21:43:02 client2-1  | {'loss': 0.0756, 'grad_norm': 2.4300296306610107, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 21:28:49 client1-1  | {'loss': 0.1105, 'grad_norm': 1.9957869052886963, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 21:28:59 client1-1  | {'loss': 0.1042, 'grad_norm': 3.3972933292388916, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 21:29:08 client1-1  | {'loss': 0.1084, 'grad_norm': 2.8539600372314453, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 21:29:18 client1-1  | {'loss': 0.1249, 'grad_norm': 8.035198211669922, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 21:43:11 client2-1  | {'loss': 0.0647, 'grad_norm': 1.8690286874771118, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 21:43:21 client2-1  | {'loss': 0.0825, 'grad_norm': 1.9096176624298096, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 21:43:34 client2-1  | {'loss': 0.0894, 'grad_norm': 2.665442705154419, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 21:43:44 client2-1  | {'loss': 0.0767, 'grad_norm': 8.0185546875, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 21:43:54 client2-1  | {'loss': 0.0831, 'grad_norm': 3.88236927986145, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 21:44:03 client2-1  | {'loss': 0.0809, 'grad_norm': 5.814642906188965, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 21:44:12 client2-1  | {'loss': 0.0788, 'grad_norm': 3.0860660076141357, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 21:44:22 client2-1  | {'loss': 0.079, 'grad_norm': 3.7079567909240723, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 21:44:35 client2-1  | {'loss': 0.0942, 'grad_norm': 4.261107444763184, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 21:44:45 client2-1  | {'loss': 0.1067, 'grad_norm': 6.431185245513916, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 21:44:54 client2-1  | {'loss': 0.085, 'grad_norm': 5.343654155731201, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 21:45:04 client2-1  | {'loss': 0.0944, 'grad_norm': 2.4392175674438477, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 21:45:13 client2-1  | {'loss': 0.0863, 'grad_norm': 1.6973445415496826, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 21:29:27 client1-1  | {'loss': 0.1266, 'grad_norm': 5.383060455322266, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 21:29:37 client1-1  | {'loss': 0.1475, 'grad_norm': 6.5899739265441895, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 21:29:50 client1-1  | {'loss': 0.1564, 'grad_norm': 10.131680488586426, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 21:30:00 client1-1  | {'loss': 0.1879, 'grad_norm': 8.813668251037598, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 21:30:09 client1-1  | {'loss': 0.1829, 'grad_norm': 9.398365020751953, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 21:30:19 client1-1  | {'loss': 0.2683, 'grad_norm': 9.776108741760254, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 21:45:23 client2-1  | {'loss': 0.0757, 'grad_norm': 3.49824595451355, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 21:30:28 client1-1  | {'loss': 0.3189, 'grad_norm': 11.323969841003418, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 21:30:38 client1-1  | {'loss': 0.3568, 'grad_norm': 11.073196411132812, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 21:30:51 client1-1  | {'loss': 0.5597, 'grad_norm': 16.447452545166016, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 21:31:01 client1-1  | {'loss': 0.5743, 'grad_norm': 17.54656982421875, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 21:31:10 client1-1  | {'loss': 0.729, 'grad_norm': 12.685921669006348, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 21:31:20 client1-1  | {'loss': 0.7552, 'grad_norm': 17.093408584594727, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 21:31:30 client1-1  | {'loss': 0.5238, 'grad_norm': 8.888728141784668, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 21:31:39 client1-1  | {'loss': 0.1659, 'grad_norm': 3.6975643634796143, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 21:31:52 client1-1  | {'loss': 0.0795, 'grad_norm': 6.191405773162842, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 21:45:36 client2-1  | {'loss': 0.0885, 'grad_norm': 6.5213189125061035, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 21:45:45 client2-1  | {'loss': 0.108, 'grad_norm': 3.9909615516662598, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 21:45:55 client2-1  | {'loss': 0.1013, 'grad_norm': 1.9859375953674316, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 21:31:52 client1-1  | {'train_runtime': 506.5094, 'train_samples_per_second': 1.974, 'train_steps_per_second': 0.987, 'train_loss': 0.1633798336982727, 'epoch': 1.0}
2025-05-13 21:31:58 client1-1  | INFO :      Sent reply
2025-05-13 21:46:04 client2-1  | {'loss': 0.1035, 'grad_norm': 7.130936622619629, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 21:32:10 client1-1  | INFO :      
2025-05-13 21:46:14 client2-1  | {'loss': 0.1027, 'grad_norm': 5.2483320236206055, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 21:46:23 client2-1  | {'loss': 0.0923, 'grad_norm': 4.551262378692627, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 21:46:37 client2-1  | {'loss': 0.1036, 'grad_norm': 4.002014636993408, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 21:46:46 client2-1  | {'loss': 0.1112, 'grad_norm': 4.670133113861084, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 21:46:56 client2-1  | {'loss': 0.1175, 'grad_norm': 1.8050997257232666, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 21:47:05 client2-1  | {'loss': 0.1292, 'grad_norm': 8.021267890930176, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 21:47:15 client2-1  | {'loss': 0.1672, 'grad_norm': 8.483105659484863, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 21:32:10 client1-1  | INFO :      Received: train message e2cb25e5-76bc-46c2-bc3a-2480fbcbe79c
2025-05-13 21:47:24 client2-1  | {'loss': 0.1356, 'grad_norm': 6.153296947479248, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 21:32:21 client1-1  | {'loss': 0.0598, 'grad_norm': 1.1940654516220093, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 21:32:31 client1-1  | {'loss': 0.0634, 'grad_norm': 2.5304782390594482, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 21:32:44 client1-1  | {'loss': 0.0588, 'grad_norm': 3.6660494804382324, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 21:47:37 client2-1  | {'loss': 0.1562, 'grad_norm': 3.7540276050567627, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 21:47:47 client2-1  | {'loss': 0.1835, 'grad_norm': 12.792731285095215, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 21:47:56 client2-1  | {'loss': 0.2511, 'grad_norm': 8.089343070983887, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 21:48:06 client2-1  | {'loss': 0.2879, 'grad_norm': 6.867630958557129, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 21:48:15 client2-1  | {'loss': 0.3137, 'grad_norm': 13.224011421203613, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 21:48:25 client2-1  | {'loss': 0.4444, 'grad_norm': 11.256196022033691, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 21:32:53 client1-1  | {'loss': 0.0596, 'grad_norm': 1.730766773223877, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 21:33:03 client1-1  | {'loss': 0.0581, 'grad_norm': 1.7047491073608398, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 21:33:13 client1-1  | {'loss': 0.061, 'grad_norm': 2.881472110748291, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 21:33:22 client1-1  | {'loss': 0.0782, 'grad_norm': 2.640937328338623, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 21:33:32 client1-1  | {'loss': 0.0574, 'grad_norm': 2.3841922283172607, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 21:33:45 client1-1  | {'loss': 0.0694, 'grad_norm': 4.827442646026611, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 21:48:34 client2-1  | {'loss': 0.471, 'grad_norm': 12.498891830444336, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 21:48:48 client2-1  | {'loss': 0.6757, 'grad_norm': 15.119866371154785, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 21:48:57 client2-1  | {'loss': 0.8074, 'grad_norm': 14.695798873901367, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 21:33:55 client1-1  | {'loss': 0.072, 'grad_norm': 3.232682466506958, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 21:34:04 client1-1  | {'loss': 0.0798, 'grad_norm': 2.074709892272949, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 21:34:14 client1-1  | {'loss': 0.0859, 'grad_norm': 5.1105852127075195, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 21:49:07 client2-1  | {'loss': 0.3636, 'grad_norm': 4.489425182342529, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 21:49:16 client2-1  | {'loss': 0.1711, 'grad_norm': 2.237565755844116, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 21:34:23 client1-1  | {'loss': 0.0667, 'grad_norm': 2.318800687789917, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 21:34:33 client1-1  | {'loss': 0.0818, 'grad_norm': 2.223132371902466, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 21:34:46 client1-1  | {'loss': 0.102, 'grad_norm': 2.2952637672424316, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 21:34:56 client1-1  | {'loss': 0.0782, 'grad_norm': 2.998955488204956, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 21:35:05 client1-1  | {'loss': 0.0777, 'grad_norm': 3.278609275817871, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 21:35:15 client1-1  | {'loss': 0.0878, 'grad_norm': 4.654491424560547, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 21:49:26 client2-1  | {'loss': 0.0814, 'grad_norm': 4.752817153930664, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 21:49:26 client2-1  | {'train_runtime': 506.3074, 'train_samples_per_second': 1.975, 'train_steps_per_second': 0.988, 'train_loss': 0.15069797265529633, 'epoch': 1.0}
2025-05-13 21:35:25 client1-1  | {'loss': 0.0879, 'grad_norm': 1.8523187637329102, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 21:35:34 client1-1  | {'loss': 0.093, 'grad_norm': 3.948428153991699, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 21:35:47 client1-1  | {'loss': 0.0789, 'grad_norm': 2.8535454273223877, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 21:35:57 client1-1  | {'loss': 0.0797, 'grad_norm': 5.606276035308838, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 21:36:06 client1-1  | {'loss': 0.092, 'grad_norm': 6.549661159515381, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 21:36:17 client1-1  | {'loss': 0.0712, 'grad_norm': 1.9247701168060303, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 21:36:30 client1-1  | {'loss': 0.0882, 'grad_norm': 4.359533309936523, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 21:36:39 client1-1  | {'loss': 0.1002, 'grad_norm': 4.542123317718506, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 21:36:49 client1-1  | {'loss': 0.0949, 'grad_norm': 4.004480361938477, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 21:36:58 client1-1  | {'loss': 0.1087, 'grad_norm': 8.554088592529297, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 21:37:08 client1-1  | {'loss': 0.0942, 'grad_norm': 5.971658706665039, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 21:37:21 client1-1  | {'loss': 0.0998, 'grad_norm': 3.461533546447754, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 21:37:30 client1-1  | {'loss': 0.1147, 'grad_norm': 2.8463499546051025, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 21:49:35 client2-1  | INFO :      Sent reply
2025-05-13 21:49:48 client2-1  | INFO :      
2025-05-13 21:49:48 client2-1  | INFO :      Received: train message 8af6cbb0-b47e-4b84-92c4-40842ee99125
2025-05-13 21:49:58 client2-1  | {'loss': 0.06, 'grad_norm': 1.5643917322158813, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 21:37:40 client1-1  | {'loss': 0.1025, 'grad_norm': 3.633119821548462, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 21:37:49 client1-1  | {'loss': 0.0901, 'grad_norm': 1.6333115100860596, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 21:37:59 client1-1  | {'loss': 0.103, 'grad_norm': 2.8496010303497314, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 21:38:08 client1-1  | {'loss': 0.1267, 'grad_norm': 5.776820182800293, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 21:38:22 client1-1  | {'loss': 0.1176, 'grad_norm': 4.901575088500977, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 21:38:31 client1-1  | {'loss': 0.1435, 'grad_norm': 7.590938568115234, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 21:38:41 client1-1  | {'loss': 0.141, 'grad_norm': 9.099174499511719, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 21:38:50 client1-1  | {'loss': 0.1695, 'grad_norm': 11.033476829528809, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 21:39:00 client1-1  | {'loss': 0.1647, 'grad_norm': 7.644298076629639, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 21:39:13 client1-1  | {'loss': 0.2463, 'grad_norm': 10.772293090820312, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 21:50:08 client2-1  | {'loss': 0.0568, 'grad_norm': 1.538329005241394, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 21:50:21 client2-1  | {'loss': 0.0457, 'grad_norm': 2.5297818183898926, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 21:50:31 client2-1  | {'loss': 0.0652, 'grad_norm': 2.143148422241211, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 21:39:23 client1-1  | {'loss': 0.2993, 'grad_norm': 10.501119613647461, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 21:39:33 client1-1  | {'loss': 0.3261, 'grad_norm': 9.415099143981934, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 21:50:41 client2-1  | {'loss': 0.0787, 'grad_norm': 7.655077934265137, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 21:50:50 client2-1  | {'loss': 0.08, 'grad_norm': 2.159092664718628, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 21:51:00 client2-1  | {'loss': 0.0738, 'grad_norm': 3.4649486541748047, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 21:51:09 client2-1  | {'loss': 0.0734, 'grad_norm': 2.151357412338257, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 21:51:19 client2-1  | {'loss': 0.062, 'grad_norm': 3.719770669937134, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 21:51:32 client2-1  | {'loss': 0.0776, 'grad_norm': 11.277645111083984, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 21:51:42 client2-1  | {'loss': 0.0648, 'grad_norm': 1.9410853385925293, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 21:51:51 client2-1  | {'loss': 0.0793, 'grad_norm': 3.6100826263427734, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 21:52:01 client2-1  | {'loss': 0.0721, 'grad_norm': 2.3529181480407715, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 21:52:10 client2-1  | {'loss': 0.0718, 'grad_norm': 1.8730090856552124, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 21:52:20 client2-1  | {'loss': 0.086, 'grad_norm': 2.8471977710723877, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 21:52:29 client2-1  | {'loss': 0.0808, 'grad_norm': 4.982432842254639, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 21:52:42 client2-1  | {'loss': 0.08, 'grad_norm': 3.7596986293792725, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 21:52:52 client2-1  | {'loss': 0.0865, 'grad_norm': 9.305231094360352, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 21:39:42 client1-1  | {'loss': 0.5383, 'grad_norm': 17.68105697631836, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 21:39:55 client1-1  | {'loss': 0.5561, 'grad_norm': 19.595834732055664, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 21:53:01 client2-1  | {'loss': 0.0868, 'grad_norm': 5.241065502166748, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 21:53:11 client2-1  | {'loss': 0.0817, 'grad_norm': 4.356966495513916, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 21:40:05 client1-1  | {'loss': 0.727, 'grad_norm': 13.865349769592285, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 21:53:20 client2-1  | {'loss': 0.0945, 'grad_norm': 1.7996954917907715, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 21:53:30 client2-1  | {'loss': 0.0853, 'grad_norm': 4.118070602416992, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 21:53:39 client2-1  | {'loss': 0.0834, 'grad_norm': 3.7289624214172363, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 21:53:49 client2-1  | {'loss': 0.0914, 'grad_norm': 3.4885239601135254, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 21:54:02 client2-1  | {'loss': 0.0916, 'grad_norm': 1.3161433935165405, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 21:40:14 client1-1  | {'loss': 0.7094, 'grad_norm': 18.55879783630371, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 21:40:24 client1-1  | {'loss': 0.5164, 'grad_norm': 6.802910804748535, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 21:40:34 client1-1  | {'loss': 0.1513, 'grad_norm': 2.956127166748047, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 21:40:43 client1-1  | {'loss': 0.0755, 'grad_norm': 4.927356719970703, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 21:54:12 client2-1  | {'loss': 0.081, 'grad_norm': 4.057227611541748, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 21:54:21 client2-1  | {'loss': 0.097, 'grad_norm': 5.202818393707275, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 21:54:31 client2-1  | {'loss': 0.097, 'grad_norm': 2.0464253425598145, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 21:54:40 client2-1  | {'loss': 0.0919, 'grad_norm': 2.043041944503784, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 21:54:50 client2-1  | {'loss': 0.1109, 'grad_norm': 4.642995357513428, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 21:54:59 client2-1  | {'loss': 0.0948, 'grad_norm': 4.558996677398682, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 21:55:12 client2-1  | {'loss': 0.0961, 'grad_norm': 4.154425621032715, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 21:40:43 client1-1  | {'train_runtime': 511.4563, 'train_samples_per_second': 1.955, 'train_steps_per_second': 0.978, 'train_loss': 0.15610611844062805, 'epoch': 1.0}
2025-05-13 21:55:22 client2-1  | {'loss': 0.0926, 'grad_norm': 3.646726608276367, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 21:55:31 client2-1  | {'loss': 0.1148, 'grad_norm': 4.733541488647461, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 21:55:41 client2-1  | {'loss': 0.1388, 'grad_norm': 3.4120428562164307, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 21:55:50 client2-1  | {'loss': 0.1154, 'grad_norm': 6.253478527069092, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 21:40:47 client1-1  | INFO :      Sent reply
2025-05-13 21:40:59 client1-1  | INFO :      
2025-05-13 21:40:59 client1-1  | INFO :      Received: train message 26dda4da-1082-4807-88df-04cc58c79f51
2025-05-13 21:41:11 client1-1  | {'loss': 0.0665, 'grad_norm': 2.853641986846924, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 21:41:21 client1-1  | {'loss': 0.0511, 'grad_norm': 1.9172050952911377, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 21:56:00 client2-1  | {'loss': 0.1485, 'grad_norm': 5.000114917755127, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 21:56:09 client2-1  | {'loss': 0.1427, 'grad_norm': 4.906798839569092, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 21:56:19 client2-1  | {'loss': 0.1646, 'grad_norm': 3.9830780029296875, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 21:41:34 client1-1  | {'loss': 0.0606, 'grad_norm': 4.149043560028076, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 21:41:44 client1-1  | {'loss': 0.0649, 'grad_norm': 1.8319040536880493, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 21:56:32 client2-1  | {'loss': 0.1667, 'grad_norm': 9.045103073120117, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 21:56:42 client2-1  | {'loss': 0.2327, 'grad_norm': 7.992748737335205, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 21:56:51 client2-1  | {'loss': 0.2474, 'grad_norm': 8.935277938842773, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 21:41:53 client1-1  | {'loss': 0.0531, 'grad_norm': 1.934498906135559, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 21:42:07 client1-1  | {'loss': 0.0622, 'grad_norm': 1.6818760633468628, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 21:42:16 client1-1  | {'loss': 0.06, 'grad_norm': 2.3934221267700195, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 21:42:26 client1-1  | {'loss': 0.0628, 'grad_norm': 3.7828328609466553, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 21:42:36 client1-1  | {'loss': 0.0648, 'grad_norm': 3.6556663513183594, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 21:42:45 client1-1  | {'loss': 0.0633, 'grad_norm': 1.9895066022872925, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 21:42:59 client1-1  | {'loss': 0.0789, 'grad_norm': 3.090693235397339, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 21:43:08 client1-1  | {'loss': 0.0823, 'grad_norm': 3.8624637126922607, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 21:43:18 client1-1  | {'loss': 0.072, 'grad_norm': 5.182377338409424, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 21:43:28 client1-1  | {'loss': 0.084, 'grad_norm': 2.0401296615600586, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 21:43:37 client1-1  | {'loss': 0.1235, 'grad_norm': 3.4093470573425293, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 21:43:51 client1-1  | {'loss': 0.08, 'grad_norm': 3.8901169300079346, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 21:44:00 client1-1  | {'loss': 0.077, 'grad_norm': 4.477551460266113, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 21:44:10 client1-1  | {'loss': 0.081, 'grad_norm': 4.681048393249512, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 21:44:19 client1-1  | {'loss': 0.0817, 'grad_norm': 4.482594013214111, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 21:44:29 client1-1  | {'loss': 0.0912, 'grad_norm': 10.037930488586426, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 21:44:42 client1-1  | {'loss': 0.0919, 'grad_norm': 1.7543435096740723, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 21:44:51 client1-1  | {'loss': 0.0948, 'grad_norm': 3.893315553665161, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 21:45:01 client1-1  | {'loss': 0.0845, 'grad_norm': 4.282000541687012, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 21:45:11 client1-1  | {'loss': 0.0668, 'grad_norm': 2.7142574787139893, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 21:45:20 client1-1  | {'loss': 0.0841, 'grad_norm': 2.6239328384399414, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 21:45:34 client1-1  | {'loss': 0.0964, 'grad_norm': 4.524376392364502, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 21:45:43 client1-1  | {'loss': 0.0854, 'grad_norm': 2.1982154846191406, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 21:45:53 client1-1  | {'loss': 0.1004, 'grad_norm': 6.04847526550293, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 21:46:02 client1-1  | {'loss': 0.101, 'grad_norm': 8.12905502319336, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 21:46:12 client1-1  | {'loss': 0.0984, 'grad_norm': 4.547202110290527, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 21:46:25 client1-1  | {'loss': 0.1119, 'grad_norm': 3.4446725845336914, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 21:46:34 client1-1  | {'loss': 0.1001, 'grad_norm': 3.712646961212158, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 21:46:44 client1-1  | {'loss': 0.0988, 'grad_norm': 1.44041109085083, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 21:46:54 client1-1  | {'loss': 0.0948, 'grad_norm': 2.8928518295288086, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 21:47:03 client1-1  | {'loss': 0.1318, 'grad_norm': 6.999297142028809, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 21:47:13 client1-1  | {'loss': 0.1195, 'grad_norm': 3.8955798149108887, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 21:47:26 client1-1  | {'loss': 0.1465, 'grad_norm': 6.39834451675415, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 21:47:35 client1-1  | {'loss': 0.1368, 'grad_norm': 10.064943313598633, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 21:47:45 client1-1  | {'loss': 0.171, 'grad_norm': 10.374921798706055, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 21:47:54 client1-1  | {'loss': 0.165, 'grad_norm': 6.111945629119873, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 21:48:04 client1-1  | {'loss': 0.2309, 'grad_norm': 9.795310020446777, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 21:48:13 client1-1  | {'loss': 0.299, 'grad_norm': 9.590296745300293, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 21:48:26 client1-1  | {'loss': 0.2987, 'grad_norm': 10.08295726776123, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 21:48:36 client1-1  | {'loss': 0.5242, 'grad_norm': 19.729646682739258, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 21:48:46 client1-1  | {'loss': 0.5206, 'grad_norm': 18.249984741210938, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 21:48:55 client1-1  | {'loss': 0.6882, 'grad_norm': 13.251409530639648, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 21:49:05 client1-1  | {'loss': 0.7045, 'grad_norm': 17.524946212768555, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 21:49:14 client1-1  | {'loss': 0.474, 'grad_norm': 8.212808609008789, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 21:49:28 client1-1  | {'loss': 0.1488, 'grad_norm': 1.7094608545303345, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 21:49:33 client1-1  | {'loss': 0.0807, 'grad_norm': 4.394961357116699, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 21:49:33 client1-1  | {'train_runtime': 511.8314, 'train_samples_per_second': 1.954, 'train_steps_per_second': 0.977, 'train_loss': 0.15220700764656067, 'epoch': 1.0}
2025-05-13 21:49:37 client1-1  | INFO :      Sent reply
2025-05-13 21:49:47 client1-1  | INFO :      
2025-05-13 21:49:47 client1-1  | INFO :      Received: train message 688fcec8-ac1e-4286-9c62-2aa68b4a7e73
2025-05-13 21:50:00 client1-1  | {'loss': 0.0705, 'grad_norm': 1.8689552545547485, 'learning_rate': 4.91e-05, 'epoch': 0.02}
2025-05-13 21:50:10 client1-1  | {'loss': 0.0527, 'grad_norm': 1.1743066310882568, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.04}
2025-05-13 21:50:20 client1-1  | {'loss': 0.069, 'grad_norm': 2.9150402545928955, 'learning_rate': 4.71e-05, 'epoch': 0.06}
2025-05-13 21:50:29 client1-1  | {'loss': 0.0631, 'grad_norm': 4.385656356811523, 'learning_rate': 4.61e-05, 'epoch': 0.08}
2025-05-13 21:50:43 client1-1  | {'loss': 0.0714, 'grad_norm': 2.9935812950134277, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.1}
2025-05-13 21:50:52 client1-1  | {'loss': 0.0599, 'grad_norm': 1.5626050233840942, 'learning_rate': 4.41e-05, 'epoch': 0.12}
2025-05-13 21:51:02 client1-1  | {'loss': 0.071, 'grad_norm': 2.9315361976623535, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.14}
2025-05-13 21:51:12 client1-1  | {'loss': 0.0617, 'grad_norm': 2.5450685024261475, 'learning_rate': 4.21e-05, 'epoch': 0.16}
2025-05-13 21:51:21 client1-1  | {'loss': 0.0777, 'grad_norm': 2.4530599117279053, 'learning_rate': 4.11e-05, 'epoch': 0.18}
2025-05-13 21:51:31 client1-1  | {'loss': 0.0695, 'grad_norm': 3.6393070220947266, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.2}
2025-05-13 21:51:44 client1-1  | {'loss': 0.0798, 'grad_norm': 6.572072505950928, 'learning_rate': 3.91e-05, 'epoch': 0.22}
2025-05-13 21:51:54 client1-1  | {'loss': 0.0859, 'grad_norm': 5.199493885040283, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.24}
2025-05-13 21:52:03 client1-1  | {'loss': 0.0711, 'grad_norm': 3.679135322570801, 'learning_rate': 3.71e-05, 'epoch': 0.26}
2025-05-13 21:52:12 client1-1  | {'loss': 0.0739, 'grad_norm': 2.139683961868286, 'learning_rate': 3.61e-05, 'epoch': 0.28}
2025-05-13 21:52:22 client1-1  | {'loss': 0.0998, 'grad_norm': 3.722794771194458, 'learning_rate': 3.51e-05, 'epoch': 0.3}
2025-05-13 21:52:31 client1-1  | {'loss': 0.077, 'grad_norm': 1.7042617797851562, 'learning_rate': 3.41e-05, 'epoch': 0.32}
2025-05-13 21:52:41 client1-1  | {'loss': 0.0637, 'grad_norm': 2.39345645904541, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.34}
2025-05-13 21:52:54 client1-1  | {'loss': 0.0836, 'grad_norm': 7.944639205932617, 'learning_rate': 3.21e-05, 'epoch': 0.36}
2025-05-13 21:53:04 client1-1  | {'loss': 0.0853, 'grad_norm': 4.145519256591797, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.38}
2025-05-13 21:53:13 client1-1  | {'loss': 0.0802, 'grad_norm': 4.080754280090332, 'learning_rate': 3.01e-05, 'epoch': 0.4}
2025-05-13 21:53:23 client1-1  | {'loss': 0.089, 'grad_norm': 3.4720118045806885, 'learning_rate': 2.91e-05, 'epoch': 0.42}
2025-05-13 21:53:32 client1-1  | {'loss': 0.089, 'grad_norm': 4.029751777648926, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.44}
2025-05-13 21:53:42 client1-1  | {'loss': 0.0833, 'grad_norm': 3.5414369106292725, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.46}
2025-05-13 21:53:55 client1-1  | {'loss': 0.0703, 'grad_norm': 3.720419406890869, 'learning_rate': 2.61e-05, 'epoch': 0.48}
2025-05-13 21:54:05 client1-1  | {'loss': 0.0875, 'grad_norm': 3.4382436275482178, 'learning_rate': 2.51e-05, 'epoch': 0.5}
2025-05-13 21:54:14 client1-1  | {'loss': 0.0946, 'grad_norm': 5.297275543212891, 'learning_rate': 2.41e-05, 'epoch': 0.52}
2025-05-13 21:54:24 client1-1  | {'loss': 0.0935, 'grad_norm': 1.4560487270355225, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.54}
2025-05-13 21:54:33 client1-1  | {'loss': 0.1013, 'grad_norm': 3.8674800395965576, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.56}
2025-05-13 21:54:43 client1-1  | {'loss': 0.0948, 'grad_norm': 4.624529838562012, 'learning_rate': 2.11e-05, 'epoch': 0.58}
2025-05-13 21:54:52 client1-1  | {'loss': 0.0953, 'grad_norm': 4.609094142913818, 'learning_rate': 2.01e-05, 'epoch': 0.6}
2025-05-13 21:55:05 client1-1  | {'loss': 0.1167, 'grad_norm': 2.174886465072632, 'learning_rate': 1.91e-05, 'epoch': 0.62}
2025-05-13 21:55:15 client1-1  | {'loss': 0.1016, 'grad_norm': 2.501317024230957, 'learning_rate': 1.81e-05, 'epoch': 0.64}
2025-05-13 21:55:24 client1-1  | {'loss': 0.0981, 'grad_norm': 1.8645282983779907, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.66}
2025-05-13 21:55:34 client1-1  | {'loss': 0.0984, 'grad_norm': 2.3429667949676514, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.68}
2025-05-13 21:55:43 client1-1  | {'loss': 0.1254, 'grad_norm': 7.858593463897705, 'learning_rate': 1.51e-05, 'epoch': 0.7}
2025-05-13 21:55:53 client1-1  | {'loss': 0.1135, 'grad_norm': 4.112585067749023, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.72}
2025-05-13 21:56:02 client1-1  | {'loss': 0.1409, 'grad_norm': 6.950201511383057, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.74}
2025-05-13 21:56:16 client1-1  | {'loss': 0.1485, 'grad_norm': 8.77146053314209, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.76}
2025-05-13 21:57:01 client2-1  | {'loss': 0.2926, 'grad_norm': 13.900419235229492, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 21:57:10 client2-1  | {'loss': 0.4141, 'grad_norm': 11.973499298095703, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 21:56:25 client1-1  | {'loss': 0.1613, 'grad_norm': 9.602090835571289, 'learning_rate': 1.11e-05, 'epoch': 0.78}
2025-05-13 21:56:35 client1-1  | {'loss': 0.1571, 'grad_norm': 7.9303131103515625, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.8}
2025-05-13 21:57:20 client2-1  | {'loss': 0.4372, 'grad_norm': 13.497037887573242, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 21:57:29 client2-1  | {'loss': 0.6629, 'grad_norm': 16.6589298248291, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 21:57:39 client2-1  | {'loss': 0.7583, 'grad_norm': 13.37173080444336, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 21:57:52 client2-1  | {'loss': 0.3527, 'grad_norm': 5.666657447814941, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 21:58:02 client2-1  | {'loss': 0.1572, 'grad_norm': 3.2874999046325684, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 21:58:11 client2-1  | {'loss': 0.0728, 'grad_norm': 3.9500889778137207, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 21:58:11 client2-1  | {'train_runtime': 502.0665, 'train_samples_per_second': 1.992, 'train_steps_per_second': 0.996, 'train_loss': 0.14580161929130553, 'epoch': 1.0}
2025-05-13 21:56:44 client1-1  | {'loss': 0.2246, 'grad_norm': 9.671689987182617, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.82}
2025-05-13 21:58:16 client2-1  | INFO :      Sent reply
2025-05-13 21:58:23 client2-1  | INFO :      
2025-05-13 21:58:23 client2-1  | INFO :      Received: reconnect message d48f5367-e6b2-4cd0-94ae-1811e219925b
2025-05-13 21:56:54 client1-1  | {'loss': 0.2692, 'grad_norm': 12.395649909973145, 'learning_rate': 8.1e-06, 'epoch': 0.84}
2025-05-13 21:57:03 client1-1  | {'loss': 0.2976, 'grad_norm': 11.802560806274414, 'learning_rate': 7.1e-06, 'epoch': 0.86}
2025-05-13 21:57:13 client1-1  | {'loss': 0.4906, 'grad_norm': 15.968847274780273, 'learning_rate': 6.1e-06, 'epoch': 0.88}
2025-05-13 21:58:24 client2-1  | INFO :      Disconnect and shut down
2025-05-13 21:57:26 client1-1  | {'loss': 0.498, 'grad_norm': 16.077299118041992, 'learning_rate': 5.1e-06, 'epoch': 0.9}
2025-05-13 21:57:36 client1-1  | {'loss': 0.6745, 'grad_norm': 15.396512985229492, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.92}
2025-05-13 21:57:45 client1-1  | {'loss': 0.6648, 'grad_norm': 21.416549682617188, 'learning_rate': 3.1e-06, 'epoch': 0.94}
2025-05-13 21:57:55 client1-1  | {'loss': 0.4574, 'grad_norm': 6.317409992218018, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.96}
2025-05-13 21:58:04 client1-1  | {'loss': 0.1401, 'grad_norm': 1.4703702926635742, 'learning_rate': 1.1e-06, 'epoch': 0.98}
2025-05-13 21:58:14 client1-1  | {'loss': 0.0798, 'grad_norm': 2.7685792446136475, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.0}
2025-05-13 21:58:14 client1-1  | {'train_runtime': 505.7072, 'train_samples_per_second': 1.977, 'train_steps_per_second': 0.989, 'train_loss': 0.14846681201457979, 'epoch': 1.0}
2025-05-13 21:58:18 client1-1  | INFO :      Sent reply
2025-05-13 21:58:23 client1-1  | INFO :      
2025-05-13 21:58:23 client1-1  | INFO :      Received: reconnect message d0d627f2-3db7-428c-bf0f-742b285238f2
2025-05-13 21:58:24 client1-1  | INFO :      Disconnect and shut down
